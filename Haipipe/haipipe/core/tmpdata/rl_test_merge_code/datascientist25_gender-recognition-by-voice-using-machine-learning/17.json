{"seq": [{"operator": "VarianceThreshold", "edge_id": "34---before"}], "code": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('../input/voicegender/voice.csv')\ndf.columns\ndf.shape\ndf.dtypes\ncolors = ['pink','Lightblue']\ndata_y = df[df.columns[-1]]\n#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])\n#plt.axis('equal')\n#df.boxplot(column = 'meanfreq',by='label',grid=False)\ncorrelation =df.corr()\n#sns.heatmap(correlation)\n#plt.show()\nfrom sklearn.model_selection import train_test_split\nX = df[df.columns[:-1]].values\ny = df[df.columns[-1]].values\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier()\n#rand_forest.fit(Xtrain, ytrain)\n#y_pred = rand_forest.predict(Xtest)\nfrom sklearn import metrics, neighbors\nfrom sklearn.metrics import accuracy_score\n#print(metrics.accuracy_score(ytest, y_pred))\nfrom sklearn.metrics import confusion_matrix\n#print(confusion_matrix(ytest, y_pred))\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nCVFirst = GaussianNB()\n#CVFirst = CVFirst.fit(Xtrain, ytrain)\n#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')\n#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())\nmale_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index\nfemale_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index\nindex_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)\nlen(index_to_remove)\ndata_x = df[df.columns[0:20]].copy()\ndata2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()\ndata2.head(3)\n\nfrom sklearn.feature_selection import VarianceThreshold\nimport pandas as pd\nimport numpy as np\nfrom itertools import compress\n        \ndata2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()\nadd_seletcion = VarianceThreshold()\nadd_seletcion.fit(data2)\n\ncols = list(data2.columns)\nmask = add_seletcion.get_support(indices=False)\nfinal_cols = list(compress(cols, mask))\ndata2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)\n        \ndata2 = data2.drop(index_to_remove,axis=0)\ndata_y = pd.Series(y).drop(index_to_remove,axis=0)\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)\nclf1 = RandomForestClassifier()\n#clf1.fit(Xtrain, ytrain)\n#y_pred = clf1.predict(Xtest)\n#print(metrics.accuracy_score(ytest, y_pred))\nfrom sklearn.tree import DecisionTreeClassifier\nclf2 = DecisionTreeClassifier()\n#clf2.fit(Xtrain, ytrain)\n#y_predict = clf2.predict(Xtest)\n#print(metrics.accuracy_score(ytest, y_predict))\nclf3 = GaussianNB()\n#clf3 = clf3.fit(Xtrain, ytrain)\n#y_predd = clf3.predict(Xtest)\n#print(metrics.accuracy_score(ytest,y_predd))\nfrom sklearn.linear_model import LogisticRegression\nclf4 = LogisticRegression()\n#clf4.fit(Xtrain,ytrain)\n#y_predict4 = clf4.predict(Xtest)\n#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')\n#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')\nimport pylab as pl\nlabels = ['female', 'male']\n#cm = confusion_matrix(ytest,y_pred,labels)  \n#print(cm)\n#fig = plt.figure()\n#ax = fig.add_subplot(111)\n#cax =ax.matshow(cm)\npl.title('Confusion matrix of the classifier')\n#fig.colorbar(cax)\n#ax.set_xticklabels([''] + labels)\n#ax.set_yticklabels([''] + labels)\npl.xlabel('Predicted')\npl.ylabel('True')\n#pl.show()\nfrom sklearn.metrics import classification_report\n#print(classification_report(ytest, y_pred))\n#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,\"meanfun\").add_legend()\n#plt.show()\nfrom sklearn.cluster import KMeans\nfrom matplotlib import style\nstyle.use(\"ggplot\")\ndata_x = np.array(df[['meanfreq','meanfun']])\nkmeans = KMeans(n_clusters= 2)\nkmeans.fit(data_x)\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\ncolors = [\"g.\",\"b.\"]  \n#for i in range(len(data_x)):\n#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)\n    \n#plt.scatter(centroids[:,0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\n#plt.ylabel('meanfun')\n#plt.xlabel('meanfun')\n#plt.show()\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model.logistic import LogisticRegression\n#print(\"start running model training........\")\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nmodel.fit(Xtrain, ytrain)\ny_pred = model.predict(Xtest)\nscore = accuracy_score(ytest, y_pred)\nimport numpy as np\nnp.save(\"haipipe/core/tmpdata/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning/17.npy\", { \"accuracy_score\": score })\n\n"}