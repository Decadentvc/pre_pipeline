{"seq": [{"operator": "MinMaxScaler", "edge_id": "5---before"}], "code": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,roc_curve,classification_report,roc_auc_score\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\ndf=pd.read_csv('../input/heart-patients/US_Heart_Patients.csv')\ndf.describe()\ndf.isnull().sum()\nno_m_v=df.dropna(axis=0)\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nimport numpy as np\ndef catch_num(data):\n    num_cols = [col for col in data.columns if str(data[col].dtypes) != 'object']\n    cat_cols = [col for col in data.columns if col not in num_cols]\n    cat_train_x = data[cat_cols]\n    num_train_x = data[num_cols]\n    return cat_train_x, num_train_x\n        \nadd_scaler = MinMaxScaler()\ndata_w_d = pd.DataFrame(data_w_d).reset_index(drop=True).infer_objects()\ncat_train_x, num_train_x = catch_num(data_w_d)\nadd_scaler.fit(num_train_x)\nnum_train_x = pd.DataFrame(add_scaler.transform(num_train_x), columns=list(num_train_x.columns)).reset_index(drop=True).infer_objects()\ndata_w_d = pd.concat([cat_train_x.reset_index(drop=True), num_train_x.reset_index(drop=True)],axis=1)\n        \ndata_w_d=pd.get_dummies(no_m_v,drop_first=True)\ndata_w_d.columns\ny=data_w_d['TenYearCHD']\nx=data_w_d.drop(['TenYearCHD'],axis=1)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=1-0.8, random_state=0)\nmodels={'Logistic Regression':LogisticRegression(),'KNN':KNeighborsClassifier(),'RandomForest':RandomForestClassifier()}\ndef fit_and_plot(models,x_train,x_test,y_train,y_test):\n    model_scores={}\n#    for name,model in models.items():\n#        model.fit(x_train,y_train)\n#        model_scores[name]=model.score(x_test,y_test)\n    return model_scores\n#scores=fit_and_plot(models,x_train,x_test,y_train,y_test)\n#scores\ntrain_scores=[]\ntest_scores=[]\nknn=KNeighborsClassifier()\nneighbors=range(1,41)\n#for i in neighbors:\n#    knn.fit(x_train,y_train)\n#    knn.set_params(n_neighbors=i)\n#    train_scores.append(knn.score(x_train,y_train))\n#    test_scores.append(knn.score(x_test,y_test))\n#plt.plot(neighbors,train_scores,label='train',color='orange')\n#plt.plot(neighbors,test_scores,label='test',color='darkblue')\n#plt.legend()\n#plt.show()\n#print(f'Test maximum score:{max(test_scores)*100:.2f}%')\nlog_grid={'C':np.logspace(-4,4,20),'solver':['liblinear']}\nrandom_grid={'n_estimators':[50,100,150],'max_depth':[None,5,10],'max_features':['auto','sqrt'],'min_samples_split':[2,4,6],'min_samples_leaf':[1,2,3]}\nrs_log_search=RandomizedSearchCV(LogisticRegression(),param_distributions=log_grid,n_iter=5,cv=5,verbose=True)\nrs_random_search=RandomizedSearchCV(RandomForestClassifier(),param_distributions=random_grid,n_iter=5,cv=5,verbose=True)\n#rs_log_search.fit(x_train,y_train)\n#rs_random_search.fit(x_train,y_train)\n#print(rs_log_search.score(x_test,y_test))\n#print(rs_random_search.score(x_test,y_test))\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n#print(\"start running model training........\")\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore = accuracy_score(y_test, y_pred)\nimport numpy as np\nnp.save(\"haipipe/core/tmpdata/merge_max_result_rl/kulchorobeknazarov_heart-disease-prediction/36.npy\", { \"accuracy_score\": score })\n\n"}