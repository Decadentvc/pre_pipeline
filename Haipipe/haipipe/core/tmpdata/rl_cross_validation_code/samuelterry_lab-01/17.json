{"seq": [{"operator": "ImputerMedian", "edge_id": "end"}, {"operator": "MinMaxScaler", "edge_id": "end"}, {"operator": "IncrementalPCA", "edge_id": "end"}], "code": "import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata = pd.read_csv('/kaggle/input/wisconsin-breast-cancer-cytology-features/wisconsin_breast_cancer.csv')\ndata.head()\nnew_data = data.dropna()\nnew_data.info()\nimport seaborn as sns\n#sns.pairplot(data=new_data, hue=\"class\", palette=\"Set2\" ,diag_kind=\"hist\")\nX = new_data.iloc[0:683 , 1:10]\nprint(X)\ny = new_data.iloc[0:683 , 10:11]\nprint(y)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=1-0.8, random_state=0)\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nimport numpy as np\ndef catch_num(data):\n    num_cols = [col for col in data.columns if str(data[col].dtypes) != 'object']\n    cat_cols = [col for col in data.columns if col not in num_cols]\n    cat_train_x = data[cat_cols]\n    num_train_x = data[num_cols]\n    return cat_train_x, num_train_x\n        \nadd_scaler = MinMaxScaler()\nX = pd.DataFrame(X).reset_index(drop=True).infer_objects()\ncat_train_x, num_train_x = catch_num(X)\nadd_scaler.fit(num_train_x)\nnum_train_x = pd.DataFrame(add_scaler.transform(num_train_x), columns=list(num_train_x.columns)).reset_index(drop=True).infer_objects()\nX = pd.concat([cat_train_x.reset_index(drop=True), num_train_x.reset_index(drop=True)],axis=1)\n        \n\nfrom sklearn.decomposition import IncrementalPCA\nimport pandas as pd\nimport numpy as np\n        \nX = pd.DataFrame(X).reset_index(drop=True).infer_objects()\nadd_engine = IncrementalPCA()\ncols = list(X.columns)\nadd_engine.fit(X)\n\ntrain_data_x = add_engine.transform(X)\nX = pd.DataFrame(train_data_x, columns=cols[:train_data_x.shape[1]])\n        \nfrom sklearn.svm import SVC\nmodel = SVC()\n#model.fit(X_train, y_train)\n#pred = model.predict(X_test)\n#print(pred[0:10])\nprint(y_test[0:10])\nfrom sklearn.metrics import confusion_matrix\n#print(confusion_matrix(y_test,pred))\nfrom sklearn.metrics import classification_report\n#print(classification_report(y_test,pred))\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n#print(\"start running model training........\")\nmodel = SVC(random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nimport numpy as np\nnp.save(\"haipipe/core/tmpdata/merge_max_result_rl/samuelterry_lab-01/17.npy\", { \"accuracy_score\": score })\n\n\n", "validation_code": "import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata = pd.read_csv('/kaggle/input/wisconsin-breast-cancer-cytology-features/wisconsin_breast_cancer.csv')\ndata.head()\nnew_data = data.dropna()\nnew_data.info()\nimport seaborn as sns\n#sns.pairplot(data=new_data, hue=\"class\", palette=\"Set2\" ,diag_kind=\"hist\")\nX = new_data.iloc[0:683 , 1:10]\nprint(X)\ny = new_data.iloc[0:683 , 10:11]\nprint(y)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=1-0.8, random_state=0)\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nimport numpy as np\ndef catch_num(data):\n    num_cols = [col for col in data.columns if str(data[col].dtypes) != 'object']\n    cat_cols = [col for col in data.columns if col not in num_cols]\n    cat_train_x = data[cat_cols]\n    num_train_x = data[num_cols]\n    return cat_train_x, num_train_x\n        \nadd_scaler = MinMaxScaler()\nX = pd.DataFrame(X).reset_index(drop=True).infer_objects()\ncat_train_x, num_train_x = catch_num(X)\nadd_scaler.fit(num_train_x)\nnum_train_x = pd.DataFrame(add_scaler.transform(num_train_x), columns=list(num_train_x.columns)).reset_index(drop=True).infer_objects()\nX = pd.concat([cat_train_x.reset_index(drop=True), num_train_x.reset_index(drop=True)],axis=1)\n        \n\nfrom sklearn.decomposition import IncrementalPCA\nimport pandas as pd\nimport numpy as np\n        \nX = pd.DataFrame(X).reset_index(drop=True).infer_objects()\nadd_engine = IncrementalPCA()\ncols = list(X.columns)\nadd_engine.fit(X)\n\ntrain_data_x = add_engine.transform(X)\nX = pd.DataFrame(train_data_x, columns=cols[:train_data_x.shape[1]])\n        \nfrom sklearn.svm import SVC\nmodel = SVC()\n#model.fit(X_train, y_train)\n#pred = model.predict(X_test)\n#print(pred[0:10])\nprint(y_test[0:10])\nfrom sklearn.metrics import confusion_matrix\n#print(confusion_matrix(y_test,pred))\nfrom sklearn.metrics import classification_report\n#print(classification_report(y_test,pred))\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n#print(\"start running model training........\")\nmodel = SVC(random_state=0)\n#model.fit(X_train, y_train)\n#y_pred = model.predict(x_validation_varible)\n#score = accuracy_score(y_validation_varible, y_pred)\nfrom sklearn.model_selection import cross_val_score\ncross_score = cross_val_score(model, X_train, y_train,cv=4)\nimport numpy as np\n#np.save(\"haipipe/core/tmpdata/merge_max_result_rl/samuelterry_lab-01/17.npy\", { \"accuracy_score\": score })\nnp.save(\"haipipe/core/tmpdata/rl_cross_val_res/samuelterry_lab-01/17.npy\", { \"accuracy_score\": cross_score })\n\n\n\n"}