{"seq": [{"operator": "ImputerMedian", "edge_id": "5---before"}, {"operator": "PolynomialFeatures", "edge_id": "9---after"}], "code": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,roc_curve,classification_report,roc_auc_score\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\ndf=pd.read_csv('../input/heart-patients/US_Heart_Patients.csv')\ndf.describe()\ndf.isnull().sum()\nno_m_v=df.dropna(axis=0)\ndata_w_d=pd.get_dummies(no_m_v,drop_first=True)\ndata_w_d.columns\ny=data_w_d['TenYearCHD']\nx=data_w_d.drop(['TenYearCHD'],axis=1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport pandas as pd\nimport numpy as np\n        \nx = pd.DataFrame(x).reset_index(drop=True).infer_objects()\nadd_engine = PolynomialFeatures(include_bias=False)\nadd_engine.fit(x)\ntrain_data_x = add_engine.transform(x)\ntrain_data_x = pd.DataFrame(train_data_x)\nx = train_data_x.loc[:, ~train_data_x.columns.duplicated()]\n        \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=1-0.8, random_state=0)\nmodels={'Logistic Regression':LogisticRegression(),'KNN':KNeighborsClassifier(),'RandomForest':RandomForestClassifier()}\ndef fit_and_plot(models,x_train,x_test,y_train,y_test):\n    model_scores={}\n#    for name,model in models.items():\n#        model.fit(x_train,y_train)\n#        model_scores[name]=model.score(x_test,y_test)\n    return model_scores\n#scores=fit_and_plot(models,x_train,x_test,y_train,y_test)\n#scores\ntrain_scores=[]\ntest_scores=[]\nknn=KNeighborsClassifier()\nneighbors=range(1,41)\n#for i in neighbors:\n#    knn.fit(x_train,y_train)\n#    knn.set_params(n_neighbors=i)\n#    train_scores.append(knn.score(x_train,y_train))\n#    test_scores.append(knn.score(x_test,y_test))\n#plt.plot(neighbors,train_scores,label='train',color='orange')\n#plt.plot(neighbors,test_scores,label='test',color='darkblue')\n#plt.legend()\n#plt.show()\n#print(f'Test maximum score:{max(test_scores)*100:.2f}%')\nlog_grid={'C':np.logspace(-4,4,20),'solver':['liblinear']}\nrandom_grid={'n_estimators':[50,100,150],'max_depth':[None,5,10],'max_features':['auto','sqrt'],'min_samples_split':[2,4,6],'min_samples_leaf':[1,2,3]}\nrs_log_search=RandomizedSearchCV(LogisticRegression(),param_distributions=log_grid,n_iter=5,cv=5,verbose=True)\nrs_random_search=RandomizedSearchCV(RandomForestClassifier(),param_distributions=random_grid,n_iter=5,cv=5,verbose=True)\n#rs_log_search.fit(x_train,y_train)\n#rs_random_search.fit(x_train,y_train)\n#print(rs_log_search.score(x_test,y_test))\n#print(rs_random_search.score(x_test,y_test))\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n#print(\"start running model training........\")\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore = accuracy_score(y_test, y_pred)\nimport numpy as np\nnp.save(\"haipipe/core/tmpdata/merge_max_result_rl/kulchorobeknazarov_heart-disease-prediction/20.npy\", { \"accuracy_score\": score })\n\n", "validation_code": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,roc_curve,classification_report,roc_auc_score\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\ndf=pd.read_csv('../input/heart-patients/US_Heart_Patients.csv')\ndf.describe()\ndf.isnull().sum()\nno_m_v=df.dropna(axis=0)\ndata_w_d=pd.get_dummies(no_m_v,drop_first=True)\ndata_w_d.columns\ny=data_w_d['TenYearCHD']\nx=data_w_d.drop(['TenYearCHD'],axis=1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport pandas as pd\nimport numpy as np\n        \nx = pd.DataFrame(x).reset_index(drop=True).infer_objects()\nadd_engine = PolynomialFeatures(include_bias=False)\nadd_engine.fit(x)\ntrain_data_x = add_engine.transform(x)\ntrain_data_x = pd.DataFrame(train_data_x)\nx = train_data_x.loc[:, ~train_data_x.columns.duplicated()]\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=1-0.8, random_state=0)\nmodels={'Logistic Regression':LogisticRegression(),'KNN':KNeighborsClassifier(),'RandomForest':RandomForestClassifier()}\ndef fit_and_plot(models,x_train,x_test,y_train,y_test):\n    model_scores={}\n#    for name,model in models.items():\n#        model.fit(x_train,y_train)\n#        model_scores[name]=model.score(x_test,y_test)\n    return model_scores\n#scores=fit_and_plot(models,x_train,x_test,y_train,y_test)\n#scores\ntrain_scores=[]\ntest_scores=[]\nknn=KNeighborsClassifier()\nneighbors=range(1,41)\n#for i in neighbors:\n#    knn.fit(x_train,y_train)\n#    knn.set_params(n_neighbors=i)\n#    train_scores.append(knn.score(x_train,y_train))\n#    test_scores.append(knn.score(x_test,y_test))\n#plt.plot(neighbors,train_scores,label='train',color='orange')\n#plt.plot(neighbors,test_scores,label='test',color='darkblue')\n#plt.legend()\n#plt.show()\n#print(f'Test maximum score:{max(test_scores)*100:.2f}%')\nlog_grid={'C':np.logspace(-4,4,20),'solver':['liblinear']}\nrandom_grid={'n_estimators':[50,100,150],'max_depth':[None,5,10],'max_features':['auto','sqrt'],'min_samples_split':[2,4,6],'min_samples_leaf':[1,2,3]}\nrs_log_search=RandomizedSearchCV(LogisticRegression(),param_distributions=log_grid,n_iter=5,cv=5,verbose=True)\nrs_random_search=RandomizedSearchCV(RandomForestClassifier(),param_distributions=random_grid,n_iter=5,cv=5,verbose=True)\n#rs_log_search.fit(x_train,y_train)\n#rs_random_search.fit(x_train,y_train)\n#print(rs_log_search.score(x_test,y_test))\n#print(rs_random_search.score(x_test,y_test))\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n#print(\"start running model training........\")\nmodel = RandomForestClassifier(random_state=0)\n#model.fit(x_train, y_train)\n#y_pred = model.predict(x_validation_varible)\n#score = accuracy_score(y_validation_varible, y_pred)\nfrom sklearn.model_selection import cross_val_score\ncross_score = cross_val_score(model, x_train, y_train,cv=4)\nimport numpy as np\n#np.save(\"haipipe/core/tmpdata/merge_max_result_rl/kulchorobeknazarov_heart-disease-prediction/20.npy\", { \"accuracy_score\": score })\nnp.save(\"haipipe/core/tmpdata/rl_cross_val_res/kulchorobeknazarov_heart-disease-prediction/20.npy\", { \"accuracy_score\": cross_score })\n\n\n"}