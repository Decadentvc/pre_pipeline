{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this data set we have the data of Telccom Customers.Based on the dataset we will use machine learning to predict who will leave or who will stay,Predicting the churn could be useful for a company to do targeted marketing to ensure that customers dont leave the company.This kernel is work in process.If you like my work please do vote."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Python Modules"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing the data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring the Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.SeniorCitizen.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.tenure.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"len(df.MonthlyCharges.unique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Summary of Dataset \nprint('Rows     :',df.shape[0])\nprint('Columns  :',df.shape[1])\nprint('\\nFeatures :\\n     :',df.columns.tolist())\nprint('\\nMissing values    :',df.isnull().values.sum())\nprint('\\nUnique values :  \\n',df.nunique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df[\"Churn\"].value_counts(sort = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected this is not a balanced data.We have less customer churn compared to retebtion."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Creating a copy of the data \ndf_copy = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In case we want to see the real data it good to have a back up if the data with us."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_copy.drop(['customerID','MonthlyCharges','TotalCharges','tenure'],axis=1,inplace = True)\ndf_copy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Summarizing the Churn Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary = pd.concat([pd.crosstab(df_copy[x],df_copy.Churn) for x in df_copy.columns[:-1]], keys= df_copy.columns[:-1])\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above table we can see the inlfuence of each categorical variable on the customer churn from the telecon company."},{"metadata":{},"cell_type":"markdown","source":"#### Lets gets percentage summary"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"summary['Churn_Percentage'] = summary['Yes']*100/(summary['No'] + summary['Yes'])\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the percentage churn we can take decision on where to focus to prevent or reduce the churn of customers."},{"metadata":{},"cell_type":"markdown","source":"## Vizualizing the data"},{"metadata":{},"cell_type":"markdown","source":"### Churn Percentage"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from pylab import rcParams # For customizing the plots\n\n# Data to plot\nlabels = df['Churn'].value_counts(sort= True).index\nsizes = df['Churn'].value_counts(sort = True)\n\ncolors = [\"lightgreen\",\"red\"]\nexplode = (0.05,0)  # explode first slize\n\nrcParams['figure.figsize'] = 7,7\n\n#plot\nplt.pie(sizes, explode = explode,labels = labels,colors=colors,autopct='%1.1f%%',shadow = True,startangle=90)\n\nplt.title('Customer churn breakdown')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Effect of Monthly Charges on Churn"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x='Churn',y=\"MonthlyCharges\",data=df,kind=\"violin\",palette=\"spring\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can very clearly see that the customers who have churned the most pay high monthy charges.So we need to pay more attention to high paying customers to reduce churn."},{"metadata":{},"cell_type":"markdown","source":"### Effect of tenure on Churn"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"g = sns.factorplot(x='Churn',y=\"tenure\",data=df,kind=\"violin\",palette=\"spring\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can very clearly see that churn is higher when the customer is new.So we have to take care during initial tenure to see that the churn is low."},{"metadata":{},"cell_type":"markdown","source":"### Preparing the data for Machine Learning Classifier"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Removing blank space in our date \nlen(df[df[\"TotalCharges\"] == \"\"])\ndf = df[df[\"TotalCharges\"] != \" \"]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Dropping missing values\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# Customer id col\nId_col = ['customerID']\n\n#Target columns\ntarget_col = [\"Churn\"]\n\n#categorical columns \ncat_cols = df.nunique()[df.nunique() < 6].keys().tolist()\ncat_cols = [x for x in cat_cols if x not in target_col]\n\n#numerical columns\nnum_cols = [x for x in df.columns if x not in cat_cols + target_col + Id_col]\n\n#Binary columns with 2 values\nbin_cols = df.nunique()[df.nunique() == 2].keys().tolist()\n\n#Columns more than 2 values \nmulti_cols = [i for i in cat_cols if i not in bin_cols]       \n\n#Label encoding Binary columns \nle = LabelEncoder()\nfor i in bin_cols :\n    df[i] = le.fit_transform(df[i])\n\n#Duplicating columns for multi value columns\ndf = pd.get_dummies(data = df ,columns = multi_cols)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"len(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"num_cols","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scaling Numerical columns \nstd = StandardScaler()\n\n# Scale data\nscaled = std.fit_transform(df[num_cols])\nscaled = pd.DataFrame(scaled,columns= num_cols)\n\n#dropping original values merging scaled values for numerical values \ndf_telecom_og = df.copy()\ndf = df.drop(columns = num_cols,axis =1)\ndf = df.merge(scaled,left_index = True,right_index=True,how='left')\n\n#df.info()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now with the above lines of code we have managed to scale the numerical values and keep the encoded features as it is.To see the effect of scalling check to the right sode of our dataframe"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.drop([\"customerID\"],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Double check that nulls been removed \ndf[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We remove the label values from our training data\nX = df.drop(['Churn'],axis=1).values\n\n# We have to get the matrix of target variables\ny = df[\"Churn\"].values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Spliting the dataset\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train = pd.DataFrame(X_train)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"type(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(len(df.columns))\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\n\npredictions = model.predict(X_test)\nscore = model.score(X_test,y_test)\n\nprint(\"Accuracy =\" + str(score))\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature importance using Logistic Regression"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# We will be trying the find the parameters which are most important for our prediction\ncoef = model.coef_[0]\ncoef = [abs(number) for number in coef]\nprint(coef)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Finding and deleting the label columns\ncols = list(df.columns)\ncols.index(\"Churn\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del cols[6]\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sorting on Feature Importance \nsorted_index = sorted(range(len(coef)),key = lambda k:coef[k],reverse = True)\nfor idx in sorted_index:\n    print(cols[idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above mentioned features are as per their importance to predict the churn."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train,y_train)\n\npredictions = model_rf.predict(X_test)\nscore = model.score(X_test,y_test)\n\nprint(\"Accuracy =\" + str(score))\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont get much change in accuracy for Random Forest and Logistic Regression,We also need to consider the Precision and Recall Values to have more accurate idea of the model.For the case of Churn our models are not Performing well.This will need feature engineering and fine tuning of the model to improve the model Performance"},{"metadata":{},"cell_type":"markdown","source":"### Saving the Model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pickle \n\n#Save\nwith open('model.pkl','wb') as f:\n    pickle.dump(model_rf,f)\n    \n#Load \nwith open('model.pkl','rb') as f:\n    loaded_model_rf = pickle.load(f)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}