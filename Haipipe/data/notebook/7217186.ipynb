{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis project was done purely with educational purposes. I wanted to try and implement several classifying algorithms with parameter tweaking on a dataset that seemed very interesting to me. When looking at the dataset, one of the suggestions that the publisher had was trying to predict the attitudes towards mental health within a workplace, and that is what I decided to look into. The survey did not have an explicit question about the attitudes, that is why I decided to treat the question \"Do you think that discussing a mental health issue with your employer would have negative consequences?\" as \"Is the attitude within your workplace negative?\" \n\nI decided that I would implement the algorithms in a binary setting where I got rid of 'Maybe' in the answers to the question and then add it back in for a multiclass setting.\n\nOverall, this was a great learning experience. The best accuracy rate from binary classifiers was around 90% and from multiclass classifiers was around 63%. Initially, I was assuming that when adding back 'Maybe,' I would get an accuracy rate of 50%. I was surprised to see that I could push it till 60%. \n\nOne of the implications of this model is seeing that talking with supervisors and coworkers about mental health is really important. Even then, it is important to recognize that, essentially, supervisors are the ones who set the environment. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2\n\ndata = pd.read_csv('/kaggle/input/mental-health-in-tech-survey/survey.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"I decided to get rid of countries with values less than 10, since it seemed that having those countries would have a significant impact on the models."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = data.drop(columns=['self_employed'])\ndata = data[data.Country.map(data.Country.value_counts()) >= 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I created two versions of X and y, where X2 and y2 are the data for a binary classifier.\n\nI also got rid of columns that I thought were unnecessary to the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data[data.mental_health_consequence != 'Maybe']\n\ny = data.mental_health_consequence\ny2 = data2.mental_health_consequence\n\ndata = data.drop(columns=['mental_health_consequence', 'Timestamp', 'state','comments', 'Age', 'Gender'])\ndata2 = data2.drop(columns=['mental_health_consequence', 'Timestamp', 'state','comments', 'Age', 'Gender'])\n\nX = data\nX2 = data2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(3,2,figsize=(10,10))\n\nX2.Country.value_counts().plot(kind='bar', ax=ax[0,0])\nax[0,0].set_title('Frequency of Countries')\n\ny.value_counts().plot(kind='bar', ax=ax[0,1])\nax[0,1].set_title('Do you think that discussing a mental health issue \\n with your employer would have negative consequences?')\n\nX2.supervisor.value_counts().plot(kind='bar', ax=ax[1,0])\nax[1,0].set_title('Would you be willing to discuss a mental health issue \\n with your direct supervisor(s)?')\n\nX2.coworkers.value_counts().plot(kind='bar', ax=ax[1,1])\nax[1,1].set_title('Would you be willing to discuss a mental health issue \\n with your coworkers?')\n\nX2.obs_consequence.value_counts().plot(kind='bar', ax=ax[2,0])\nax[2,0].set_title('Have you heard of or observed negative consequences for coworkers \\n with mental health conditions in your workplace?')\nax[2,0].title.set_size(10)\n\nX2.anonymity.value_counts().plot(kind='bar', ax=ax[2,1])\nax[2,1].set_title('Is your anonymity protected if you choose to take advantage of \\n mental health or substance abuse treatment resources?')\nax[2,1].title.set_size(10)\n\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict1 = {'No':0,\n            'Maybe':1,\n             'Yes':2\n            }\ny = y.map(dict1)\n\ndict2 = {'No':0,\n            'Yes':1,\n            }\ny2 = y2.map(dict2)\n\nX.work_interfere = X.work_interfere.fillna('Never')\nencoder_dict = defaultdict(LabelEncoder)\nX = X.apply(lambda a: encoder_dict[a.name].fit_transform(a))\nX2 = X2.apply(lambda a: encoder_dict[a.name].fit_transform(a.astype(str)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n## Binary Version\nI found this code ([here](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)) when looking for information about feature selection and decided to use it. This shows all the features from the most impactful to least. I then decided to get rid of features with a score less than 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=20)\nfit = bestfeatures.fit(X2,y2)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(20,'Score'))  #print 20 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = X2.drop(columns=featureScores.Feature[featureScores.Score < 10].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.25, random_state=1)\nprint('Train', X_train.shape, y_train.shape)\nprint('Test', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = OneVsRestClassifier(SVC())\nmodel.fit(X_train, y_train)\npredict = model.predict(X_test)\nacc1 = accuracy_score(y_test, predict)\n\nprint(confusion_matrix(y_test, predict))\nprint('Accuracy: ', acc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = OneVsOneClassifier(LinearSVC())\nmodel.fit(X_train, y_train)\ny_pred1 = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred1)\n\nprint(confusion_matrix(y_test, y_pred1))\nprint('Accuracy: ', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearSVC(random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy: ', acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tree Based Classifiers\nWhen looking for information about encoding categorical values, I found a chart that recommended using one hot encoder for tree based algorithms. Hence why I am getting dummy categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate binary values using get_dummies\nXdum = pd.get_dummies(X2, columns=X2.columns )\n\nX_train, X_test, y_train, y_test = train_test_split(Xdum, y2, test_size=0.25, random_state=1)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 15, 15, endpoint=True)\naccs = []\nfor maxd in max_depths:\n    model = DecisionTreeClassifier(max_depth = maxd)\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    accs.append(accuracy_score(y_test, pred))\n\nprint('Best max_depth: ', max_depths[accs.index(max(accs))])\nprint('Accuracy with best max_depth: ', max(accs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(Xdum, y2, test_size=0.25, random_state=1)\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = np.linspace(1, 200, 10, endpoint=True, dtype='int')\naccs_est = []\nfor n in estimators:\n    model = RandomForestClassifier(n_estimators=n)\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    accs_est.append(accuracy_score(y_test, pred))\n    \nprint('Best estimator: ', estimators[accs_est.index(max(accs_est))])\nprint('Accuracy with best estimator: ', max(accs_est))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 32, 32, endpoint=True)\naccs_maxd = []\nfor n in max_depths:\n    model = RandomForestClassifier(max_depth=n, n_estimators = estimators[accs_est.index(max(accs_est))])\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    accs_maxd.append(accuracy_score(y_test, pred))\n    \nprint('Best max_depth: ', max_depths[accs_maxd.index(max(accs_maxd))])\nprint('Accuracy with best estimator and max_depth: ', max(accs_maxd))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiclass Version"},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=20)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(20,'Score'))  #print 20 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(columns=featureScores.Feature[featureScores.Score < 10].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\nprint('Train', X_train.shape, y_train.shape)\nprint('Test', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = OneVsRestClassifier(SVC())\nmodel.fit(X_train, y_train)\npredict = model.predict(X_test)\nacc5 = accuracy_score(y_test, predict)\n\nprint(confusion_matrix(y_test, predict))\nprint('Accuracy: ', acc5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = OneVsOneClassifier(LinearSVC())\nmodel.fit(X_train, y_train)\ny_pred1 = model.predict(X_test)\nacc7 = accuracy_score(y_test, y_pred1)\n\nprint(confusion_matrix(y_test, y_pred1))\nprint('Accuracy: ', acc7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearSVC(random_state=0)\nmodel.fit(X_train, y_train)\ny_pred2 = model.predict(X_test)\nacc4 = accuracy_score(y_test, y_pred2)\n\nprint(confusion_matrix(y_test, y_pred2))\nprint('Accuracy: ', acc4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tree Based Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pr = model.predict(X_test)\na = accuracy_score(y_test, y_pr)\n\nprint(confusion_matrix(y_test, y_pr))\nprint('Accuracy:', a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 15, 15, endpoint=True)\naccs = []\nfor maxd in max_depths:\n    model = DecisionTreeClassifier(max_depth = maxd)\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    accs.append(accuracy_score(y_test, pred))\n\nprint('Best max_depth: ', max_depths[accs.index(max(accs))])\nprint('Accuracy with best max_depth: ', max(accs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = np.linspace(1, 200, 10, endpoint=True, dtype='int')\naccs_est = []\nfor n in estimators:\n    model = RandomForestClassifier(n_estimators=n)\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    accs_est.append(accuracy_score(y_test, pred))\n    \nprint('Best estimator: ', estimators[accs_est.index(max(accs_est))])\nprint('Accuracy with best estimator: ', max(accs_est))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 32, 32, endpoint=True)\naccs_maxd = []\nfor n in max_depths:\n    model = RandomForestClassifier(max_depth=n, n_estimators = estimators[accs_est.index(max(accs_est))])\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    accs_maxd.append(accuracy_score(y_test, pred))\n    \nprint('Best max_depth: ', max_depths[accs_maxd.index(max(accs_maxd))])\nprint('Accuracy with best estimator and max_depth: ', max(accs_maxd))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}