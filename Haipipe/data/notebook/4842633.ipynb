{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In this dataset, we are going to talk about the falling of elderly people and see how we can prevent this\n"},{"metadata":{},"cell_type":"markdown","source":"To start off, we need to import the dataset and take a look at it"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we already have numpy and pandas loaded, we will need to load the others that we are going to need","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detectfall = pd.read_csv(r\"../input/falldeteciton.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us look at the data now\nprint(detectfall.head())\n\n# If you prefer another amount than the default, you can customized it\nprint(detectfall.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can see I have provided two different printing and got two different answers. This is how you can customize how many rows you would like to see","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DESCRIPTIVE STATISTICS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now take a further look at the data and inspect the distribution\ndetectfall.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detectfall.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check to see if there are any null values. It is good to check for this\ndetectfall.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now it is plotting time"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['TIME', 'SL', 'EEG', 'BP', 'HR', 'CIRCLUATION']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at some plots now\nfig = plt.figure(figsize = (10, 20)) # (Breite, Lange)\nfor i in range (0, len(cols)):\n    fig.add_subplot(len(cols), 1, i+1)\n    sns.distplot(detectfall[cols[i]]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot\nsns.boxplot(data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It looks like we have some heavy outliers in the EEG column. Since it is only a few value, we need to cut them out of the distribution. Also, SL has some outliers that need to be removed\ndetectfall = detectfall[(detectfall['EEG'] < detectfall['EEG'].quantile(0.999) ) \n& (detectfall['EEG'] > detectfall['EEG'].quantile(0.001))]\n\n# For SL\ndetectfall = detectfall[(detectfall['SL'] < detectfall['SL'].quantile(0.999) ) \n& (detectfall['SL'] > detectfall['SL'].quantile(0.001))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us look at another boxplot of detectfall\nsns.boxplot(data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we start with the regression, we should take a time to look at what a picture of the data looks like. \nsns.lmplot('TIME', 'HR', data = detectfall,\n          palette='Set1', fit_reg=False, scatter_kws={\"s\": 70})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We could as well do a joint plot with seaborn to gather both histogram and scatter plot. This is one of the thing I like with seaborn. It gives you more option than matplotlib.\nsns.jointplot('TIME', 'HR', detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another great feature that comes with seaborn is the heatmap plot. It allows you to see a plot of the correlation that each attribute has with each other. So here, I am sure you can see that the darker an attribute is in comparison with another, the least like there is any correlation between these two attributes. Just as in statistics, the closer the probability is to 1, the more there is a correlation; if it is closer to 0, the least likelyhood there is a correlation. So, this heatmap from seaborn just basically answered a few questions we may have had. This is very cool!!!\nsns.heatmap(detectfall.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this heatmap, the lighter the square, the higher the correlation; the darker the square, the  less they are correlated"},{"metadata":{},"cell_type":"markdown","source":"# MACHINE LEARNING 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Classifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is time to split the data and create a training and testing set\ntarget = detectfall['ACTIVITY']\nattribute = detectfall[['TIME','SL','EEG','BP','HR','CIRCLUATION']]\n\n# Training and test sets\nX_train, X_test, y_train, y_test = train_test_split(attribute, target, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are going to test some classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our first Classifiers testing is the Logistic Regression\nLogR = LogisticRegression()\nLogR.fit (X_train, y_train)\npred = LogR.predict(X_test)\nacc = accuracy_score(y_test, pred)\nacc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It is not good to use logistic regression for this dataset, since the target value is not a DUMMY variable such as 1 or 0. So, we cannot use Logistic Regression for this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you take a look at the answer given, we see that we cannot do a logistic regression because the target value is not between 0 and 1. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we are going to do a decision tree\ndtcfall = DecisionTreeClassifier()\ndtcfall.fit(X_train, y_train)\ndtcfallpred = dtcfall.predict(X_test)\ndtcfallacc = accuracy_score(y_test, dtcfallpred)\ndtcfallacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us move on to the Random Forest Classifier\nrfcfall = RandomForestClassifier()\nrfcfall.fit(X_train, y_train)\nrfcfallpred = rfcfall.predict(X_test)\nrfcfallacc = accuracy_score(rfcfallpred, y_test)\nrfcfallacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try the KNN classifier\nknnfall = KNeighborsClassifier()\nknnfall.fit(X_train, y_train)\nknnfallpred = knnfall.predict(X_test)\nknnfallacc = accuracy_score(knnfallpred, y_test)\nknnfallacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our final classifier is the Support Vector Machine\nsvcfall = SVC(gamma='auto')\nsvcfall.fit(X_train, y_train)\nsvcfallpred = svcfall.predict(X_test)\nsvcfallacc = accuracy_score(svcfallpred,y_test)\nsvcfallacc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So far, the best model we have is the Random Forest Model"},{"metadata":{},"cell_type":"markdown","source":"# Maybe we need to do some grid search to find the best model for this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search Parameter Tuning\nalphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0])\n\n# Create and fit a ridge regression model, testing each alpha\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nmodel = Ridge()\ngrid = GridSearchCV(estimator=model, param_grid=dict(alpha = alphas))\ngrid.fit(attribute, target)\nprint(grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us try and randomized search\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_rand\n# Prepare a uniform distribution to sample for the alpha parameter\nparamgrid = {'alpha': sp_rand()}\n\n# Create and fit a ridge regression model, testing random alpha values\nmodel = Ridge()\nrsearch = RandomizedSearchCV(estimator = model, param_distributions= paramgrid, n_iter=1000)\nrsearch.fit(attribute, target)\nprint(rsearch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize the results of the random parameter search\nprint(rsearch.best_score_)\nprint(rsearch.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to make this regression simpler; and the way to do it is to keep the attribute that contribute more to the falling of elderly people\nplt.scatter('ACTIVITY', 'EEG', c = 'red', data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter('ACTIVITY', 'TIME', c = 'blue', data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter('ACTIVITY', 'SL', c = 'black', data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter('ACTIVITY', 'HR', c = 'yellow', data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter('ACTIVITY', 'BP', c = 'green', data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter('ACTIVITY', 'CIRCLUATION', c = 'purple', data = detectfall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So far, I think that Standing is the primary cause of the elderly people falling since it has the higher rate with almost all the other attribute of the dataset. \n# Please feel free to comment and bring your suggestions. Thank you"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}