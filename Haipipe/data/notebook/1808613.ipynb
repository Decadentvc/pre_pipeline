{"cells":[{"metadata":{"_uuid":"8b69d0ced55871e9807811c42136126151e8b709"},"cell_type":"markdown","source":"# **5ED3S Mini Project #9**\n## Motivation\nI have been working on this kernel as part of the 5th European Data Science Summer School. In addition to developing a classification model for the dataset, the aim is also to develop a business model whose core element is the solution to the machine learning problem.\n\nThe problem is to classify whether a person earns more or less than 50k. As data source the [UCI Adult Data Set](http://archive.ics.uci.edu/ml/datasets/Adult) is used which is also available for import on Kaggle.\n\nThis kernel is structured in five parts:\n1. Gathering Data\n2. Data Exploration\n3. Developing a Business Modell\n4. Data Cleaning\n5. Model the Data\n6. Conclusion"},{"metadata":{"_uuid":"44f8c975f9d068993194bcc7fb0ccf3fc848e163"},"cell_type":"markdown","source":"# 1. Gathering Data\n## 1.1 The Dataset\n\nThe dataset was created in 1994 by Barry Becker. It is an extraction of the census database of the [United States Census Bureus](https://www.census.gov/en.html) from 1994. Together with Ronny Kohavi, Becker published the dataset in 1996."},{"metadata":{"_uuid":"7edb47ef0cde365f1c697f640a3fbc21ccef6a7e"},"cell_type":"markdown","source":"## 1.2 Get the Data\nLet's start with checking the pyhton version and importing the data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"#Start with importing system and operating system libraries\nimport sys\nimport os\n\n#Suppress warnings, this will not affect the result\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n#Check python version\nprint('Python version:',sys.version)\n\n#Import pandas and numpy\nimport pandas as pd\nimport numpy as np\n\n#Load the data set\ndata = pd.read_csv('../input/adult.csv', sep=',')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f504ef447b74e3dd018b73b02841aeea8f69e6b5"},"cell_type":"markdown","source":"# 2. Data Exploration\nAs seen above, there are already some missing values in the first rows of the data set. So let's look at the relative amount of missing data in the dataset and see if it has an impact."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#Initialize an empty array to collect the sum of missing values per column\nmvs = []\n\n#Count the missing values\nfor x in data.columns:\n    mvs.append(data[x].isin([\"?\"]).sum())\n\n#Build the plot\nfig, ax =  plt.subplots(figsize=(10,3))\nindex   = np.arange(data.shape[1])\n\nax.bar(index, mvs, alpha = 0.4, color = 'b')\nax.set_ylabel('Missing Values')\nax.set_xticks(index)\nax.set_xticklabels((data.columns))\n\nfig.tight_layout()\n\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4f04d197b7368dfb5356d168df8227e692facc"},"cell_type":"code","source":"#Only three features contain missing values\n#To see if the missing values have an significant effect on the dataset\n#Visualize the missing values compared with the complete dataset to see the effect\n\n#Build the plot\nyvalues = [data.shape[0], mvs[1], mvs[6], mvs[13]]\n\nfig, ax = plt.subplots()\nindex   = np.arange(4)\n\nax.bar(index, yvalues, alpha = 0.4, color = 'b')\nax.set_ylabel('Data')\nax.set_xticks(index)\nax.set_xticklabels(('data set size','workclass','occupation','native.country'))\n\nfig.tight_layout()\n\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec5495c832ae89aabd17e9f677dac6ee7beb8a12"},"cell_type":"markdown","source":"In addition to the structural properties of the dataset, we also want to know distributions within the data. What is the average age and what is the age structure like? Are there more men or women covered by the dataset? How many hours do people work on average? What are the top 5 native countries? And how is the classification variable, income, distributed?"},{"metadata":{"trusted":true,"_uuid":"8d17dcf6ce7dacce9f80b5f62026befab6fc9680"},"cell_type":"code","source":"#Question 1: Age\n\n#Set up a histogram\nplt.hist(data.age, facecolor='green', alpha=0.5, bins=18, edgecolor='black')\nplt.xlabel('Age')\nplt.axvline(data.age.mean(), color='red', label='average age')\nplt.axis([15, 95, 0, 3500])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f55ef6fc50cfd1a162273e800b591fa16c7d084"},"cell_type":"markdown","source":"The histogram acts as expected. The number of employees rises sharply at the beginning of the graph until it reaches its maximum between 30 and 45 years. This is also where the average age of 38 is. After that, the graph falls slowly. What is noticeable is that towards the end there is growth again."},{"metadata":{"trusted":true,"_uuid":"0bff4677e5556b98a6e968c3ae02ecbdc16ef6b1"},"cell_type":"code","source":"#Question 2: Gender\n\n#Count male and female\nm = 0\nf = 0\nfor g in data.sex:\n    if g == 'Male':\n        m += 1\n    if g == 'Female':\n        f += 1\n\n#Set up pie chart\ncolors = ['lightskyblue', 'lightcoral']\nvalues = [m, f] \nlabels = ['Male', 'Female'] \nplt.pie(values, labels=labels, colors=colors, shadow=True, startangle=90, autopct='%.2f')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eed7b3cb1adc51606c251a6738d8bd42e2233a5d"},"cell_type":"markdown","source":"The graph shows that more than twice as many men are represented in this dataset than women."},{"metadata":{"trusted":true,"_uuid":"d4635a80dd63938f8b2043613b8457966a717fb1"},"cell_type":"code","source":"#Question 3: Hours per Week\n\nprint('Mean:', data['hours.per.week'].mean())\n#Set up a histogram\nplt.hist(data['hours.per.week'], facecolor='green', alpha=0.5, bins=18, edgecolor='black')\nplt.xlabel('Age')\nplt.axvline(data['hours.per.week'].mean(), color='red', label='average hour per week')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29a46b1da82c254590f687e2e51ee9f2942ac72e"},"cell_type":"markdown","source":"This statistic was to be expected. In the dataset there is a big peak of 40 hours per week. This is also proven by the average time of 40.43 hours per week. The increase of almost 1000 employees working in a 50 hour week should also be noted."},{"metadata":{"trusted":true,"_uuid":"38c9ab2ed67db41ff10a3ebdc330333705af7fb0"},"cell_type":"code","source":"#Question 4: Native Country\n\n#Count the countries \nf = data['native.country'].value_counts()\n\n#Show the top 5\nf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dce6a92e4b843014c86d33122098b6981bfbe63"},"cell_type":"markdown","source":"About 90% of all persons come from the USA. This is due to the fact that the data was taken from the database of the U.S. Census Bureau, so it was a census in the US."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"5be216688950464018b99a0dc7fd040ffd76029d"},"cell_type":"code","source":"#Question 5: Income\n\n#Count how often people earn more and less than 50k\nm = 0\nl = 0\nfor i in data.income:\n    if i == '<=50K':\n        l += 1\n    if i == '>50K':\n        m += 1\n\n#Set up pie chart\ncolors = ['lightskyblue', 'lightcoral']\nvalues = [l, m] \nlabels = ['<=50k', '>50k'] \nplt.pie(values, labels=labels, colors=colors, shadow=True, startangle=90, autopct='%.2f')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24a3d3f6a4298ef9a8ed0865bdd0c39fb45ec91c"},"cell_type":"markdown","source":"About three quarters of employees earn less than 50k. This figure will also be important during the evaluation of our prediction model. To be usable, it must be significantly better than 75%."},{"metadata":{"_uuid":"7cd8c9af962b54c35a6a97c65968d6eee8772612"},"cell_type":"markdown","source":"# 3. Developing a Business Model\n\nNow that you have gained an insight into the data set, you can begin to define the problem. Several Questions have to be asked. Which problem should the classification model solve? Why is it relevant to solve the problem? For whom is the solution important?\n\nBefore we devote ourselves to the benefits of problem solving, we first clarify for whom the problem will be important. This question is very easy to answer because it is an US American dataset. The information in the dataset only concerns the American market, which we were able to show in part with the distribution of the native countries. A special feature of the US market is the handling of money. Or rather, dealing with debt. In contrast to other markets, such as the German-speaking countries, credits in the US are not so ridiculed and are often the only means. An example of this would be the indebtedness that the majority of students have to afford because they would otherwise not be able to pay the high tuition fees. \n\nThat is why credit institutions often have to ask themselves whether someone is creditworthy. And here the classification model should be a solution. By recording simple personal data, employees, without a statistical background, should be able to determine the creditworthiness of individual persons.\n\nNow that the problem has been defined, you can proceed and take the next step developing a model."},{"metadata":{"_uuid":"23555ca4850414fdd8cabc2123983bac265ca30c"},"cell_type":"markdown","source":"# 4. Data Cleaning\n\nData cleaning is about simplifying the data. A data model is to be created which enables a machine learning algorithm to process the information optimally. For this purpose, it should be avoided to leave unnecessary data and information redundancies in the dataset.\n\nBut before we take a closer look at the individual features, we first go into the structural condition of the data. As seen above, a small part of the data has missing values. Due to the fact there is only a low percentage of missing values in the data, we can delete this rows."},{"metadata":{"trusted":true,"_uuid":"2af1275d8af3ba6cf845dcffc3a02c2e4ccb4955"},"cell_type":"code","source":"print('Number of rows with missing values:', data.shape[0],'\\n')\n\n#Save the number of size of the dataset before deleting the missing values\nnumRows = data.shape[0]\n\n#Check if there are missing values in the dataset and delete the data if so\nfor x in data.columns:\n    mv = data[x].isin([\"?\"]).sum()\n    if mv > 0:\n        data = data[data[x] != '?']\n\nprint('Number of rows without missing values:',data.shape[0])\nprint('We dropped of',numRows - data.shape[0],'rows')\n\n#Plot a pie chart to visualize the result\ncolors = ['lightskyblue', 'lightcoral']\npatches, texts = plt.pie([data.shape[0], numRows - data.shape[0]], colors=colors, shadow=True, startangle=90)\nplt.legend(patches, ['Number of Rows', 'Number of dropped Rows'], loc=\"best\")\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa96ff25023af93beb0ee7017aaca5e700f0d1e2"},"cell_type":"markdown","source":"After deleting about 2400 rows containing missing values, the data set is still big enough to build a valid prediction model on it.\n\n## 4.1 Feature Tuning\nIn the next step we want to look on the attributes of each feature and identify if there are similar attributes which we can combine."},{"metadata":{"_uuid":"94af4298dc37704db7d1eba092c12e3e125c80e0","trusted":true,"scrolled":true},"cell_type":"code","source":"#Print the attributes of workclass an their occurrence.\nf = data['workclass'].value_counts().reset_index()\nf.columns = ['workclass', 'count']\nprint(f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a23a905ea6dfe9ec42ba8e04d5745d07d4d7451"},"cell_type":"markdown","source":"Due to similarity we can combine *Self-emp-not-inc* and *Self-emp-inc* to one attribute as well as *Local-gov*, *State-gov* and *Federal-gov*.  Instead of 5, we now have 2 new attributes - *Self-emp* and *Gov*."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b51bfcaf9b5db1f3ac3c019846592ea12f35fb05"},"cell_type":"code","source":"#Combine the mentioned attributes\ndata.workclass = data.workclass.replace({'Self-emp-not-inc': 'Self-emp',\n                                        'Self-emp-inc': 'Self-emp',\n                                        'Local-gov': 'Gov',\n                                        'Federal-gov': 'Gov',\n                                        'State-gov': 'Gov'})\n\n#Count all distinct attributes of fnlwgt\nf = data['fnlwgt'].value_counts().reset_index()\nf.columns = ['fnlwgt', 'count']\nprint('Number of distinct attribute in column fnlwgt :',f.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60d217c4fc39703dea3c6213d4fa447935de7a14"},"cell_type":"markdown","source":"Column *fnlwgt* has over 20000 distinct attributes. There are too many for simplification."},{"metadata":{"trusted":true,"_uuid":"7c5430685511ecb9573c0a070c9714cea9f5cf0f"},"cell_type":"code","source":"#Print the attributes of education and their occurrence.\nf = data['education'].value_counts().reset_index()\nf.columns = ['education', 'count']\nprint('\\n',f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8041c98eedf506942fbcbc9756a04e91a403aaab"},"cell_type":"markdown","source":"According to the calculation of *education*, we can combine some features. *Preschool*, *1st-4th*, *5th-6th*, *7th-8th*, *9th*, *10th*, *11th* and *12th* we summarize as *No-school*. *Some-college*, *Assoc-voc* and *Assoc-acdm* can be combined as *College*. "},{"metadata":{"trusted":true,"_uuid":"0148bfcc616b4b3713108d47df38e9521438efc2","scrolled":true},"cell_type":"code","source":"#Combine the mentioned attributes\ndata.education = data.education.replace({'Preschool': 'No-school',\n                                        '1st-4th': 'No-school',\n                                        '5th-6th': 'No-school',\n                                        '7th-8th': 'No-school',\n                                        '9th': 'No-school',\n                                        '10th': 'No-school',\n                                        '11th': 'No-school',\n                                        '12th': 'No-school',\n                                        'Some-college': 'College',\n                                        'Assoc-voc': 'College',\n                                        'Assoc-acdm': 'College'})\n\n#Print the attributes of education an their occurrence\nf = data['marital.status'].value_counts().reset_index()\nf.columns = ['marital.status', 'count']\nprint('\\n',f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aad5920201ecb7dd944737bd6d82586c187df0a7"},"cell_type":"markdown","source":"For reasons of simplicity, we simplify the attributes. *Married-civ-spouse* and *Married-AF-spouse* are combined as *Married*. *Never-married* and *Married-spouse-absent* are combined as *Not-married*. *Divorced* is added to *Separated*. *Widowed* remains untouched."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"de09432003134798ae369edd1420c0deba40dedb"},"cell_type":"code","source":"#Combine the mentioned attributes\ndata['marital.status'].replace(['Married-civ-spouse'], 'Married', inplace=True)\ndata['marital.status'].replace('Never-married', 'Not-married', inplace=True)\ndata['marital.status'].replace(['Divorced'], 'Separated', inplace=True)\ndata['marital.status'].replace(['Separated'], 'Separated', inplace=True)\ndata['marital.status'].replace(['Married-spouse-absent'], 'Not-married', inplace=True)\ndata['marital.status'].replace(['Married-AF-spouse'], 'Married', inplace=True)\n\n#Show the result\nf = data['marital.status'].value_counts().reset_index()\nf.columns = ['marital.status', 'count']\nprint('\\n',f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b643f6f6846278f0e3b439538f3442aa56d7da56"},"cell_type":"markdown","source":"Next we look at *occupation*, *relationship*, *race* and *sex*. "},{"metadata":{"trusted":true,"_uuid":"885f187e9ac40c3b0673c789419b019369ded211"},"cell_type":"code","source":"#Print the attributes of occupation an their occurrence\nf = data['occupation'].value_counts().reset_index()\nf.columns = ['occupation', 'count']\nprint('\\n',f)\n\n#Print the attributes of relationship an their occurrence\nf = data['relationship'].value_counts().reset_index()\nf.columns = ['relationship', 'count']\nprint('\\n',f)\n\n#Print the attributes of race an their occurrence\nf = data['race'].value_counts().reset_index()\nf.columns = ['race', 'count']\nprint('\\n',f)\n\n#Print the attributes of sex an their occurrence\nf = data['sex'].value_counts().reset_index()\nf.columns = ['sex', 'count']\nprint('\\n',f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49f9575ce7d28afda0ae3de9fc54ea4b7711eb9c"},"cell_type":"markdown","source":"*occupation* is one of the important feature in the dataset, so we leave it unchanged.\n*relationship*, *race* and *sex*, all have a moderate number of different attributes, which differ in their information content. This means that we don't have to change these columns.\n\nFinally, we consider the native countries of the people."},{"metadata":{"trusted":true,"_uuid":"0f3c4adc1457799cf035de5f89bf48d46214c436","scrolled":true},"cell_type":"code","source":"#Print the attributes of native.country an their occurrence\nf = data['native.country'].value_counts().reset_index()\nf.columns = ['native.country', 'count']\nprint('\\n',f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1fa7262b38a48a567f98e7295388bbdd8b2c6b9"},"cell_type":"markdown","source":"The *United-States* is significantly ahead of *Mexico* as native country, the second in the list, as we already saw above. The difference between the other countries is not so clear. Because there are more than 40 countries in the list, a grouping by continent makes sense. "},{"metadata":{"_uuid":"4fa8278c7fde3e4e49298d1190c830b66e0f04b9","trusted":true},"cell_type":"code","source":"data['native.country'].replace(['United-States'], 'N-America', inplace=True)\ndata['native.country'].replace(['Mexico'], 'N-America', inplace=True)\ndata['native.country'].replace(['Philippines'], 'Asia', inplace=True)\ndata['native.country'].replace(['Germany'], 'Europe', inplace=True)\ndata['native.country'].replace(['Puerto-Rico'], 'N-America', inplace=True)\ndata['native.country'].replace(['Canada'], 'N-America', inplace=True)\ndata['native.country'].replace(['India'], 'Asia', inplace=True)\ndata['native.country'].replace(['El-Salvador'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Cuba'], 'MS-America', inplace=True)\ndata['native.country'].replace(['England'], 'Europe', inplace=True)\ndata['native.country'].replace(['Jamaica'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Italy'], 'Europe', inplace=True)\n\ndata['native.country'].replace(['China'], 'Asia', inplace=True)\ndata['native.country'].replace(['Dominican-Republic'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Vietnam'], 'Asia', inplace=True)\ndata['native.country'].replace(['Guatemala'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Japan'], 'Asia', inplace=True)\ndata['native.country'].replace(['Columbia'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Poland'], 'Europe', inplace=True)\ndata['native.country'].replace(['Taiwan'], 'Asia', inplace=True)\ndata['native.country'].replace(['Haiti'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Iran'], 'Asia', inplace=True)\ndata['native.country'].replace(['Portugal'], 'Europe', inplace=True)\ndata['native.country'].replace(['Nicaragua'], 'MS-America', inplace=True)\n\ndata['native.country'].replace(['Peru'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Greece'], 'Europe', inplace=True)\ndata['native.country'].replace(['Ecuador'], 'MS-America', inplace=True)\ndata['native.country'].replace(['France'], 'Europe', inplace=True)\ndata['native.country'].replace(['Ireland'], 'Europe', inplace=True)\ndata['native.country'].replace(['Hong'], 'Asia', inplace=True)\ndata['native.country'].replace(['Trinadad&Tobago'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Cambodia'], 'Asia', inplace=True)\ndata['native.country'].replace(['Laos'], 'Asia', inplace=True)\ndata['native.country'].replace(['Thailand'], 'Asia', inplace=True)\ndata['native.country'].replace(['Yugoslavia'], 'Europe', inplace=True)\ndata['native.country'].replace(['Outlying-US(Guam-USVI-etc)'], 'N-America', inplace=True)\n\ndata['native.country'].replace(['Hungary'], 'Europe', inplace=True)\ndata['native.country'].replace(['Honduras'], 'MS-America', inplace=True)\ndata['native.country'].replace(['Scotland'], 'Europe', inplace=True)\ndata['native.country'].replace(['Holand-Netherlands'], 'Europe', inplace=True)\n\n#Show the result\nf = data['native.country'].value_counts().reset_index()\nf.columns = ['native.country', 'count']\nprint('\\n',f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91688ea30c238c0f0ec5305fe922069d712cd400"},"cell_type":"markdown","source":"Now we can look at our dataset again."},{"metadata":{"trusted":true,"_uuid":"016fb3eb97121bd9f63f00dfe99f78d263ad9da0","scrolled":false},"cell_type":"code","source":"print('dataset size')\nprint('# of rows:', data.shape[0])\nprint('# of columns:', data.shape[1])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e62e5f709a1126da51a8d94cf7b4d1737c12c31"},"cell_type":"markdown","source":"The data is simplfied but there are still have 15 columns including the *income* column left. When looking at *education* and *education.num* , you can quickly see that they contain the same information. People with a high level of education have usually invested more years in their education than those with a lower level of education. Therefore, in the following we will only look at one column in our prediction model - *education*.\nFor further simplification, we will also remove the columns *relationship* and *fnlwgt*.  \nThe statement of *fnlwgt*, final weight, is very complex. To understand this, one has to know that the data set of the U.S. Census Bureaus consists of 51 individual state samples, which were collected with different methods. Persons with the same demographic characteristics should have a similar weight. However, this weight should only have significance within a state, so that we cannot take it further into account for the overall analysis ([more information about fnlwgt](http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)). *relationship* contains information that can be partly derived from the data in *gender* and *marital.status*. Redundant information is not needed in our prediction model.  It can bias the prediction. Consequential, *relationship* is also dropped."},{"metadata":{"trusted":true,"_uuid":"7488193be39343a01969504aa4f04e8e1c16564e"},"cell_type":"code","source":"#Drop the columns not needed\ndata.drop(columns=['education.num', 'fnlwgt', 'relationship'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00eeb3016da8a2d69e5c6916a42c71f1ee9313b1"},"cell_type":"markdown","source":"## 4.2 Process the Data\n\nNow that all features are selected, we want to prepare the data. The prediction variable, *income*, is encoded binary, in 0 and 1. Numeric features are scaled with a MinMax method into a number range between 0 and 1. The remaining features are first label encoded and afterwards one hot encoded. Thus, the final data model consists of 47 attributes with a value between 0 and 1."},{"metadata":{"trusted":true,"_uuid":"7a0e7d694194ee0c072d3b2fe877bd8e3196fdbf","scrolled":true},"cell_type":"code","source":"#Import OneHotEncoder, LabelEncoder & MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n\n#Define how to process which feature\ncolumns_to_label_encode   = ['income']\ncolumns_to_scale          = ['age', 'capital.gain', 'capital.loss', 'hours.per.week']\n\n#Instantiate encoder/scaler\nohe    = OneHotEncoder(sparse=False)\nle     = LabelEncoder()\nmms    = MinMaxScaler()\n\n#To one hot encode the string values, they need to be in a numeric format,\n#To do so, we first label encode those features  \nw = np.reshape(le.fit_transform(data['workclass']), (30162, 1))\ne = np.reshape(le.fit_transform(data['education']), (30162, 1))\nm = np.reshape(le.fit_transform(data['marital.status']), (30162, 1))\no = np.reshape(le.fit_transform(data['occupation']), (30162, 1))\nr = np.reshape(le.fit_transform(data['race']), (30162, 1))\ns = np.reshape(le.fit_transform(data['sex']), (30162, 1))\nn = np.reshape(le.fit_transform(data['native.country']), (30162, 1))\n\n#Concatenate the label encoded features\nwemorsn = np.concatenate([w, e, m, o, r, s, n], axis=1)\n\n#Scale and encode separate columns\none_hot_encoded_columns = ohe.fit_transform(wemorsn)\nlabel_encoded_columns   = np.reshape(le.fit_transform(data[columns_to_label_encode]), (30162, 1))\nmin_max_scaled_columns  = mms.fit_transform(data[columns_to_scale])\n\n#Concatenate again\nprocessed_data = np.concatenate([min_max_scaled_columns, one_hot_encoded_columns, label_encoded_columns], axis=1)\n\n#Turn processed data into DataFrame typ\npd_df = pd.DataFrame(processed_data, index=data.index)\n\npd_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d44879656b4513836b1d93164d154d1b09a0090"},"cell_type":"markdown","source":"# 5. Model the Data\nIn this step we want to train different classification models and compare them with each other. We do this in order to select models, which we will examine in more detail in a further step.\n\nBut before we train the models, we have to seperate our prediction variable of the features"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c45262214b65f8a937eb5b53cbcf059bed1804aa"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Split prediction variable and features\nX = pd_df.values[:, :-1]\ny = pd_df.values[:, -1]\n\n#Split test and training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b9703ee827f24ddc67890590445a6a256aac893"},"cell_type":"markdown","source":"## 5.1 Model Selection\nEight different prediction models will be compared in their accuracy:\n* Decision Tree\n* K Nearest Neighbors\n* Logistic Regression\n* Naive Bayes\n* Random Forest\n* Support Vector Machine (linear kernel)\n* Support Vector Machine (radial basis function kernel)\n* Support Vector Machine (sigmoid kernel)"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"0d29903f396a4caee729c068f4a4198e2ee034e2"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#Import the classification models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n#Initialize the models\nmodels = []\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('LSVM', SVC(kernel='linear')))\nmodels.append(('SSVM', SVC(kernel='sigmoid')))\nmodels.append(('RSVM', SVC(kernel='rbf')))\n\nscores = []\nnames = []\n\n#Set up charts to visualize the results\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(9, 9))\nxAxis = 0\nyAxis = 0\n\naxes[4, 0].set_title('Mean Accuracy')\n\nprint('Mean accuracy on test data:')\n\n#Train and test each model - save the results\nfor name, model in models:\n    model.fit(X_train, y_train)\n    score = cross_val_score(model, X_test, y_test, cv=7)\n    scores.append(score)\n    names.append(name)\n    axes[xAxis, yAxis].set_title(name)\n    axes[xAxis, yAxis].plot(['1','2','3','4','5','6','7'], score, color='C'+str(len(names)-1))\n    axes[xAxis, yAxis].set_xlabel(\"Validation\")\n    axes[xAxis, yAxis].set_ylabel(\"Accuracy\")\n    axes[4, 0].bar(name, score.mean(), alpha=0.4, color='C'+str(len(names)-1))\n    axes[4, 0].set_ylim(0.79, 0.85)\n    \n    if len(names)%2 == 1:\n        yAxis += 1\n    else:\n        xAxis +=1\n        yAxis -=1\n    print(name, model.score(X_test, y_test))\n\n#Remove empty chart\naxes[4, 1].remove()\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"825e5fcd59e7eb92e7f84dd8f4da3bf1155e667c"},"cell_type":"markdown","source":"You can see that almost all models have on average more than 80% accuracy in their prediction. Only Gaussian Naives Baiyes model is out of this range with 54%. Furthermore, it can also be seen that some models achieve an accuracy of around 84% during cross validation.\n\n## 5.2 Parameter Tuning\n\nThe next step is to use GridSearch to take a closer look at these models and find the right parameters so that their accuracy increases further. The three model types with the best potential are investigated. Logistic Regression, Support Vector Machines and Random Forest. "},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"d552e7ae0952f9302aaf11c8660ab9872eb5672b"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#Construct pipelines\npipe1 = Pipeline((('LR', LogisticRegression()),))\npipe2 = Pipeline((('SVC', SVC()),))\npipe3 = Pipeline((('RF', RandomForestClassifier()),))\n\n#Define parameters for each pipeline\npara1 = {\n    'LR__penalty' : ['l1', 'l2'],\n    'LR__C': [0.01, 0.1, 1.0, 10]\n}\npara2 = {\n    'SVC__C': [0.1, 1.0, 10],\n    'SVC__kernel': ['linear', 'sigmoid', 'rbf'],\n    'SVC__gamma': [0.01, 0.1, 1.0]\n}\npara3 = {\n    'RF__n_estimators': [5, 10, 50],\n    'RF__max_features': ['auto', 'sqrt', 'log2'],\n    'RF__max_depth': [1, 5, 10],\n    'RF__min_samples_split': [2, 5, 10],\n    'RF__min_samples_leaf': [1, 2, 5]\n}\n\n\nparas = [ para1, para2, para3]\npipes = [ pipe1, pipe2, pipe3]\n\n\nfor i in range(len(pipes)):\n    print('GridSearch for model', i)\n    grid = GridSearchCV(pipes[i], paras[i], verbose=1, refit=False, n_jobs=-1)\n    grid = grid.fit(X_train, y_train)\n    print('Finished GridSearch\\n')\n    print('Best score:', grid.best_score_)\n    print(grid.best_params_, '\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86669dd2791178129a94cb55826637c0331dd040"},"cell_type":"markdown","source":"With the GridSearch method, the best parameters for each model could be found. Especially the Random Forest Model benefited from this and achieved an accuracy of more than 85%. \n\nIn the final step, the model with the parameters found must be trained with the entire data set."},{"metadata":{"trusted":true,"_uuid":"e5aa08653d49bba972637c7f46c2b9058d74f513"},"cell_type":"code","source":"#Initialize the model with the evaluated parameters\nRF  = RandomForestClassifier(max_depth=10, max_features='auto', min_samples_leaf=2, min_samples_split=10, n_estimators=10)\n\n#Train the models on the whole data set\nRF.fit(X, y)\n\nprint('Classification model is ready')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c58337bd9221f41c76ad6c2e913893dcc7b98bf4"},"cell_type":"markdown","source":"# 6. Conclusion\n\nIn summary, what has been achieved so far. A classification model based on the random forest algorithm has been developed to help determine the creditworthiness of individuals. It is important to note that this is only an assistance in this case. Since the data basis allows only a determination as to whether a person earns more or less than 50k, more complex calculations around the granting of credit cannot be carried out. The model could therefore be used in the first step of lending. If a potential lender asks his bank for a loan, the service personnel can determine with the help of personal data that is easy to determine whether the person has a possible creditworthiness and whether it is profitable for the credit institution to process the loan application further. In this case, the service staff could refer the customer to an expert.\n\nThe last question in this context is how the credit institution can use the model. Since banks often use software they have developed themselves for security reasons, they can integrate the classification model into their solutions via API. Thus they can also design the interface of the application according to their requirements and link it with other solutions."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}