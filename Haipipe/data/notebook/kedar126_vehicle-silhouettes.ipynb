{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np   \nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd    \nimport matplotlib.pyplot as plt \n%matplotlib inline \nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data = pd.read_csv('/kaggle/input/vehicle-silhouettes/vehicle.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shape of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Datatype information of each coloumns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking if there are any null values present in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.apply(lambda x: sum(x.isnull()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The target variable 'class' count. "},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the categorical values into numerical values for target coloumn."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label encode the target class\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nvehicle_data['class'] = labelencoder.fit_transform(vehicle_data['class'])\nvehicle_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset description.\nThe below step shows the mean, std.deviation, percentile quartile values, min and max for the coloumns present in the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"vehicle_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Performing necessary data pre-processing steps."},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Missing Values"},{"metadata":{},"cell_type":"markdown","source":"There are missing values in the vehicle dataset. Before we train a model, we have to deal with missing values present in the dataset. Hence we will replace missing values with the coloumns ***mean values***."},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below heatmap representation shows us the missing values presence in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(vehicle_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean values applied to the missing values ."},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.fillna(vehicle_data.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Below heatmap representation shows us the there are no missing values present in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(vehicle_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below is the box plot visualisation for the coloumns. We can see the outliers present in the dataset through the boxplot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=[col for col in vehicle_data.select_dtypes(np.number).columns]\n\nplt.figure(figsize=(20,20))\nfor i,col in enumerate(num_features,start=1):\n    plt.subplot(5,4,i);\n    sns.boxplot(vehicle_data['class'],vehicle_data[col]);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with the outliers present in the dataset."},{"metadata":{},"cell_type":"raw","source":"Once we have the missing values fixed. Now we have to remove the outliers present in the dataset.\nHence we will ***drop*** these outliers present in the dataset.\nBelow steps remove the outliers present in the respective coloumns."},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['radius_ratio']>276].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['pr.axis_aspect_ratio']>77].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['max.length_aspect_ratio']>14.5].index,axis=0,inplace=True)\nvehicle_data.drop(vehicle_data[vehicle_data['max.length_aspect_ratio']<2.5].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data[vehicle_data['scaled_variance']>292]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['scaled_variance.1']>989.5].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['scaled_radius_of_gyration.1']>87].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['skewness_about']>19.5].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data.drop(vehicle_data[vehicle_data['skewness_about.1']>40].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the dataset after fixing the outliers:\",vehicle_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Understanding the attribute relation with each other and finding the corelation between the attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(vehicle_data,diag_kind='kde', hue='class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above pair plots we can see that many columns are ***correlated*** and there are no long tails in any coloums which is an indication of ***no outliers present***."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=[col for col in vehicle_data.select_dtypes(np.number).columns ]\n\nplt.figure(figsize=(20,20))\nfor i,col in enumerate(num_features,start=1):\n    plt.subplot(5,4,i);\n    sns.distplot(vehicle_data[col])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph we can see that most of the coloums have normal ditribution and some have multiple peaks such as distance_circularity,elongatedness."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,4))\nsns.heatmap(vehicle_data.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.Our main goal is to categorize whether an object is a van or car based on the input features.\n2.Our assumption for the features which will categorize the object is that they are truly independent.There is no multicolinearity between the features.\n3.If two features is highly correlated then there is no use in using both features.In such a sceanrio we can \"drop one feature\".\n4.The heatmap above gives us the correlation matrix where we can see which features are highly correlated.\n5.From above correlation matrix we can see that there are many features which are highly correlated. \n6.If we observe carefully then \"scaled_variance.1\" and \"scatter_ratio\" has 0.99(~1) \n7.There are total 8 coloumns having correlation.\n8.They are max.length_rectangularity ->scaled_radius_of_gyration ->skewness_about.2 ->scatter_ratio ->elongatedness ->pr.axis_rectangularity ->scaled_variance ->scaled_variance.1\n9.Since there are features which are corelated will drop the them to make the features truly independent.\n               \"***We will be acheiving this by using PCA technique for dimensionality reduction***\"."},{"metadata":{},"cell_type":"markdown","source":"# 3. split the data into train and test data set."},{"metadata":{},"cell_type":"markdown","source":"#### Standardising the values from the dataset before training a model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaled_df = scaler.fit_transform(vehicle_data.drop(columns = 'class'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = scaled_df\ny = vehicle_data['class']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.3,random_state = 10)\n\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # 4.Train a Support vector machine using the train set and get the accuracy on the test set using original scaled attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training an SVC using the actual attributes(scaled)\n\nmodel = SVC(gamma = 'auto')\n\nmodel.fit(X_train,Y_train)\n\nscore_using_actual_attributes = model.score(X_test, Y_test)\n\nprint(score_using_actual_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.Perform K-fold cross validation on original scaled attributes and get the cross validation score of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(C=1, kernel=\"rbf\", gamma='auto')\n\nscores = cross_val_score(model, X, y, cv=10)\n\nCV_score = scores.mean()\nprint(CV_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.Using PCA from Scikit learn, extract Principal Components that capture about 95% of the variance in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(scaled_df)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nprint(np.cumsum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(list(range(1,19)),pca.explained_variance_ratio_,alpha=0.5,align='center')\nplt.ylabel('cum of variation explained')\nplt.xlabel('eigen value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.step(list(range(1,19)),np.cumsum(pca.explained_variance_ratio_),where= 'mid')\nplt.ylabel('cum of variation explained')\nplt.xlabel('eigen values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Picking up 8 prinicipal components as the first 8 capture more than 95% of the variance in the data.\n## 7.a Lets split the dataset into training and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=8)\n\nX = pca.fit_transform(scaled_df)\nY = vehicle_data['class']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=10)\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.b.Train a Support vector machine using the principal component analysis features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training an SVC using the PCs instead of the actual attributes \nmodel = SVC(gamma= 'auto')\n\nmodel.fit(X_train,Y_train)\n\nscore_PCs = model.score(X_test, Y_test)\n\nprint(score_PCs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.c.Perform K-fold cross validation on the principal components analysis and get the cross validation score of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(C=1, kernel=\"rbf\", gamma='auto')\n\nscores = cross_val_score(model, X, y, cv=10)\n\nCV_score_pca = scores.mean()\nprint(CV_score_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.Compare the accuracy scores and cross validation scores of Support vector machines – one trained using raw data and the other using Principal Components, and mention your findings."},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.DataFrame({'SVC' : ['All scaled attributes', '8 Principle components'],\n                      'Accuracy' : [score_using_actual_attributes,score_PCs],\n                      'Cross-validation score' : [CV_score,CV_score_pca]})\nmatrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion:\n***From above we can conclude that PCA is doing a pretty good job.\nAccuracy with PCA is approx 95% and with original attributes being approx 96%.\nNote that achieving 95% accuracy with only 8 dimensions against initial 18 dimensions is very good.\nWhat we would achieve otherwise with 18 dimensions can be achieved with 8 dimensions.\nHere \"SVC\" algorithm can be used as it has high levels of accuracy(94%) amd cross-validation score(94%) and can be applied on this dataset***."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}