{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL 5: FEATURE SELECTION\n",
    "by [Nikola S. Nikolov](http://bdarg.org/niknikolov)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on Tutoial 4 by introducing feature selection into the process of selecting the best classifier for a binary classification problem. It also demonstrates how to split a dataset into a training and test sets and use the test set for evaluation.\n",
    "\n",
    "The feature selection method applied here is Recursive Feature Elimination (RFE) as demonstrated in the tutorial at https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/.\n",
    "\n",
    "In this demonstration we use a modified version of the seeds dataset (see https://archive.ics.uci.edu/ml/datasets/seeds), which is the same dataset used in Tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing #needed for scaling attributes to the nterval [0,1]\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Dataset for Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>compactness</th>\n",
       "      <th>length of kernel</th>\n",
       "      <th>width of kernel</th>\n",
       "      <th>asymmetry coefficient</th>\n",
       "      <th>length of kernel groove</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>14.847524</td>\n",
       "      <td>14.559286</td>\n",
       "      <td>0.870999</td>\n",
       "      <td>5.628533</td>\n",
       "      <td>3.258605</td>\n",
       "      <td>3.700201</td>\n",
       "      <td>5.408071</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.909699</td>\n",
       "      <td>1.305959</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.443063</td>\n",
       "      <td>0.377714</td>\n",
       "      <td>1.503557</td>\n",
       "      <td>0.491480</td>\n",
       "      <td>0.472531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>10.590000</td>\n",
       "      <td>12.410000</td>\n",
       "      <td>0.808100</td>\n",
       "      <td>4.899000</td>\n",
       "      <td>2.630000</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>4.519000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>12.270000</td>\n",
       "      <td>13.450000</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>5.262250</td>\n",
       "      <td>2.944000</td>\n",
       "      <td>2.561500</td>\n",
       "      <td>5.045000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>14.355000</td>\n",
       "      <td>14.320000</td>\n",
       "      <td>0.873450</td>\n",
       "      <td>5.523500</td>\n",
       "      <td>3.237000</td>\n",
       "      <td>3.599000</td>\n",
       "      <td>5.223000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>17.305000</td>\n",
       "      <td>15.715000</td>\n",
       "      <td>0.887775</td>\n",
       "      <td>5.979750</td>\n",
       "      <td>3.561750</td>\n",
       "      <td>4.768750</td>\n",
       "      <td>5.877000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>21.180000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>0.918300</td>\n",
       "      <td>6.675000</td>\n",
       "      <td>4.033000</td>\n",
       "      <td>8.456000</td>\n",
       "      <td>6.550000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             area  perimeterÂ   compactness  length of kernel  width of kernel  \\\n",
       "count  210.000000  210.000000   210.000000        210.000000       210.000000   \n",
       "mean    14.847524   14.559286     0.870999          5.628533         3.258605   \n",
       "std      2.909699    1.305959     0.023629          0.443063         0.377714   \n",
       "min     10.590000   12.410000     0.808100          4.899000         2.630000   \n",
       "25%     12.270000   13.450000     0.856900          5.262250         2.944000   \n",
       "50%     14.355000   14.320000     0.873450          5.523500         3.237000   \n",
       "75%     17.305000   15.715000     0.887775          5.979750         3.561750   \n",
       "max     21.180000   17.250000     0.918300          6.675000         4.033000   \n",
       "\n",
       "       asymmetry coefficient  length of kernel groove        type  \n",
       "count             210.000000               210.000000  210.000000  \n",
       "mean                3.700201                 5.408071    0.333333  \n",
       "std                 1.503557                 0.491480    0.472531  \n",
       "min                 0.765100                 4.519000    0.000000  \n",
       "25%                 2.561500                 5.045000    0.000000  \n",
       "50%                 3.599000                 5.223000    0.000000  \n",
       "75%                 4.768750                 5.877000    1.000000  \n",
       "max                 8.456000                 6.550000    1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/seeds-dataset-binary/seeds_dataset_binary.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target attribute\n",
    "target_attribute_name = 'type'\n",
    "target = df[target_attribute_name]\n",
    "\n",
    "# predictor attributes\n",
    "predictors = df.drop(target_attribute_name, axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into a training (80%) and test (20%) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pepare independent stratified data sets for training and test of the final model\n",
    "predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.20, shuffle=True, stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale all predictor values to the range [0, 1]. Note the target attribute is already binary. This is a useful pre-processing technique to ensure that all attributes are treated equally during training. Applying a scaler (MinMaxScaler) can be seen as another parameter of the ML to be applied. It may or may not improve the accuracy of the trained model, which can be evaluated with a test dataset. \n",
    "\n",
    "Note that the MinMaxScaler is applied separately to the training and the testing datasets. \n",
    "This is to ensure that this transformation when performed on the testing dataset is not influnced by the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "predictors_train = min_max_scaler.fit_transform(predictors_train)\n",
    "predictors_test = min_max_scaler.fit_transform(predictors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply RFE with SVM for Selecting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True False  True  True]\n",
      "[5 2 3 1 4 1 1]\n"
     ]
    }
   ],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "estimatorSVM = svm.SVR(kernel=\"linear\")\n",
    "selectorSVM = RFE(estimatorSVM, 3)\n",
    "selectorSVM = selectorSVM.fit(predictors_train, target_train)\n",
    "# summarize the selection of the attributes\n",
    "print(selectorSVM.support_)\n",
    "print(selectorSVM.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply RFE with Logistic Regression for Selecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False  True  True]\n",
      "[3 5 1 4 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "estimatorLR = LogisticRegression(solver='lbfgs')\n",
    "# create the RFE model and select 3 attributes\n",
    "selectorLR = RFE(estimatorLR, 3)\n",
    "selectorLR = selectorLR.fit(predictors_train, target_train)\n",
    "# summarize the selection of the attributes\n",
    "print(selectorLR.support_)\n",
    "print(selectorLR.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the selectors to prepare a training dataset only with the selected features.\n",
    "\n",
    "__Note:__ The same selectors are applied to the test dataset. However, it is important that the test dataset was not used by (it's invisible to) the selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train_SVMselected = selectorSVM.transform(predictors_train)\n",
    "predictors_test_SVMselected = selectorSVM.transform(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train_LRselected = selectorLR.transform(predictors_train)\n",
    "predictors_test_LRselected = selectorLR.transform(predictors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate SVM classifiers with both the selected features and all features \n",
    "\n",
    "Here we train three models:\n",
    "* model1 - with the features selected by SVM\n",
    "* model2 - with the features selected by Logistic Regression\n",
    "* model3 - with all features (i.e. without feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = classifier.fit(predictors_train_SVMselected, target_train)\n",
    "model1.score(predictors_test_SVMselected, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = classifier.fit(predictors_train_LRselected, target_train)\n",
    "model2.score(predictors_test_LRselected, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = classifier.fit(predictors_train, target_train)\n",
    "model3.score(predictors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Conclusion and Further Work\n",
    "\n",
    "When you execute this code again, it is very likely to get different results.\n",
    "\n",
    "To get more accurate results, accounting for the variance in the results, it is better to run the whole experiment multiple times and measure the variance in the results. Then pick the model that gives better results.\n",
    "\n",
    "The process outlined in this tutorial can be further authomated with the use of scikit-learn pipelines. As an exercise build at least two pipelines for training classifiers for the seeds dataset. Each pipeline should include a feature-selection method, and the feature-selection method in pipeline 1 should be different from the feature-selection method in pipeline 2.\n",
    "\n",
    "To do this follow the examples at:\n",
    "\n",
    "* https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "* https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n",
    "* https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Continue with Tutorial 6: Clustering and Manifold Learning](https://www.kaggle.com/nikniko101v/tutorial-6-clustering-and-manifold-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
