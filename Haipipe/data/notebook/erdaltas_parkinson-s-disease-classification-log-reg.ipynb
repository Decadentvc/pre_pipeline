{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","import os\n","print(os.listdir(\"../input\"))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{"_uuid":"8d482c6e199419394305ba0d74ef45d60b08f59f"},"source":["**Data Set Information:**\n","\n","This dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording. The main aim of the data is to discriminate healthy people from those with PD, according to \"status\" column which is set to 0 for healthy and 1 for PD."]},{"cell_type":"markdown","metadata":{"_uuid":"a523c26b282a009f277e6a362de4e6d56605a19f"},"source":["**Attribute Information:**\n","\n","Matrix column entries (attributes):\n","MDVP:Fo(Hz) - Average vocal fundamental frequency\n","MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n","MDVP:Flo(Hz) - Minimum vocal fundamental frequency\n","MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several \n","measures of variation in fundamental frequency\n","MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\n","NHR,HNR - Two measures of ratio of noise to tonal components in the voice\n","status - Health status of the subject (one) - Parkinson's, (zero) - healthy\n","RPDE,D2 - Two nonlinear dynamical complexity measures\n","DFA - Signal fractal scaling exponent\n","spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["data=pd.read_csv(\"../input/parkinsons2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"540045691d85c81f40f5f6db02be1e9dd6fcc93e","trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"94d76b7c8bbd37f0f3523f610588a5c2c83caa8a","trusted":true},"outputs":[],"source":["data.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"872175f616f62078cf51cab8e04ab73aa8ebe16c","trusted":true},"outputs":[],"source":["#correlation data\n","f,ax=plt.subplots(figsize=(14,14))\n","sns.heatmap(data.corr(),annot=True,ax=ax,fmt=\".2f\")\n","plt.xticks(rotation=90)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fbc7e2e60087b607b3da13f067d63308bddcd136","trusted":true},"outputs":[],"source":["#prepare of data\n","y=data.status.values\n","x_data=data.drop([\"status\"],axis=1)\n","#normlizasyon\n","x=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9fb2a104f8d2d73cb5b43ce5f00e422ccd7abbf4","trusted":true},"outputs":[],"source":["#split data for test%train\n","from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f436e4fd3df9fe022b0b7e4ae884fce84c0ba3c7","trusted":true},"outputs":[],"source":["#transpose of each \n","x_train=x_train.T\n","x_test=x_test.T\n","y_train=y_train.T\n","y_test=y_test.T"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8d2664d811a192090c564d2698302c4be6fc0596","trusted":true},"outputs":[],"source":["#determination of initial values of weights and bias\n","def initialize_weights_and_bias(dimension):\n","    b=0.0\n","    w=np.full((dimension,1),0.01)\n","    return w,b"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"746a60ece751cd52575d8fd11f7008ac01e79f8b","trusted":true},"outputs":[],"source":["#the formula of the activation function\n","def sigmoid(z):\n","    y_head=1/(1+np.exp(-z))\n","    return y_head"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"51a1f92cf4bd99194f2feef1c2d24d540b01f90b","trusted":true},"outputs":[],"source":["#calculate of z, forward and backward propagation\n","def forward_backward_propagation(w,b,x_train,y_train):\n","#forward  propagation   \n","    z=np.dot(w.T,x_train)+b\n","    y_head=sigmoid(z)\n","    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n","    cost=(np.sum(loss))/x_train.shape[1]    \n","#backward propagation\n","    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n","    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n","    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n","    return cost,gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"15f548be198f07d00277fab96bada31c7541e34b","trusted":true},"outputs":[],"source":["#update parameter values to reduce cost value\n","def update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n","    cost_list = []\n","    cost_list2 = []\n","    index = []\n","    \n","    for i in range(number_of_iteration):\n","        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n","        cost_list.append(cost)\n","        w = w - learning_rate * gradients[\"derivative_weight\"]\n","        b = b - learning_rate * gradients[\"derivative_bias\"]\n","        if i % 10 == 0:\n","            cost_list2.append(cost)\n","            index.append(i)\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    parameters = {\"weight\": w,\"bias\": b}\n","    plt.plot(index,cost_list2)\n","    plt.xticks(index,rotation='vertical')\n","    plt.xlabel(\"Number of Iterarion\")\n","    plt.ylabel(\"Cost\")\n","    plt.show()\n","    return parameters, gradients, cost_list"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"dcb5b9fdb3cb72e1c7a88795b62d0f792841b2c9","trusted":true},"outputs":[],"source":["#prediction of test values\n","def predict(w,b,x_test):\n","    # x_test is a input for forward propagation\n","    z = sigmoid(np.dot(w.T,x_test)+b)\n","    Y_prediction = np.zeros((1,x_test.shape[1]))\n","    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n","    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n","    for i in range(z.shape[1]):\n","        if z[0,i]<= 0.5:\n","            Y_prediction[0,i] = 0\n","        else:\n","            Y_prediction[0,i] = 1\n","\n","    return Y_prediction\n","def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n","    dimension =  x_train.shape[0] \n","    w,b = initialize_weights_and_bias(dimension)\n","    # do not change learning rate\n","    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n","    \n","    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n","\n","    # Print test Errors\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4a25fc18a337e3928510b0aa9f61ab6b91ca6027","trusted":true},"outputs":[],"source":["logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1.2, num_iterations = 400)"]},{"cell_type":"markdown","metadata":{"_uuid":"fdaa14509cc0a3ff7cb8f1b2ce96113cb8339ffd"},"source":["**There is also a short version of the above codes.**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"66dcbb7a179bb9f34be55df25fc950b6e6fcc590","trusted":true},"outputs":[],"source":["#Logistic regression with sklearn \n","from sklearn.linear_model import LogisticRegression\n","log=LogisticRegression()\n","log.fit(x_train.T,y_train.T)\n","print(\"test-accuracy : {} \".format(log.score(x_test.T,y_test.T)*100))\n","x_train = x_train.T\n","y_train = y_train.T\n","x_test = x_test.T\n","y_test = y_test.T"]},{"cell_type":"markdown","metadata":{"_uuid":"45dd1c306665f97f74a431aff6005f5fdb23c3ce"},"source":["\n","**Logistic Regression  predicted correctly  the class of data at 89.74 percent.  I hope you like. Waiting for comments and suggestions.**\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}
