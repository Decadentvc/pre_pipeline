{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # Predicting Pulsar Stars\n\n### Kaggle\n\n\n#### HTRU2 is a data set which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey .\n\n#### Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter .\n\n#### As pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, produces a detectable pattern of broadband radio emission. As pulsars rotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals with large radio telescopes.\n\n#### Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation . Thus a potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However in practice almost all detections are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find.\n\n## Introduction\n\n#### Firstly, we will take a look in the behaviour of our data! We are going to use a couple of manifold learning algorithms in order to visualise the high dimensional data in a 2D plot!\n\n#### Lastly, we will use some machine learning approaches to correctly identify Pulsar Stars. We want to find the best algorithm, accuracy-wise, however, we need to find which one provides the most important features to identify Pulsar Stars."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/predicting-a-pulsar-star/pulsar_stars.csv')\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How many samples do we have for which class?"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = data[['target_class']]\n\ndata.drop('target_class', axis=1, inplace=True)\n\ntarget['target_class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Some statistical information about our data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Taking a look of how our features are correlated:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that are \"groups\" of features that are highly correlated. That is good news for linear approaches!"},{"metadata":{},"cell_type":"markdown","source":"## High Dimensional Data Visualization using Manifold Learning Algorithms"},{"metadata":{},"cell_type":"markdown","source":"#### I highly recommend to visualise data in the original space, however, our data has 8 features, therefore we cannot plot it. But, in order to do so, we can use manifold learning algorithms to analyse the structure of our data in the original space and, then, embedding it in a low dimensial space, so we can plot it!\n\n#### We are going to use two algorithms: (1) t-SNE and (2) ISOMAP.\n\n#### t-SNE uses a statistical/probability approach to identify and reconstruct the original spac. Meanwhile, ISOMAP uses classical manifold theory to do so. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom sklearn.manifold import Isomap\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (1) t-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2, init='pca', perplexity = 40)\ntsne_data = tsne.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_pulsar = []\npulsar = []\n\nfor i in range(len(target)):\n    if target['target_class'][i] == 0:\n        not_pulsar.append(tsne_data[i])\n    if target['target_class'][i] == 1:\n        pulsar.append(tsne_data[i])\n        \nnot_pulsar = np.array(not_pulsar)\npulsar = np.array(pulsar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.scatter(not_pulsar[:,0], not_pulsar[:,1], c='blue', label='Not Pulsar Stars')\nplt.scatter(pulsar[:,0], pulsar[:,1], c='red', label='Pulsar Stars')\nplt.legend()\nplt.title('Low dimensional visualization (t-SNE) - Pulsar Stars');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (2) ISOMAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"isomap = Isomap(n_components=2, n_neighbors=5, path_method='D', n_jobs=-1)\n\nisomap_data = isomap.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_pulsar = []\npulsar = []\n\nfor i in range(len(target)):\n    if target['target_class'][i] == 0:\n        not_pulsar.append(isomap_data[i])\n    if target['target_class'][i] == 1:\n        pulsar.append(isomap_data[i])\n        \nnot_pulsar = np.array(not_pulsar)\npulsar = np.array(pulsar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.scatter(not_pulsar[:,0], not_pulsar[:,1], c='blue', label='Not Pulsar Stars')\nplt.scatter(pulsar[:,0], pulsar[:,1], c='red', label='Pulsar Stars')\nplt.legend()\nplt.title('Low dimensional visualization (ISOMAP) - Pulsar Stars');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One of the benefits of these approachs is that it often cluster our data. Both methods illustrated samples of Pulsar Stars clustered, meaning that features of our data are distinct among sample groups!"},{"metadata":{},"cell_type":"markdown","source":"## Classification Approaches\n\n#### We are going to use three approaches: (1) PCA + kNN, (2) LDA + kNN and (3) kNN.\n\n#### PCA is a classical unsupervised algorithm of dimensionality reduction based on variance-covariance between sample features! Meanwhile, LDA is a supervised method to discriminate sample groups finding a hyperplane of n-1 dimension, where n = (number of distinct sample groups). Finally, kNN is a method to classify samples based of proximity between samples. \n\n#### Our simplest method is the third approach. However is the method that gives less information about our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train, data_test, target_train, target_test = train_test_split(data, np.array(target['target_class']), test_size=0.2, random_state=0)\n\nprint('train size = ', len(data_train))\nprint('test size = ', len(data_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(target_train).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(target_test).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In order to avoid mistakes caused by variations of each feature, we need to scale it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(data_train)\n\ndata_train_scaled = scaler.transform(data_train)\ndata_test_scaled = scaler.transform(data_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (1) PCA + kNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(data_train_scaled)\npca_data_train = pca.transform(data_train_scaled)\nprint(\"Variance explained by each component (%): \")\nfor i in range(len(pca.explained_variance_ratio_)):\n      print(\"\\n\",i+1,\"ยบ:\", pca.explained_variance_ratio_[i]*100)\n        \nprint(\"\\nTotal sum (%): \",sum(pca.explained_variance_ratio_)*100)\n\nprint(\"\\nSum of the first two components (%): \",(pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1])*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since we have a lot of samples for a 8 dimensional data, we can calculate all principal components of data. However, for simplicity, we are going to use just the first and second principal component, representing 78% of information of our original data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_pulsar = []\npulsar = []\n\nfor i in range(len(target_train)):\n    if target_train[i] == 0:\n        not_pulsar.append(pca_data_train[i])\n    if target_train[i] == 1:\n        pulsar.append(pca_data_train[i])\n        \nnot_pulsar = np.array(not_pulsar)\npulsar = np.array(pulsar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.scatter(not_pulsar[:,0], not_pulsar[:,1], c='blue', label='Not Pulsar Stars')\nplt.scatter(pulsar[:,0], pulsar[:,1], c='red', label='Pulsar Stars')\nplt.legend()\nplt.title('Low dimensional visualization (PCA) - Pulsar Stars');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that PCA, in a unsupervised way, separated our sample groups! That result means that the first principal component can be used to describe the differences between sample features of each group!\n\n#### Let's evaluate the accuracy of our model when classifing data using kNN!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 2).fit(data_train_scaled)\n\npca_data_test = pca.transform(data_test_scaled)\npca_data_train = pca.transform(data_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\nfor k in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=k, p=2)\n    knn.fit(pca_data_train, target_train)\n    accuracy.append(knn.score(pca_data_test, target_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,20),accuracy, 'bx-');\nplt.xlabel('k number of neighbors')\nplt.ylabel('Accuracy')\nplt.title('Optimal number of neighbors');\n\nprint( \"The best accuracy was\", np.round(np.array(accuracy).max()*100, 2), \"% with k =\",  np.array(accuracy).argmax()+1) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our model has a very high accuracy with all possibilities of neighbors in the range of study! "},{"metadata":{},"cell_type":"markdown","source":"### (2) Linear Discriminant Analysis + kNN"},{"metadata":{},"cell_type":"markdown","source":"#### Using the same approach as before.. let's computate LDA on our data, then plot it, and evaluate our model using kNN!"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LDA(n_components=1).fit(data_train_scaled, target_train)\n\nlda_data_train = lda.transform(data_train_scaled)\nlda_data_test = lda.transform(data_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_pulsar = []\npulsar = []\n\nfor i in range(len(target_train)):\n    if target_train[i] == 0:\n        not_pulsar.append(lda_data_train[i])\n    if target_train[i] == 1:\n        pulsar.append(lda_data_train[i])\n        \nnot_pulsar = np.array(not_pulsar)\npulsar = np.array(pulsar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It is very helpful to computate the probability density function of the LDA result in order to visualise statistical information of it, such as distance between sample groups and variance of each group!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pulsar_mean, pulsar_std = norm.fit(pulsar)\nnot_pulsar_mean, not_pulsar_std = norm.fit(not_pulsar)\nall_mean, all_std = norm.fit(lda_data_train)\n\nx = np.linspace(-7, 12, 10000)\npulsar_pdf = norm.pdf(x, pulsar_mean, pulsar_std)\nnot_pulsar_pdf = norm.pdf(x, not_pulsar_mean, not_pulsar_std)\nall_pdf = norm.pdf(x, all_mean, all_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(pulsar_mean,0, marker='X', c='red',s=400)\nplt.scatter(not_pulsar_mean,0, marker='X', c='blue',s=400)\nplt.scatter(all_mean,0, marker='X', c='k',s=400)\nplt.scatter(lda_data_train[:,0], np.zeros((len(lda_data_train),1)), c= ['red' if l==1  else 'blue' for l in target_train])\nplt.ylim([-0.5,0.7])\nplt.xlim([-7,12])\nplt.plot(x, pulsar_pdf, 'r', linewidth=1.2, label='Pulsar Stars')\nplt.plot(x, not_pulsar_pdf, 'b', linewidth=1.2, label='Not Pulsar Stars')\nplt.plot(x, all_pdf, 'k', linewidth=1.2, label='All data')\nplt.xlabel('Discriminant Hyperplane')\nplt.ylabel('Probability Density Function')\nplt.legend()\nplt.title('LDA model');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that PDFs are very separated! That means that we found good news! Not-pulsar-stars samples are very near each other (lower variance), in the other hand, pulsar stars samples seem to has a higher variance, therefore, the behaviour of each pulsar star may vary among distinct stars, meaning the uniqueness of this phenomena!"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\nfor k in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=k, p=2)\n    knn.fit(lda_data_train, target_train)\n    accuracy.append(knn.score(lda_data_test, target_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,20),accuracy, 'bx-');\nplt.xlabel('k number of neighbors')\nplt.ylabel('Accuracy')\nplt.title('Optimal number of neighbors');\n\nprint( \"The best accuracy was\", np.round(np.array(accuracy).max()*100, 2), \"% with k =\",  np.array(accuracy).argmax()+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that we achieved similar results to our previous approach! Very good!"},{"metadata":{},"cell_type":"markdown","source":"### (3) kNN\n\n\n#### In this method we are not transforming our data, neither extracting important features. Simple as that."},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\nfor k in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=k, p=2)\n    knn.fit(data_train_scaled, target_train)\n    accuracy.append(knn.score(data_test_scaled, target_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,20),accuracy, 'bx-');\nplt.xlabel('k number of neighbors')\nplt.ylabel('Accuracy')\nplt.title('Optimal number of neighbors');\n\nprint( \"The best accuracy was\", np.round(np.array(accuracy).max()*100, 2), \"% with k =\",  np.array(accuracy).argmax()+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We achieved a very good accuracy! Since we did not use other algorithms, this method performed better, computational-wise. However, this method is just useful to classify our data, and can not be used to extract further information about it!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}