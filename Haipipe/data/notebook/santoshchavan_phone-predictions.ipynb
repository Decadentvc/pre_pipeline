{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mobile-price-classification/train.csv\n",
      "/kaggle/input/mobile-price-classification/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/mobile-price-classification/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is 2000\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the data is', df.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "battery_power    0.0\n",
       "blue             0.0\n",
       "clock_speed      0.0\n",
       "dual_sim         0.0\n",
       "fc               0.0\n",
       "four_g           0.0\n",
       "int_memory       0.0\n",
       "m_dep            0.0\n",
       "mobile_wt        0.0\n",
       "n_cores          0.0\n",
       "pc               0.0\n",
       "px_height        0.0\n",
       "px_width         0.0\n",
       "ram              0.0\n",
       "sc_h             0.0\n",
       "sc_w             0.0\n",
       "talk_time        0.0\n",
       "three_g          0.0\n",
       "touch_screen     0.0\n",
       "wifi             0.0\n",
       "price_range      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null values\n",
    "(df.isnull().sum()/df.count())*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
       "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
       "       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
       "       'touch_screen', 'wifi', 'price_range'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the features\n",
    "X = df.drop(['price_range'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the label\n",
    "y = df['price_range'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiating a SVM model\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(random_state = 0)\n",
    "#Fitiing the model on the train data\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.5766666666666667\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the model is:' ,model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy with C=4 is: 0.5766666666666667\n",
      "====================================================================================================\n",
      "The accuracy with C=0.01 is: 0.4583333333333333\n",
      "====================================================================================================\n",
      "The accuracy with C=0.005 is: 0.5316666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Regularization parameters\n",
    "\n",
    "c_model_1 = LinearSVC(C=4,random_state = 0)\n",
    "c_model_1.fit(X_train,y_train)\n",
    "acc_1 = c_model_1.score(X_test,y_test)\n",
    "\n",
    "\n",
    "c_model_2 = LinearSVC(C=0.01,random_state = 0)\n",
    "c_model_2.fit(X_train,y_train)\n",
    "acc_2 = c_model_2.score(X_test,y_test)\n",
    "\n",
    "\n",
    "c_model_3 = LinearSVC(C=0.005,random_state = 0)\n",
    "c_model_3.fit(X_train,y_train)\n",
    "acc_3 = c_model_3.score(X_test,y_test)\n",
    "print('The accuracy with C=4 is:', acc_1)\n",
    "print(50*'==')\n",
    "print('The accuracy with C=0.01 is:', acc_2)\n",
    "print(50*'==')\n",
    "print('The accuracy with C=0.005 is:', acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Poly: 0.9533333333333334\n",
      "==================================================\n",
      "The accuracy of rbf: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "#Trying with different types of Kernels\n",
    "from sklearn.svm import SVC\n",
    "poly_model = SVC(kernel = 'poly', random_state = 0)\n",
    "poly_model.fit(X_train,y_train)\n",
    "acc_poly = poly_model.score(X_test,y_test)\n",
    "print('The accuracy of Poly:', acc_poly)\n",
    "\n",
    "print(50*'=')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "rbf_model = SVC(kernel = 'rbf', random_state = 0)\n",
    "rbf_model.fit(X_train,y_train)\n",
    "acc_rbf = rbf_model.score(X_test,y_test)\n",
    "print('The accuracy of rbf:', acc_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One vs All(OVA)**\n",
    "This is the most simple(and naive) approach. \n",
    "In order to classify the four classes, we will construct four different binary classifiers. \n",
    "For a given class, the positive examples are all the points in the class, and the negative examples are all the points of the other class.As a result we obtain one binary classifier for each problem.\n",
    "For every new prediction, we use each classifer and return the classifier which returns a positive result.\n",
    "\n",
    "Drawback\n",
    "\n",
    "    Can lead to multiple classes\n",
    "    Can lead to no class\n",
    "\n",
    "One heuristic to avoid these ambiguities is to assign it based on the maximum value that the classifier will give out instead of the absolute value(i.e. Instead of just assigning class based on whether the result is positive or negative, assign it based on the actual positive or negative value).\n",
    "\n",
    "That helps this approach choose the class of the hyperplane that is closest to the example when all classifiers disagree. \n",
    "Even after that, this method doesn't always give satisfactory results.\n",
    "\n",
    "Still, it does remain a popular method for multi-class classification owing to its easy implementation and understandability\n",
    "\n",
    "In sklearn implementation, LinearSVC implements this strategy by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of One vs all: 0.9783333333333334\n"
     ]
    }
   ],
   "source": [
    "#Multi class SVM\n",
    "# One Vs All\n",
    "model_ova = SVC(random_state = 0,kernel = 'linear', decision_function_shape = 'ova')\n",
    "model_ova.fit(X_train,y_train)\n",
    "acc_ova = model_ova.score(X_test,y_test)\n",
    "print('Accuracy of One vs all:',acc_ova)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One vs One\n",
    "\n",
    "This approach is a upgraded version of OVA, where instead of differentiating one class from all the other classifers, we try to distinguish one class from every other other class.\n",
    "\n",
    "As a result, we train one classifier per pair of classes, which leads to a total of K(K-1)/2 classifiers for K classes. Predictions are then made using voting. Each new instance is passed to each classifier and the predicted class is recorded. Then the class having the most votes is assigned to the instance.\n",
    "\n",
    "In SVC, it already comes as a hyper-parameter\n",
    "\n",
    "class sklearn.svm.SVC(kernel='rbf', C=1.0 , decision_function_shape='ova', random_state=None)\n",
    "\n",
    "Let's try and implement it to solve our modified phone problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of One vs One: 0.9783333333333334\n",
      "[3, 0, 2, 2, 2, 0, 0, 3, 3, 1, 1, 3, 0, 2, 3, 0, 3, 2, 2, 1, 0, 0, 3, 1, 2, 2, 3, 1, 3, 1, 1, 0, 2, 0, 1, 3, 0, 0, 3, 3, 2, 1, 3, 3, 1, 3, 0, 1, 3, 1, 1, 3, 0, 3, 0, 2, 2, 2, 0, 3, 3, 1, 3, 2, 1, 2, 3, 2, 2, 2, 3, 2, 1, 0, 1, 3, 2, 2, 1, 2, 3, 3, 3, 0, 0, 0, 2, 1, 2, 3, 1, 2, 2, 1, 0, 3, 3, 3, 0, 3, 1, 1, 2, 1, 3, 2, 2, 3, 2, 3, 3, 0, 0, 1, 3, 3, 0, 0, 1, 0, 0, 3, 2, 2, 1, 1, 1, 1, 0, 2, 1, 3, 3, 3, 3, 3, 3, 2, 0, 1, 1, 2, 1, 3, 0, 3, 0, 0, 2, 0, 1, 1, 1, 1, 3, 0, 0, 3, 1, 3, 2, 1, 3, 1, 2, 3, 3, 2, 1, 0, 3, 1, 2, 3, 3, 0, 2, 2, 3, 0, 2, 1, 0, 1, 2, 1, 2, 0, 3, 3, 1, 1, 0, 2, 3, 0, 1, 2, 2, 0, 3, 3, 3, 1, 2, 3, 3, 3, 0, 0, 0, 2, 3, 3, 0, 0, 1, 3, 2, 3, 3, 3, 0, 0, 2, 2, 3, 1, 0, 2, 0, 0, 0, 3, 2, 0, 2, 2, 1, 1, 0, 2, 3, 3, 0, 0, 1, 3, 3, 2, 3, 0, 3, 1, 1, 0, 2, 3, 3, 2, 0, 0, 1, 2, 3, 2, 2, 3, 1, 1, 0, 3, 3, 2, 1, 3, 2, 2, 2, 1, 0, 2, 2, 1, 0, 0, 2, 2, 2, 2, 0, 1, 3, 0, 2, 2, 3, 0, 2, 0, 1, 1, 3, 0, 0, 2, 3, 1, 2, 0, 2, 0, 3, 0, 3, 3, 2, 3, 1, 2, 2, 1, 1, 1, 0, 1, 0, 3, 1, 0, 3, 1, 0, 1, 3, 0, 3, 1, 2, 0, 1, 3, 0, 2, 2, 1, 2, 1, 1, 0, 2, 0, 0, 3, 1, 2, 3, 2, 2, 0, 3, 2, 2, 1, 3, 2, 3, 3, 3, 0, 2, 0, 3, 0, 1, 1, 2, 2, 1, 3, 1, 2, 0, 1, 2, 3, 0, 0, 1, 3, 0, 3, 0, 2, 2, 1, 1, 0, 2, 1, 0, 1, 3, 0, 3, 3, 0, 2, 1, 3, 1, 1, 3, 2, 0, 3, 2, 2, 0, 0, 3, 0, 1, 1, 1, 3, 2, 3, 2, 0, 3, 0, 0, 1, 3, 0, 0, 3, 2, 2, 2, 3, 0, 0, 1, 2, 1, 2, 0, 3, 3, 0, 3, 3, 0, 2, 2, 1, 0, 2, 2, 1, 3, 2, 2, 0, 2, 0, 3, 3, 2, 1, 0, 3, 1, 2, 0, 0, 1, 3, 0, 3, 0, 0, 1, 2, 0, 1, 3, 0, 2, 2, 1, 2, 0, 3, 0, 2, 3, 2, 2, 2, 3, 2, 3, 3, 0, 0, 2, 0, 3, 1, 1, 1, 0, 3, 2, 0, 2, 1, 3, 1, 1, 0, 2, 1, 0, 3, 1, 3, 0, 0, 2, 0, 3, 1, 0, 0, 2, 3, 0, 0, 0, 1, 2, 3, 1, 3, 2, 2, 0, 2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 2, 1, 3, 0, 3, 0, 0, 1, 1, 2, 0, 1, 0, 1, 3, 3, 1, 3, 0, 2, 0, 0, 1, 3, 1, 2, 1, 3, 2, 1, 1, 2, 2, 0, 1, 3, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#One vs One\n",
    "\n",
    "model_ovo = SVC(random_state = 0, kernel = 'linear', decision_function_shape = 'ova')\n",
    "model_ovo.fit(X_train,y_train)\n",
    "acc_ovo = model_ovo.score(X_test,y_test)\n",
    "y_pred = model_ovo.predict(X_test)\n",
    "print('Accuracy of One vs One:', acc_ovo)\n",
    "print((y_pred).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages\n",
    "\n",
    "   Having the flexibility to choose kernel, SVM works well on a wide range of classification problems, even problems in high dimensions and that are not linearly separable.\n",
    "\n",
    "   SVM models have generalization(regularisation parameters) in practice, therefore the risk of overfitting is less in SVM.\n",
    "\n",
    "   SVM works well with even unstructured and semi structured data like text, images and trees.\n",
    "Disadvantages\n",
    "\n",
    "   Optimisation of key parameters needs to be done for each problem to get satisfactorily results. Parameters that may result in an excellent classification accuracy for problem A, may result in a poor classification accuracy for problem B.\n",
    "\n",
    "   Design of Multiclass SVM classifiers(OVA and OVO require multiple SVM instances) are still not optimal enough for practical applications\n",
    "\n",
    "   The quadratic programming problem(primal formulation) is computationally intensive and with increase in training data it can be very slow on normal machines\n",
    "\n",
    "   SVM in its pure implementation cannot return a probabilistic confidence value(class probabilities) like another classifier say Logistic Regression. This confidence of prediction is sometimes more important than the actual prediction in many applications\n",
    "\n",
    "Despite being powerful in theory, it isn't being used extensively today. On unstructured data, it is easily outperformed by neural networks and on structured data, by gradient boosted trees.\n",
    "\n",
    "Still it's an algorithm worth having in your ML toolset owing to its radical and intuitive approach of solving ML problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
