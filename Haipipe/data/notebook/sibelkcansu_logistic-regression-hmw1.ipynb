{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"a713fe5cb9a7cd558d3ca22f50f5852abcf242e4"},"source":["# INTRODUCTION\n","\n","In this kernel, we will apply Logistic Regression procedure to \"Gender Recognition by Voice Data\"\n","1. [Read Data](#1)\n","1. [Logistic Regression](#2)\n","    1. [Determine Values](#3)\n","    1. [Train Test Split](#4)\n","    1. [Forward Backward Propagation](#5)\n","    1. [Prediction](#6)\n","    1. [Logistic Regression Algorithm](#7)\n","    1. [Logistic Regression with sklearn Library](#8)\n","1. [Conclusion](#9) "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","import os\n","print(os.listdir(\"../input\"))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{"_uuid":"3f4dc5cfe5aabd9a03b37806c544f9d6a3c4bd68"},"source":["<a id=\"1\"></a>\n","# Read Data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["data=pd.read_csv(\"../input/voice.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4b966c09eed78c7ffe08aa0595218f987ba2ffba","trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b51dbf51d860735477a525f93b6dec661cdb52a9","trusted":true},"outputs":[],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e8fdd1aa90830a8f081c638041dbb9a0bd047ced","trusted":true},"outputs":[],"source":["data.label.value_counts()"]},{"cell_type":"markdown","metadata":{"_uuid":"617d07b97b2b5353f10f305ad8b40bbb51ee24a5"},"source":["Let's classify male and female as male=1 and female=0. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"845988bdf7e3b9617e8fe027c33ac10c7409f465","trusted":true},"outputs":[],"source":["data.label=[  1 if i==\"male\" else 0 for i in data.label]"]},{"cell_type":"markdown","metadata":{"_uuid":"4255ae868542ea9212448a989d8e8b69b4d67bcb"},"source":["Now, label is a binary output, and our data is convenient for Logistic Regression."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"29b97243f7e2ebf11c00dee461bdba9a08adea42","trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9bd54ff846c154f18fe47d1b07e479aca0d47c69","trusted":true},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","metadata":{"_uuid":"e2fd414fd4e73418e87c7aaf56ed40623a602a65"},"source":["<a id=\"2\"></a>\n","# Logistic Regression\n","\n","Logistic Regression is a classification algortihm. It is the simplest deep learning (neural network). \n","\n","First of all we want to train our data. So , we will use Computation Graph. Here are the components of Computation Graph.\n","* parameters: weights and bias(w and b )\n","* weights: coefficents of values of  each feature \n","* z = ((w)^T)*x + b  or we can write  z = b + p1*w1 + p2*w2 + ... + p20*w20 for our data\n","* p1, p2,..., p20: values of each feature in data (this will be meaningful after train test split method !)\n","* y_head = sigmoid(z)\n","    * Sigmoid function (which is called as activation function) makes z between 0 and 1 so that is a probabilitic result. \n","    * Mathematical equation of sigmoid function is   $f(x)=\\displaystyle \\frac{1}{1+\\mathbb{e}^{-x}}$.\n"]},{"cell_type":"markdown","metadata":{"_uuid":"29c71979c98d470ceee7cd994c94989d1f4e5f84"},"source":["<a id=\"3\"></a>\n","# Determine Values\n","First of all, we will determine x and y values for Logistic Regression."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"72eaf6c9beb40b01d5ed1be1fc0238ef9311be02","trusted":true},"outputs":[],"source":["y=data.label.values\n","x_data=data.drop([\"label\"],axis=1)"]},{"cell_type":"markdown","metadata":{"_uuid":"540cee2b9ecbf592eff2de01204b9c2a84a86454"},"source":["Let's check what is y and x_data.\n","* y is our output\n","* the values in x_data will be coefficients of weights. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"aa5f68b326166faee6ad326ede85cddaf6973d92","trusted":true},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0323308ded9be7b95a5392a81a60bc10e550f669","trusted":true},"outputs":[],"source":["x_data.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"aa01bdd7ea9dcd2a1e427ba141f94a8f3af8c030"},"source":["To get an appropriate model we need to normalize the values in x_data."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2d8cac21bfdce13d08b101cefea853d8d1e738ef","trusted":true},"outputs":[],"source":["# normalization =(a-min(a))/(max(a)-min(a))\n","\n","x=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data)).values"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bfd5d6582c2fc329389e63f1d01d70fee7808e84","trusted":true},"outputs":[],"source":["x.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"8963be0adc261d365179c458a10071e64ee99d36"},"source":["<a id=\"4\"></a>\n","# Train Test Split\n","We want to train our data by Linear Regression. But after getting our model, we need another data to test our model. So we will use **train_test_split**\n","to control the acurracy of our model.\n","* train_test_split says that take 80% of data to get the model and use 20% of data to control the model."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c17bec10240f3af551c11906cb5086e0e40faf66","trusted":true},"outputs":[],"source":["# create x_train, y_train, x_test, y_test arrays\n","from sklearn.model_selection import train_test_split\n","\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n","\n","# our features must be row in our matrix.\n","\n","x_train=x_train.T\n","x_test=x_test.T\n","y_train=y_train.T\n","y_test=y_test.T\n","\n","print(\"x_train: \", x_train.shape)\n","print(\"x_test: \", x_test.shape)\n","print(\"y_train: \", y_train.shape)\n","print(\"y_test: \", y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"_uuid":"98e2aa82a00a8b7bd4a22cbfacf8573c4b9a6ea7"},"source":["Now, we split our data with train_test_split and we will use x_train and y_train  for Linear Regression Model. \n","* We will define the functions which we'll use in Linear Regression. \n","*  First, we will define initial weights, initial bias and sigmoid function."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6d5866615ab56dede4747cb8102bc5b95416a5d4","trusted":true},"outputs":[],"source":["# lets initialize parameters\n","# So what we need is dimension, that is, the number of features as a parameter for our initialize method(def)\n","# dimension=20\n","#initial weights=0.01, initial bias=0\n","\n","def initialize_weights_and_bias(dimension):\n","    w = np.full((dimension,1),0.01)\n","    b = 0.0\n","    return w, b\n","\n","#sigmoid function\n","\n","def sigmoid(z):\n","    \n","    y_head=1/(1+np.exp(-z))\n","    return y_head\n","    "]},{"cell_type":"markdown","metadata":{"_uuid":"93e634b68af256b0e7d52a5e50295a2381cd83d3"},"source":["<a id=\"5\"></a>\n","# Forward Backward Propagation\n","\n","**Forward propagation** is the all steps from features (x_train) to cost.\n","*  z = ((w)^T)*x + b  or we can write  z = b + p1*w1 + p2*w2 + ... + p20*w20\n","* Then compute y_head=sigmoid(z)\n","* Calculate loss(error) function= $-(1-y).\\log(1-\\widetilde{y})-y.\\log(\\widetilde{y})$; ( actually we are finding y_head for each column in x_train matrix.)\n","    * We are using loss function to decide whether our prediction is correct or not.    \n","* Cost function=Summation of all loss functions.\n","\n","**Backward propagation** means that we are updating parameters in terms of the value of Cost Funciton. So we will use y_head that we found in forward propagation.\n","\n","*  Updating: There is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with  α (learning rate). Then update weight. \n","    * w = w - learning_rate * gradients[\"derivative_bias\"]\n","* We will do the same thing for bias. \n","     *     i.e.  Take derivative of bias according to weight and bias. Then multiply it with  α (learning rate). Then update bias.\n","         * b = b - learning_rate * gradients[\"derivative_bias\"]        \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cedddfe1f7d962d6e25b9c646a23c9ba49cf37d1","trusted":true},"outputs":[],"source":["# forward backward propagation\n","\n","def forward_backward_propagation(w,b,x_train,y_train):\n","    #forward propagation\n","    z=np.dot(w.T,x_train)+b\n","    y_head=sigmoid(z)\n","    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n","    cost=(np.sum(loss))/x_train.shape[1] # x_train.shape[1] is for scaling\n","    \n","    #backward propagation\n","    # In backward propagation we will use y_head that found in forward propagation\n","    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1] is for scaling\n","    derivative_bias=np.sum(y_head-y_train)/x_train.shape[1]                   # x_train.shape[1] is for scaling\n","    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n","    \n","    return cost,gradients"]},{"cell_type":"markdown","metadata":{"_uuid":"621ef2902927a3f5063c975fb8ac6c5bdcf216c3"},"source":["When updating parameters, we need to choose wisely learning rate. Learning rate should be neither too big nor too small.\n","* Here, number_of_iterations and learning_rate are called as hyperparameter. That is, we need to set the values by hand. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"96da3d71e1faf7df1a5e5bc1988c62b7140cb606","trusted":true},"outputs":[],"source":["# Updating(learning) parameters\n","def update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n","    cost_list = []\n","    cost_list2 = []\n","    index = []\n","    # updating(learning) parameters is number_of_iterarion times\n","    for i in range(number_of_iteration):\n","        # make forward and backward propagation and find cost and gradients\n","        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n","        cost_list.append(cost)\n","        # lets update\n","        w = w - learning_rate * gradients[\"derivative_weight\"]\n","        b = b - learning_rate * gradients[\"derivative_bias\"]\n","        if i % 10 == 0:\n","            cost_list2.append(cost)\n","            index.append(i)\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    # we update(learn) parameters weights and bias\n","    parameters = {\"weight\": w,\"bias\": b}\n","    plt.plot(index,cost_list2)\n","    plt.xticks(index,rotation='vertical')\n","    plt.xlabel(\"Number of Iteration\")\n","    plt.ylabel(\"Cost\")\n","    plt.show()\n","    return parameters, gradients, cost_list"]},{"cell_type":"markdown","metadata":{"_uuid":"72cefce8dacf2bfaf51fb96df34bc32148067e45"},"source":["<a id=\"6\"></a>\n","# Prediction\n","Up to here, we do:\n","* prepare our data for LR\n","* parameters: weights and bias\n","* initialize parameters\n","* sigmoid fuction\n","* loss function\n","* Cost function\n","* updating parameters\n","* Now let's predict.  In prediction step we have x_test as input "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"92a4d6dcc9d018d25b3ebc9200f36852f4526444","trusted":true},"outputs":[],"source":["#prediction\n","def predict(w,b,x_test):\n","    # x_test is an input for forward propagation\n","    z = sigmoid(np.dot(w.T,x_test)+b)\n","    Y_prediction = np.zeros((1,x_test.shape[1]))\n","    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n","    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n","    for i in range(z.shape[1]):\n","        if z[0,i]<= 0.5:\n","            Y_prediction[0,i] = 0\n","        else:\n","            Y_prediction[0,i] = 1\n","\n","    return Y_prediction"]},{"cell_type":"markdown","metadata":{"_uuid":"f1f6a30332fd8dbe53ba320d982077e0f0e5d613"},"source":["<a id=\"7\"></a>\n","# Logistic Regression Algorithm\n","We make prediction.  Let's define logistic_regression function with learning_rate = 1, num_iterations = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5516a8d80993c0550df9e7b0c688dd68dc905d7e","trusted":true},"outputs":[],"source":["#Logistic Regression\n","\n","def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n","    # initialize\n","    dimension =  x_train.shape[0]  # that is 20\n","    w,b = initialize_weights_and_bias(dimension)\n","    # do not change learning rate\n","    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n","    \n","    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n","\n","    # Print train/test Errors\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n","    \n","logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"12725fed2b92007575b407d122f10a3e226d25f5","trusted":true},"outputs":[],"source":["logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d9124174a7a33016558bf801574ba66ea2dddb34","trusted":true},"outputs":[],"source":["logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"63329fcdc34d566c2343d2aee71e12a60844aa9e","trusted":true},"outputs":[],"source":["logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 2, num_iterations = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4ac0ff58263692c30f56cbbea0e88e0a66d2b409","trusted":true},"outputs":[],"source":["logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 300)"]},{"cell_type":"markdown","metadata":{"_uuid":"a1dc190296f326dba2291b22e7d40a0dec7f7773"},"source":["As we see, we have the best result when  learning_rate = 3, num_iterations = 300 or  learning_rate = 2, num_iterations = 500."]},{"cell_type":"markdown","metadata":{"_uuid":"de6553eef51adf825201da51d1a63324aa485ab7"},"source":["<a id=\"8\"></a>\n","# Logistic Regression with sklearn Library\n","Also, we can use sklearn library to make Linear Regression."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1c11e0019b251a69566d2d07665b3a2df44f4088","trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","lr=LogisticRegression()\n","\n","lr.fit(x_train.T,y_train.T)\n","print(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n","x_train = x_train.T\n","y_train = y_train.T\n","x_test = x_test.T\n","y_test = y_test.T\n"]},{"cell_type":"markdown","metadata":{"_uuid":"abd4cf266563e6379d708c36582b755daf92da5d"},"source":["<a id=\"9\"></a>\n","# Conclusion\n","\n","If we write all the functions we need, then we get the best result for accuracy as  \n","* test accuracy: 97.94952681388013 % when  learning_rate = 2, num_iterations = 500.\n","* But if we use sklearn libray for Linear Regression our test accuracy is 0.9810725552050473."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}
