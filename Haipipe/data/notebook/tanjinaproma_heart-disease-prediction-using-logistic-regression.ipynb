{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction using Logistic Regression\n",
    "This notebook is created for classifying binary classification of heart disease i.e. whether the patient has the 10 year risk of coronary heart disease or not. Since logistic regression is used to model binary dependent variable, i used it to estimate the probabilities of the problem.\n",
    "# Dataset\n",
    "https://www.kaggle.com/dileep070/heart-disease-prediction-using-logistic-regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4238 entries, 0 to 4237\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   male             4238 non-null   int64  \n",
      " 1   age              4238 non-null   int64  \n",
      " 2   education        4133 non-null   float64\n",
      " 3   currentSmoker    4238 non-null   int64  \n",
      " 4   cigsPerDay       4209 non-null   float64\n",
      " 5   BPMeds           4185 non-null   float64\n",
      " 6   prevalentStroke  4238 non-null   int64  \n",
      " 7   prevalentHyp     4238 non-null   int64  \n",
      " 8   diabetes         4238 non-null   int64  \n",
      " 9   totChol          4188 non-null   float64\n",
      " 10  sysBP            4238 non-null   float64\n",
      " 11  diaBP            4238 non-null   float64\n",
      " 12  BMI              4219 non-null   float64\n",
      " 13  heartRate        4237 non-null   float64\n",
      " 14  glucose          3850 non-null   float64\n",
      " 15  TenYearCHD       4238 non-null   int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 529.9 KB\n"
     ]
    }
   ],
   "source": [
    "Data = pd.read_csv(\"/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv\")\n",
    "Data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are dropping the Education feature, since it is not relevant with weither the patient will get affected with CHD or not.\n",
    "\n",
    "After that we are dropping the rows containing null values since they dont contain all the feature information regarding the specific patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "TenYearCHD         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.drop(['education'], axis=1, inplace=True)\n",
    "Data.isnull().sum()\n",
    "\n",
    "Data.dropna(axis = 0, inplace = True)\n",
    "print(Data.shape[0])\n",
    "Data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, we are creating the independent variable X and dependent variable Y from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Data.TenYearCHD.values\n",
    "X = Data.drop(['TenYearCHD'], axis = 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that the data is ready , we have to only bring them in standardized form using Scikit learn StandardScaler Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= StandardScaler()\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 2999)\n",
      "(2999,)\n",
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b $$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "\n",
    "The cost is computed by taking the summation of all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we initialize the weights and bias as vectors of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Initialize the weights and bias\n",
    "def initialize_W_b_with_zeros(num_features):\n",
    "    w = np.zeros(shape = (num_features,1))\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we defined sigmoif function.\n",
    "\n",
    "$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+ np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagation\n",
    "For forward propagation we compute\n",
    "\n",
    "$A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "\n",
    "then compute the cost function:\n",
    "\n",
    "$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Then, for backward propagation, we compute the gradients dw and db by taking the derivatives.\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward and Backward propagation function \n",
    "def propagate(w,b, X,Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(z)\n",
    "    \n",
    "    loss =  - (Y * np.log(A) + (1-Y) * np.log( 1-A) )\n",
    "    cost=  np.sum(loss)/m\n",
    "    \n",
    "    dw = (1 / m) * np.dot(X, (A-Y).T)\n",
    "    db = (1 / m) * np.sum(A-Y)\n",
    "    \n",
    "    gradient= {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return gradient, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the weights and biases\n",
    "\n",
    "Here, we update the weights and biases after every iteration of the propagation. we record the updated weights for the next iteration and also try to minimize the cost at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w,b, X,Y, num_iterations, learning_rate):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range( num_iterations ):\n",
    "        gradient, cost = propagate(w,b, X,Y)\n",
    "        \n",
    "        dw = gradient['dw']\n",
    "        db = gradient['db']\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    parameters = {\"w\": w,\n",
    "                 \"b\": b}\n",
    "    \n",
    "    gradient= {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return parameters, gradient, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "After creating the logistic regression model, we use test set for predicting the output.\n",
    "\n",
    "$\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "We take an numpy array of m columns for m data samples to predict the output $\\hat{Y}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict( w,b,X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid( np.dot(w.T , X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if A[:,i] > 0.5 :\n",
    "              Y_prediction[:,i] = 1 \n",
    "      \n",
    "    return Y_prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "It is the final function integrating all the functions. We take the number of features present in the dependent variable which is X_Train in this case. the number of features is 14 for this dataset. So we initialize weights and biases of that diemnsion. \n",
    "\n",
    "The model uses training set to learn the optimized weights and biases which then we use for test set. finally, we calculate the test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_model(X_train, X_test, Y_train, Y_test,num_iterations, learning_rate ):\n",
    "    num_features = X_train.shape[0]\n",
    "    w,b = initialize_W_b_with_zeros(num_features)\n",
    "    parameters, gradient, costs = update(w,b, X_train,Y_train, num_iterations, learning_rate)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_Test_Predict = predict(w,b, X_test)\n",
    "    \n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_Test_Predict - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    Dictionary = {\"Prediction \": Y_Test_Predict,\n",
    "                \"Weight\": w,\n",
    "                \"Bias\" :b,\n",
    "                \"Cost Function\" : costs}\n",
    "    \n",
    "    return Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 84.4 %\n"
     ]
    }
   ],
   "source": [
    "Dictionary = Logistic_Regression_model(X_train, X_test, Y_train, Y_test, num_iterations = 1000, learning_rate = 0.10 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracy: 84.4 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Plot learning curve (with costs)\n",
    "# import matplotlib.pyplot as plt\n",
    "# costs = np.squeeze(Dictionary['Cost Function'])\n",
    "# plt.plot(costs)\n",
    "# plt.ylabel('cost')\n",
    "# plt.xlabel('iterations')\n",
    "# plt.title(\"Cost Reduction\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ensuring the accuracy of our model, the Logistic Regression classifier of Sklearn is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train.T,Y_train.T)\n",
    "print(\"test accuracy {}\".format(lr.score(X_test.T,Y_test.T)))  \n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.T\n",
    "X_test = X_test.T\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
