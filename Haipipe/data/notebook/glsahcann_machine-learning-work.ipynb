{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"51e0e76ad24a41a0252eb8bf624d4a2b3bdc31c0"},"source":["**Hi. I am learning machine learning and want to share what i learn. I hope like this kernel.**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","import os\n","print(os.listdir(\"../input\"))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"source":["**Linear Regression**\n","\n","We are going to start with linear regression.\n","\n","Firstly i am going to create a dataset and set up a model of basic linear regression.\n","\n","Lets start.."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1729ccbb93bc7b51969c481838be0e6bea687945","trusted":true},"outputs":[],"source":["#Create a simple dataset from dictionary.\n","dictionary_1 = {\"dogru_soru_sayisi\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n","              \"puan\":[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]}\n","data_1 = pd.DataFrame(dictionary_1)\n","data_1.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1e4a2e091eace7afbd2f8630b18ed814439a97a4","trusted":true},"outputs":[],"source":["#Visualize the data\n","plt.scatter(data_1.dogru_soru_sayisi,data_1.puan)\n","plt.xlabel(\"dogru_soru_sayisi\")\n","plt.ylabel(\"puan\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"123d39bbddcdddd2cba4ba0f6ba257a64a30dda9"},"source":["Nice...\n","\n","Now we make linear regression."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f609ff31b89f1bed5df891561020d0a193fb2f3c","trusted":true},"outputs":[],"source":["#Lets look a type\n","type(data_1.dogru_soru_sayisi)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ce25d0b7ec9611e92f8b13ee8dffada38c7d486c","trusted":true},"outputs":[],"source":["#Want a array\n","x = data_1.dogru_soru_sayisi.values\n","y = data_1.puan.values"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ea68e58cdf25e7024f18c3c70247bb2a5e9bd8a3","trusted":true},"outputs":[],"source":["type(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c22c6316a03c072ce8da47190f9155d92e1c0d44","trusted":true},"outputs":[],"source":["#How we x of shape?\n","x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a502ef9e0be0e8cdf42f1e78948b69b9e81180a1","trusted":true},"outputs":[],"source":["#Want shape (20,1)\n","x = data_1.dogru_soru_sayisi.values.reshape(-1,1)\n","y = data_1.puan.values.reshape(-1,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9e718e90a62436acb05c632be5080dfbba182c62","trusted":true},"outputs":[],"source":["x.shape"]},{"cell_type":"markdown","metadata":{"_uuid":"22ff3cd4deb8f3548ae823373ba5d8cf8bee688d"},"source":["We organized x and y."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6a6e28712fd960673d8a8f87b272ee76858a222a","trusted":true},"outputs":[],"source":["#We need sklearn library\n","from sklearn.linear_model import LinearRegression\n","lr = LinearRegression()"]},{"cell_type":"markdown","metadata":{"_uuid":"5306e77907add49d8e6630769fe2c0356e566886"},"source":["A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0)."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"efc38f77afd8e7b43cd89fb135d66cd54aa7b888","trusted":true},"outputs":[],"source":["#We should make fit\n","lr.fit(x,y)\n","#Visualize\n","plt.scatter(x,y)\n","plt.xlabel(\"dogru_soru_sayisi\")\n","plt.ylabel(\"puan\")\n","#We should make predict\n","y_head = lr.predict(x)\n","#Visualize\n","plt.plot(x,y_head, color = \"red\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"b90be3cee6e2e6a0e14803964c3c247e9a765b85"},"source":["**Polynomial Linear Regression**\n","\n","In some cases, the relationship between variables may not be linear. In these cases, polynomial regression is used."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ffcded38037b68f69e19690470b255c3a7fb1630","trusted":true},"outputs":[],"source":["#Create a simple dataset from dictionary.\n","dictionary_2 = {\"enerji\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n","                \"guc\":[10,22,34,46,56,61,70,83,97,100,101,104,109,109,118,120,123,123,123,124]}\n","data_2 = pd.DataFrame(dictionary_2)\n","data_2.head(20)"]},{"cell_type":"markdown","metadata":{"_uuid":"4903aff06be3186620e483ce8dc5b5af20d395b3"},"source":["Sometimes, a plot of the residuals versus a predictor may suggest there is a nonlinear relationship. One way to try to account for such a relationship is through a polynomial regression model. Such a model for a single predictor.\n","\n","Y=β0+β1*X+β2*X^2+…+βh*X^h\n","\n","where h is called the degree of the polynomial. For lower degrees, the relationship has a specific name (i.e., h = 2 is called quadratic, h = 3 is called cubic, h = 4 is called quartic, and so on). Although this model allows for a nonlinear relationship between Y and X, polynomial regression is still considered linear regression since it is linear in the regression coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c336669cfcc215f5ed687c17fad104d70e4cd0cb","trusted":true},"outputs":[],"source":["#How we make polynomial regression on python?\n","x2 = data_2.enerji.values.reshape(-1,1)\n","y2 = data_2.guc.values.reshape(-1,1)\n","#Library\n","from sklearn.linear_model import LinearRegression\n","lr2 = LinearRegression() \n","#Fit\n","lr2.fit(x2,y2)\n","#Visualize\n","plt.scatter(x2,y2, color = \"blue\") #our values\n","plt.ylabel(\"guc\")\n","plt.xlabel(\"enerji\")\n","#Predict\n","y_head2 = lr.predict(x2)\n","#Visualize\n","plt.plot(x2, y_head2 , color=\"red\" )  #that does not represent our values well\n","\n","#Library\n","from sklearn.preprocessing import PolynomialFeatures\n","polynomial_regression = PolynomialFeatures(degree = 4)  #degree of the polynomial\n","#Fit\n","x_polynomial = polynomial_regression.fit_transform(x2)\n","lr3 = LinearRegression()\n","lr3.fit(x_polynomial,y2)\n","#Predict\n","y_head3 = lr3.predict(x_polynomial)\n","#Visualize\n","plt.plot(x2,y_head3, color = \"green\")  #this is better\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"_uuid":"321c0123c51e4e1bd00fda46e9bf4ebfda2c908d"},"source":["**Decision Tree Regression**\n","\n","Decision Trees are a type of Supervised Machine Learning, where data is constantly divided according to a certain parameter. The tree can be explained by two entities, decision nodes and leaves. Leaves are decisions or final results. And the decision nodes show where the data is divided."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ef4dec85564486dc055b467c16ef5f3b8e42fc37","trusted":true},"outputs":[],"source":["#How we use decision tree regression on python?\n","#We used the data(data_1) we created above.\n","x = data_1.dogru_soru_sayisi.values.reshape(-1,1)\n","y = data_1.puan.values.reshape(-1,1)\n","#Library\n","from sklearn.tree import DecisionTreeRegressor\n","dtr = DecisionTreeRegressor()\n","#Fit\n","dtr.fit(x,y)\n","#Step\n","x_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n","#Predict\n","y_head4 = dtr.predict(x_)\n","#Visualize\n","plt.scatter(x,y,color = \"red\")\n","plt.plot(x_,y_head4, color = \"green\")\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"_uuid":"78198d1f6408a83475a419c911ed0996ec9024e5"},"source":["**Logistic Regression Classification**\n","\n","The logistic regression predicts the likelihood of a result that can only have two values.The logistic regression produces a logistic curve that is limited to values between 0 and 1. Logistic regression is similar to a linear regression, except that the curve is constructed using the natural logarithm of the probabilities of the target variable instead of probability."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0d812ac5a889e48dc892cb43e86cbeac57376949","trusted":true},"outputs":[],"source":["#We build data frames from csv.\n","df = pd.read_csv(\"../input/voice.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"41f4e9dca4f0e9f108ddc2744f8c778e9717cc4b","trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"67c43c013034e7850a67acc5198a3b3811ea4ae4","trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"89786fef5e0b738bf2335b21cab37b1d98723a12","trusted":true},"outputs":[],"source":["#df = pd.read_csv(\"../input/voice.csv\")\n","df.label.unique()"]},{"cell_type":"markdown","metadata":{"_uuid":"1adc7a29a2b9a7952ca6b7669c4105392911f516"},"source":["We need to transform the output of the labels we have into 0 and 1."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9d8959be98f2721d773cba750b939f29b5c72145","trusted":true},"outputs":[],"source":["df.label = [ 1 if each == \"male\" else 0 for each in df.label]\n","df.label.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4b797f35fa2e7e3b9ad3d9f3837c559c46b2564f","trusted":true},"outputs":[],"source":["y = df.label.values\n","x_df = df.drop([\"label\"], axis = 1)"]},{"cell_type":"markdown","metadata":{"_uuid":"894bf61a28ed6355d105f5eb37b269a164e5addd"},"source":["**Normalization**\n","\n","Rescaling data to have values between 0 and 1. This is usually called feature scaling. One possible formula to achieve this is:\n","**(x - min(x))/(max(x)-min(x))** "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4cc9782f51eb9ae33cdcc7b33757cb07a70c191a","trusted":true},"outputs":[],"source":["#Normalization on python\n","x = (x_df - np.min(x_df))/(np.max(x_df) - np.min(x_df)).values\n"]},{"cell_type":"markdown","metadata":{"_uuid":"825333dee519a4afadc13897eaf960416ff7c345","trusted":true},"source":["We build x_train from %80 of our data and x_test from %20 of our data. X_train will learn and arrive y_train. Later x_test will arrive y_test same way."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9ca23d9b9f0468a7fd4f4e5844bc25e5d36c549e","trusted":true},"outputs":[],"source":["#Train test split\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n","\n","x_train = x_train.T\n","x_test = x_test.T\n","y_train = y_train.T\n","y_test = y_test.T\n","\n","print(\"x_train:\",x_train.shape)\n","print(\"x_test:\",x_test.shape)\n","print(\"y_train:\",y_train.shape)\n","print(\"y_test:\",y_test.shape)"]},{"cell_type":"markdown","metadata":{"_uuid":"3a3ed047210d195590cd666f2058d9aeb186c0eb","trusted":true},"source":["We should first determine the values of w and b."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"921bd4940aa8222c89d51f597dbe2659b7ba7870","trusted":true},"outputs":[],"source":["#Parameter initialize \n","def initialize_weights_and_bias(dimension):\n","    w = np.full((dimension,1),0.01)\n","    b = 0.0\n","    return w , b"]},{"cell_type":"markdown","metadata":{"_uuid":"da0cdc7213b77941b6570de65a8c55a31db0f051","trusted":true},"source":["The sigmoid function ensures that y_head values are between 0 and 1."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9cfdd26880791c069eb1a6165ba994a73a60e977","trusted":true},"outputs":[],"source":["#Sigmoid function\n","def sigmoid(z):\n","    y_head_ = 1/(1 + np.exp(-z))\n","    return y_head_\n"]},{"cell_type":"markdown","metadata":{"_uuid":"156e5335333b758b65cf0aed5a10b22babce0744","trusted":true},"source":["We arrive cost values from loss values with forward propagation. We want cost values min. Backward propagation allows the updating of cost values by taking derivative of w and b. We will use this function in the update function"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"eb3a297223f53ee3face1586350a8e6b3283c8f6","trusted":true},"outputs":[],"source":["def forward_backward_propagation(w,b,x_train,y_train):\n","    # forward propagation\n","    z = np.dot(w.T,x_train) + b\n","    y_head_ = sigmoid(z)\n","    loss = -y_train*np.log(y_head_)-(1-y_train)*np.log(1-y_head_)\n","    cost = (np.sum(loss))/x_train.shape[1]\n","    \n","    #backward propagation\n","    derivative_weight = (np.dot(x_train,((y_head_-y_train).T)))/x_train.shape[1]\n","    derivative_bias = np.sum(y_head_-y_train)/x_train.shape[1]\n","    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n","    return cost,gradients"]},{"cell_type":"markdown","metadata":{"_uuid":"55f201a8e371cfc159e3b21b67f54175e0bf2dc1","trusted":true},"source":["Learning rate is our learning speed. Number of iteration is our step count. We will do forward backward propagation by the number of iterations. We created cost_list2 to see the graph better."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fb8164efdb6a547e1b81e4ac150e1e1dcdc27092","trusted":true},"outputs":[],"source":["#Updating parameters\n","def update(w,b,x_train,y_train, learning_rate, num_iterations):\n","    cost_list = []\n","    cost_list2 = []\n","    index = []\n","    \n","    #Updating parameters is num_iteration times\n","    for i in range(num_iterations):\n","        # make forward and backward propagation and find cost and gradients\n","        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n","        cost_list.append(cost)\n","        # lets update\n","        w = w - learning_rate*gradients[\"derivative_weight\"]\n","        b = b - learning_rate*gradients[\"derivative_bias\"]\n","        if i % 10 == 0:\n","            cost_list2.append(cost)\n","            index.append(i)\n","            print(\"cost after iteration %i: %f\" %(i,cost))\n","            \n","            \n","    #We update parameters weights and bias\n","    parameters = {\"weight\": w, \"bias\": b}\n","    plt.plot(index,cost_list2)\n","    plt.xticks(index,rotation='vertical')\n","    plt.xlabel(\"Number of iterarion\")\n","    plt.ylabel(\"Cost\")\n","    plt.show()\n","    return parameters,gradients,cost_list"]},{"cell_type":"markdown","metadata":{"_uuid":"973097efd92b351110e7dd4dea0762869245bf21","trusted":true},"source":["We will guess with x_test data. We will arrive y_predict values."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b7e527b3ba020c306b7268f6674fc6a55e67447b","trusted":true},"outputs":[],"source":["#Prediction\n","def predict(w,b,x_test):\n","    # x_test is a input for forward propagation\n","    z = sigmoid(np.dot(w.T,x_test)+b)\n","    y_predict = np.zeros((1,x_test.shape[1]))\n","    # if z is bigger than 0.5, our prediction is sign one (y_head_=1),\n","    # if z is smaller than 0.5, our prediction is sign zero (y_head_=0)\n","    for i in range (z.shape[1]):\n","        if z[0,i] <= 0.5:\n","            y_predict[0,i] = 0\n","        else:\n","            y_predict[0,i] = 1\n","            \n","    return y_predict"]},{"cell_type":"markdown","metadata":{"_uuid":"949d7e567049c510eb47321e671686920ddface3","trusted":true},"source":["How much the y_predict values match the y_test values?"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"26666b22ebdc91d76208498fc3f53c4ba9c96642","trusted":true},"outputs":[],"source":["#Logistic regression\n","def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n","    # initialize\n","    dimension = x_train.shape[0]  # that is 30\n","    w,b = initialize_weights_and_bias(dimension)\n","\n","    parameters, gradients, cost_list = update(w,b,x_train,y_train, learning_rate, num_iterations)\n","    \n","    y_predict_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n","    \n","    # print train/test errors\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_predict_test - y_test))*100))\n","    \n","logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, num_iterations= 300)"]},{"cell_type":"markdown","metadata":{"_uuid":"12b9652504b70599a2323ae512789b3259a9e3e7","trusted":true},"source":["Logistic Regression with sklearn. It is more easy."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"eb4b750f19ecf55ded128e900397a13221343c04","trusted":true},"outputs":[],"source":["#Library\n","from sklearn.linear_model import LogisticRegression\n","logistic_regression = LogisticRegression()\n","logistic_regression.fit(x_train.T,y_train.T)\n","print(\"test accuracy {}\".format(logistic_regression.score(x_test.T,y_test.T)))\n","x_train = x_train.T\n","y_train = y_train.T\n","x_test = x_test.T\n","y_test = y_test.T\n"]},{"cell_type":"markdown","metadata":{"_uuid":"c29c0c6701abe083529040441ad709522223abf3","trusted":true},"source":["That's all for now. I will keep to publish by more learning. Thank you."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}
