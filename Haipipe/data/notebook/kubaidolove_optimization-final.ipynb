{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "dataset = pd.read_csv('../input/heart-disease-uci/heart.csv')\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Encoding Categorical Data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#cp\n",
    "\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [2])],     remainder='passthrough')\n",
    "X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "X = X[:, 1:]\n",
    "#restecg\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [8])],     remainder='passthrough')\n",
    "X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "X = X[:, 1:]\n",
    "#slope\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [13])],     remainder='passthrough')\n",
    "X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "X = X[:, 1:]\n",
    "#ca\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [15])],     remainder='passthrough')\n",
    "X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "X = X[:, 1:]\n",
    "#thal\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [19])],     remainder='passthrough')\n",
    "X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "X = X[:, 1:]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler()\n",
    "X = scalerX.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :\n",
      "Accuracy =  0.8461538461538461\n",
      "[[34 10]\n",
      " [ 4 43]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(XTrain,yTrain)\n",
    "yPred = classifier.predict(XTest)\n",
    "cm = confusion_matrix(yTest,yPred)\n",
    "accuracy = accuracy_score(yTest,yPred)\n",
    "print(\"Logistic Regression :\")\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors :\n",
      "Accuracy =  0.8131868131868132\n",
      "[[32 12]\n",
      " [ 5 42]]\n"
     ]
    }
   ],
   "source": [
    "#K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "classifier.fit(XTrain,yTrain)\n",
    "yPred = classifier.predict(XTest)\n",
    "cm = confusion_matrix(yTest,yPred)\n",
    "accuracy = accuracy_score(yTest,yPred)\n",
    "print(\"K Nearest Neighbors :\")\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes :\n",
      "Accuracy =  0.8131868131868132\n",
      "[[33 11]\n",
      " [ 6 41]]\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(XTrain,yTrain)\n",
    "yPred = classifier.predict(XTest)\n",
    "cm = confusion_matrix(yTest,yPred)\n",
    "accuracy = accuracy_score(yTest,yPred)\n",
    "print(\"Gaussian Naive Bayes :\")\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier :\n",
      "Accuracy =  0.6813186813186813\n",
      "[[31 13]\n",
      " [16 31]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "classifier = DT(criterion='entropy', random_state=0)\n",
    "classifier.fit(XTrain,yTrain)\n",
    "yPred = classifier.predict(XTest)\n",
    "cm = confusion_matrix(yTest,yPred)\n",
    "accuracy = accuracy_score(yTest,yPred)\n",
    "print(\"Decision Tree Classifier :\")\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier :\n",
      "Accuracy =  0.8241758241758241\n",
      "[[34 10]\n",
      " [ 6 41]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "classifier = RF(n_estimators=10, criterion='entropy', random_state=0)\n",
    "classifier.fit(XTrain,yTrain)\n",
    "yPred = classifier.predict(XTest)\n",
    "cm = confusion_matrix(yTest,yPred)\n",
    "accuracy = accuracy_score(yTest,yPred)\n",
    "print(\"Random Forest Classifier :\")\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                276       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 445\n",
      "Trainable params: 445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "212/212 [==============================] - 0s 2ms/step - loss: 0.6597 - accuracy: 0.7264\n",
      "Epoch 2/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.4395 - accuracy: 0.8160\n",
      "Epoch 3/200\n",
      "212/212 [==============================] - 0s 478us/step - loss: 0.3557 - accuracy: 0.8443\n",
      "Epoch 4/200\n",
      "212/212 [==============================] - 0s 460us/step - loss: 0.3295 - accuracy: 0.8491\n",
      "Epoch 5/200\n",
      "212/212 [==============================] - 0s 450us/step - loss: 0.3132 - accuracy: 0.8585\n",
      "Epoch 6/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.2991 - accuracy: 0.8726\n",
      "Epoch 7/200\n",
      "212/212 [==============================] - 0s 447us/step - loss: 0.2889 - accuracy: 0.8915\n",
      "Epoch 8/200\n",
      "212/212 [==============================] - 0s 461us/step - loss: 0.2801 - accuracy: 0.8868\n",
      "Epoch 9/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.2712 - accuracy: 0.8915\n",
      "Epoch 10/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.2649 - accuracy: 0.8915\n",
      "Epoch 11/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.2587 - accuracy: 0.9009\n",
      "Epoch 12/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.2516 - accuracy: 0.9057\n",
      "Epoch 13/200\n",
      "212/212 [==============================] - 0s 469us/step - loss: 0.2440 - accuracy: 0.9151\n",
      "Epoch 14/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.2350 - accuracy: 0.9151\n",
      "Epoch 15/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.2254 - accuracy: 0.9198\n",
      "Epoch 16/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.2191 - accuracy: 0.9245\n",
      "Epoch 17/200\n",
      "212/212 [==============================] - 0s 458us/step - loss: 0.2075 - accuracy: 0.9340\n",
      "Epoch 18/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.1982 - accuracy: 0.9245\n",
      "Epoch 19/200\n",
      "212/212 [==============================] - 0s 470us/step - loss: 0.1884 - accuracy: 0.9340\n",
      "Epoch 20/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.1762 - accuracy: 0.9434\n",
      "Epoch 21/200\n",
      "212/212 [==============================] - 0s 466us/step - loss: 0.1695 - accuracy: 0.9528\n",
      "Epoch 22/200\n",
      "212/212 [==============================] - 0s 487us/step - loss: 0.1624 - accuracy: 0.9481\n",
      "Epoch 23/200\n",
      "212/212 [==============================] - 0s 462us/step - loss: 0.1539 - accuracy: 0.9623\n",
      "Epoch 24/200\n",
      "212/212 [==============================] - 0s 468us/step - loss: 0.1465 - accuracy: 0.9623\n",
      "Epoch 25/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.1396 - accuracy: 0.9623\n",
      "Epoch 26/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.1297 - accuracy: 0.9717\n",
      "Epoch 27/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.1264 - accuracy: 0.9575\n",
      "Epoch 28/200\n",
      "212/212 [==============================] - 0s 468us/step - loss: 0.1171 - accuracy: 0.9717\n",
      "Epoch 29/200\n",
      "212/212 [==============================] - 0s 455us/step - loss: 0.1127 - accuracy: 0.9764\n",
      "Epoch 30/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.1069 - accuracy: 0.9764\n",
      "Epoch 31/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.1014 - accuracy: 0.9764\n",
      "Epoch 32/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0942 - accuracy: 0.9811\n",
      "Epoch 33/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0885 - accuracy: 0.9811\n",
      "Epoch 34/200\n",
      "212/212 [==============================] - 0s 474us/step - loss: 0.0835 - accuracy: 0.9811\n",
      "Epoch 35/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.0777 - accuracy: 0.9811\n",
      "Epoch 36/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0729 - accuracy: 0.9811\n",
      "Epoch 37/200\n",
      "212/212 [==============================] - 0s 450us/step - loss: 0.0691 - accuracy: 0.9811\n",
      "Epoch 38/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0650 - accuracy: 0.9811\n",
      "Epoch 39/200\n",
      "212/212 [==============================] - 0s 468us/step - loss: 0.0635 - accuracy: 0.9811\n",
      "Epoch 40/200\n",
      "212/212 [==============================] - 0s 458us/step - loss: 0.0592 - accuracy: 0.9811\n",
      "Epoch 41/200\n",
      "212/212 [==============================] - 0s 470us/step - loss: 0.0524 - accuracy: 0.9858\n",
      "Epoch 42/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0483 - accuracy: 0.9858\n",
      "Epoch 43/200\n",
      "212/212 [==============================] - 0s 446us/step - loss: 0.0456 - accuracy: 0.9858\n",
      "Epoch 44/200\n",
      "212/212 [==============================] - 0s 472us/step - loss: 0.0411 - accuracy: 0.9858\n",
      "Epoch 45/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0388 - accuracy: 0.9858\n",
      "Epoch 46/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.0349 - accuracy: 0.9858\n",
      "Epoch 47/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.0319 - accuracy: 0.9858\n",
      "Epoch 48/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0299 - accuracy: 0.9858\n",
      "Epoch 49/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0273 - accuracy: 0.9858\n",
      "Epoch 50/200\n",
      "212/212 [==============================] - 0s 455us/step - loss: 0.0247 - accuracy: 0.9858\n",
      "Epoch 51/200\n",
      "212/212 [==============================] - 0s 458us/step - loss: 0.0234 - accuracy: 0.9906\n",
      "Epoch 52/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0216 - accuracy: 0.9953\n",
      "Epoch 53/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "212/212 [==============================] - 0s 469us/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "212/212 [==============================] - 0s 467us/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "212/212 [==============================] - 0s 468us/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "212/212 [==============================] - 0s 462us/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.0152 - accuracy: 0.9953\n",
      "Epoch 60/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.0147 - accuracy: 0.9953\n",
      "Epoch 61/200\n",
      "212/212 [==============================] - 0s 461us/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "212/212 [==============================] - 0s 469us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "212/212 [==============================] - 0s 461us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "212/212 [==============================] - 0s 482us/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "212/212 [==============================] - 0s 472us/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "212/212 [==============================] - 0s 462us/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "212/212 [==============================] - 0s 458us/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "212/212 [==============================] - 0s 473us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "212/212 [==============================] - 0s 450us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "212/212 [==============================] - 0s 468us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "212/212 [==============================] - 0s 447us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "212/212 [==============================] - 0s 473us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "212/212 [==============================] - 0s 461us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "212/212 [==============================] - 0s 478us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "212/212 [==============================] - 0s 464us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "212/212 [==============================] - 0s 458us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "212/212 [==============================] - 0s 460us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "212/212 [==============================] - 0s 486us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "212/212 [==============================] - 0s 470us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "212/212 [==============================] - 0s 450us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "212/212 [==============================] - 0s 462us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "212/212 [==============================] - 0s 464us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "212/212 [==============================] - 0s 446us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "212/212 [==============================] - 0s 470us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "212/212 [==============================] - 0s 466us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "212/212 [==============================] - 0s 455us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "212/212 [==============================] - 0s 470us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "212/212 [==============================] - 0s 469us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.0166 - accuracy: 0.9953\n",
      "Epoch 138/200\n",
      "212/212 [==============================] - 0s 463us/step - loss: 0.2435 - accuracy: 0.9528\n",
      "Epoch 139/200\n",
      "212/212 [==============================] - 0s 465us/step - loss: 0.0446 - accuracy: 0.9811\n",
      "Epoch 140/200\n",
      "212/212 [==============================] - 0s 461us/step - loss: 0.0409 - accuracy: 0.9906\n",
      "Epoch 141/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "212/212 [==============================] - 0s 450us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "212/212 [==============================] - 0s 445us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "212/212 [==============================] - 0s 460us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "212/212 [==============================] - 0s 466us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "212/212 [==============================] - 0s 464us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "212/212 [==============================] - 0s 464us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "212/212 [==============================] - 0s 453us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "212/212 [==============================] - 0s 451us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "212/212 [==============================] - 0s 466us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "212/212 [==============================] - 0s 452us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "212/212 [==============================] - 0s 456us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "212/212 [==============================] - 0s 457us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "212/212 [==============================] - 0s 447us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "212/212 [==============================] - 0s 446us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "212/212 [==============================] - 0s 455us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "212/212 [==============================] - 0s 454us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "212/212 [==============================] - 0s 445us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "212/212 [==============================] - 0s 466us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "212/212 [==============================] - 0s 449us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "212/212 [==============================] - 0s 462us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "212/212 [==============================] - 0s 448us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "212/212 [==============================] - 0s 674us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "212/212 [==============================] - 0s 558us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "212/212 [==============================] - 0s 586us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "212/212 [==============================] - 0s 627us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "212/212 [==============================] - 0s 596us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "212/212 [==============================] - 0s 552us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "212/212 [==============================] - 0s 597us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "212/212 [==============================] - 0s 473us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "212/212 [==============================] - 0s 478us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "212/212 [==============================] - 0s 478us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "212/212 [==============================] - 0s 478us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "212/212 [==============================] - 0s 473us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "212/212 [==============================] - 0s 484us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "212/212 [==============================] - 0s 479us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "212/212 [==============================] - 0s 509us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "212/212 [==============================] - 0s 477us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "212/212 [==============================] - 0s 469us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "212/212 [==============================] - 0s 484us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "212/212 [==============================] - 0s 480us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "212/212 [==============================] - 0s 485us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "212/212 [==============================] - 0s 480us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "212/212 [==============================] - 0s 478us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "212/212 [==============================] - 0s 483us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "212/212 [==============================] - 0s 500us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "212/212 [==============================] - 0s 485us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "212/212 [==============================] - 0s 468us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "212/212 [==============================] - 0s 469us/step - loss: 0.0012 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Neural Network Classifier :\n",
      "Accuracy =  0.7692307692307693\n",
      "[[34 10]\n",
      " [11 36]]\n"
     ]
    }
   ],
   "source": [
    "#Artificial Neural Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Initialising ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "#Adding the first hidden layer or the input layer\n",
    "classifier.add(Dense(activation='relu',\n",
    "                     kernel_initializer='uniform',\n",
    "                     input_dim=22,\n",
    "                     units=12))\n",
    "#Adding the second hidden layer\n",
    "classifier.add(Dense(activation='relu',\n",
    "                     kernel_initializer='uniform',\n",
    "                     units=12))\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(activation='sigmoid',\n",
    "                     kernel_initializer='uniform',\n",
    "                     units=1))\n",
    "\n",
    "#Compiling the ANN\n",
    "classifier.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(classifier.summary())\n",
    "\n",
    "#Fitting the ANN\n",
    "history = classifier.fit(XTrain, yTrain, batch_size=3, epochs=200, verbose=1)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'],'green')\n",
    "plt.plot(history.history['loss'],'red')\n",
    "plt.title('Model Accuracy-Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Accuracy','Loss'])\n",
    "plt.show()\n",
    "\n",
    "#Predicting the Test set Results\n",
    "yPred = classifier.predict(XTest)\n",
    "yPred = (yPred>0.5) #Since output is probability\n",
    "cm = confusion_matrix(yTest,yPred)\n",
    "accuracy = accuracy_score(yTest,yPred)\n",
    "print(\"Artificial Neural Network Classifier :\")\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
