{"cells":[{"metadata":{},"cell_type":"markdown","source":"It is a very basic kernel for someone who is gettiing started in the field of data science.\nI have tried to predict loan application approvals with a relatively small dataset.\nBelow are the steps that have been carried out in the Kernel.\n1. Pre Processing the Dataset\n2. Splitting the Input and the Target\n3. Standardizing the Dataset\n4. Splitting the Input and Target into Test and Train dataset\n5. Creating the Model\n6. Testing the Model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the Relevant libraries to be used in the Kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nfrom sklearn.linear_model import LogisticRegressionCV, SGDClassifier, LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the Dataset"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"raw_csv_data = pd.read_csv(\"../input/loan-predication/train_u6lujuX_CVtuZ9i (1).csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the first step is preprocessing.\nIn preprocessing we will try to identify the following points.\n1. Checking the numerical and categorical variables.\n2. Checking for the missing values.\n3. Filling the missing values with dummy values.\n4. Mapping the categorical variables\n5. Deleting the unwanted columns."},{"metadata":{},"cell_type":"markdown","source":"I will use the Pandas Profiling to see the different variables used in the dataset.\nIt will help us identify the missing values, outliers in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_csv_data.profile_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Map all the categorical values with numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_csv_data['Gender'] = raw_csv_data['Gender'].map({'Male':1,'Female':0})\nraw_csv_data['Married'] = raw_csv_data['Married'].map({'Yes':1,'No':0})\nraw_csv_data['Education'] = raw_csv_data['Education'].map({'Graduate':1,'Not Graduate':0})\nraw_csv_data['Self_Employed'] = raw_csv_data['Self_Employed'].map({'Yes':1,'No':0})\nraw_csv_data['Property_Area'] = raw_csv_data['Property_Area'].map({'Urban':1,'Rural':2,'Semiurban':3})\nraw_csv_data['Loan_Status'] = raw_csv_data['Loan_Status'].map({'Y':1,'N':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_csv_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that the column Dependents have some values as 3+.\nWe will create 4 cloumns as Dependent_1, Dependent_2, Dependent_3, Dependent_4 for each(0,1,2,3+) and map all the values with 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keeping a checkpoint\npre_process_data = raw_csv_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Insert new columns for dependents\npre_process_data.insert(0, \"Dependents_1\", 0) \npre_process_data.insert(1, \"Dependents_2\", 0) \npre_process_data.insert(2, \"Dependents_3\", 0) \npre_process_data.insert(3, \"Dependents_4\", 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill all the values for newly created Dependent Column.\nfor ind in pre_process_data.index: \n     if(pre_process_data['Dependents'][ind] == 0):\n        pre_process_data['Dependents_1'][ind] = 1\n     elif(pre_process_data['Dependents'][ind] == 1):\n        pre_process_data['Dependents_2'][ind] = 1\n     elif(pre_process_data['Dependents'][ind] == 2):\n        pre_process_data['Dependents_3'][ind] = 1\n     elif(pre_process_data['Dependents'][ind] == '3+'):\n        pre_process_data['Dependents_4'][ind] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Drop the Loan Id and the Dependent Column.\npre_process_data = pre_process_data.drop(['Dependents'], axis=1)\npre_process_data = pre_process_data.drop(['Loan_ID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see that there are lot of missing values in the dataset, we will use the Simple Impute from the scikit library to fill the missing values with the dummy values."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Get Dummies\ndummies= pd.get_dummies(pre_process_data, drop_first=True)\n\n# We will now impute values\nSimImp = SimpleImputer()\npre_process_data = pd.DataFrame(SimImp.fit_transform(dummies), columns=dummies.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##All the missing values have been filled.\npre_process_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the Column Loan Status is our Target, we will split the target.\nAfter that we will Standardize the Input by using Standard Scaler of Sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs=pre_process_data.iloc[:,:-1]\n\n##Standardize the input\nfrom sklearn.preprocessing import StandardScaler\nloan_scaler = StandardScaler()\nloan_scaler.fit(unscaled_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_inputs = loan_scaler.transform(unscaled_inputs)\nscaled_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Splitting the Target\ntargets = pre_process_data['Loan_Status'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into Test and Train using test_train_split from Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Split the data into train test and shuffle\nfrom sklearn.model_selection import train_test_split\ntrain_test_split(scaled_inputs,targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":" ##Logistic Regression with SKlearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs,targets, train_size=0.8)\n\n##Training the Model\nreg=LogisticRegression()\nreg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the Accuracy of the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see that there is 80% accuracy of the Model.\nNow we will test our model with the test data that we had splitted before."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Testing the Model\nreg.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Model gives around 82% accuracy with the test data.\nThanks for seeing the kernel.\nPlease provide your Comments and suggestion as I am still in the initial learning phase."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}