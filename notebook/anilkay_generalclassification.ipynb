{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ph-recognition/ph-data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>84</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>164</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>205</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>223</td>\n",
       "      <td>221</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blue  green  red  label\n",
       "0    36     27  231      0\n",
       "1    36     84  250      1\n",
       "2    37    164  255      2\n",
       "3    22    205  255      3\n",
       "4    38    223  221      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/ph-recognition/ph-data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,0:3]\n",
    "y=data.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I think, Decision Tree work best for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc=DecisionTreeClassifier(criterion= \"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7195121951219512\n",
      "[[ 8  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  2  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  2  7  2  0  1  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  8  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  7  0  1  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  9  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  4  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11  1  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0 10  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  1  9  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  5  1]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  1  3 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89         9\n",
      "           1       0.82      0.82      0.82        11\n",
      "           2       0.55      0.60      0.57        10\n",
      "           3       0.64      0.50      0.56        14\n",
      "           4       0.62      0.73      0.67        11\n",
      "           5       0.70      0.64      0.67        11\n",
      "           6       0.75      0.69      0.72        13\n",
      "           7       0.40      0.67      0.50         6\n",
      "           8       0.50      0.60      0.55         5\n",
      "           9       0.85      0.85      0.85        13\n",
      "          10       0.83      0.83      0.83        12\n",
      "          11       0.90      0.64      0.75        14\n",
      "          12       0.62      0.89      0.73         9\n",
      "          13       0.62      0.71      0.67         7\n",
      "          14       0.93      0.74      0.82        19\n",
      "\n",
      "    accuracy                           0.72       164\n",
      "   macro avg       0.71      0.72      0.71       164\n",
      "weighted avg       0.74      0.72      0.72       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc.fit(x_train,y_train)\n",
    "ypred=dtc.predict(x_test)\n",
    "import sklearn.metrics as metrik\n",
    "print(metrik.accuracy_score(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.confusion_matrix(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.classification_report(y_true=y_test,y_pred=ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy is not good enough with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scale_x=scaler.fit_transform(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scale_x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(60,kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(15,kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scale_x, dummy_y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "489/489 [==============================] - 0s 752us/step - loss: 2.6964 - acc: 0.1943\n",
      "Epoch 2/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 2.6404 - acc: 0.3006\n",
      "Epoch 3/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 2.4850 - acc: 0.2883\n",
      "Epoch 4/200\n",
      "489/489 [==============================] - 0s 58us/step - loss: 2.1960 - acc: 0.2883\n",
      "Epoch 5/200\n",
      "489/489 [==============================] - 0s 62us/step - loss: 1.9073 - acc: 0.2965\n",
      "Epoch 6/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 1.6701 - acc: 0.3763\n",
      "Epoch 7/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.5116 - acc: 0.4397\n",
      "Epoch 8/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.4084 - acc: 0.5665\n",
      "Epoch 9/200\n",
      "489/489 [==============================] - 0s 63us/step - loss: 1.3393 - acc: 0.5317\n",
      "Epoch 10/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.3011 - acc: 0.5706\n",
      "Epoch 11/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.2660 - acc: 0.5317\n",
      "Epoch 12/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 1.2351 - acc: 0.5460\n",
      "Epoch 13/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.2162 - acc: 0.5930\n",
      "Epoch 14/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 1.1976 - acc: 0.6135\n",
      "Epoch 15/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.1815 - acc: 0.5726\n",
      "Epoch 16/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.1639 - acc: 0.6237\n",
      "Epoch 17/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 1.1561 - acc: 0.6115\n",
      "Epoch 18/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.1393 - acc: 0.6074\n",
      "Epoch 19/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.1293 - acc: 0.6299\n",
      "Epoch 20/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 1.1204 - acc: 0.6278\n",
      "Epoch 21/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.1060 - acc: 0.6483\n",
      "Epoch 22/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.1012 - acc: 0.6503\n",
      "Epoch 23/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 1.0852 - acc: 0.6605\n",
      "Epoch 24/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.0854 - acc: 0.6646\n",
      "Epoch 25/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.0628 - acc: 0.6728\n",
      "Epoch 26/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.0558 - acc: 0.6933\n",
      "Epoch 27/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.0525 - acc: 0.7014\n",
      "Epoch 28/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.0415 - acc: 0.6687\n",
      "Epoch 29/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 1.0337 - acc: 0.6912\n",
      "Epoch 30/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 1.0331 - acc: 0.6748\n",
      "Epoch 31/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 1.0183 - acc: 0.6953\n",
      "Epoch 32/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 1.0144 - acc: 0.6892\n",
      "Epoch 33/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 1.0127 - acc: 0.6912\n",
      "Epoch 34/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.9985 - acc: 0.7137\n",
      "Epoch 35/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.9876 - acc: 0.7178\n",
      "Epoch 36/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.9879 - acc: 0.6912\n",
      "Epoch 37/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.9829 - acc: 0.6973\n",
      "Epoch 38/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.9761 - acc: 0.7076\n",
      "Epoch 39/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.9656 - acc: 0.7076\n",
      "Epoch 40/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.9579 - acc: 0.7321\n",
      "Epoch 41/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.9537 - acc: 0.7178\n",
      "Epoch 42/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.9445 - acc: 0.7137\n",
      "Epoch 43/200\n",
      "489/489 [==============================] - 0s 56us/step - loss: 0.9395 - acc: 0.7198\n",
      "Epoch 44/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.9323 - acc: 0.7362\n",
      "Epoch 45/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.9379 - acc: 0.7239\n",
      "Epoch 46/200\n",
      "489/489 [==============================] - 0s 61us/step - loss: 0.9325 - acc: 0.7444\n",
      "Epoch 47/200\n",
      "489/489 [==============================] - 0s 59us/step - loss: 0.9208 - acc: 0.7321\n",
      "Epoch 48/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 0.9236 - acc: 0.7260\n",
      "Epoch 49/200\n",
      "489/489 [==============================] - 0s 61us/step - loss: 0.9198 - acc: 0.7096\n",
      "Epoch 50/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 0.9103 - acc: 0.7301\n",
      "Epoch 51/200\n",
      "489/489 [==============================] - 0s 55us/step - loss: 0.9145 - acc: 0.7219\n",
      "Epoch 52/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 0.9091 - acc: 0.7362\n",
      "Epoch 53/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.8989 - acc: 0.7076\n",
      "Epoch 54/200\n",
      "489/489 [==============================] - 0s 55us/step - loss: 0.8906 - acc: 0.7403\n",
      "Epoch 55/200\n",
      "489/489 [==============================] - 0s 62us/step - loss: 0.8905 - acc: 0.7239\n",
      "Epoch 56/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 0.8912 - acc: 0.7362\n",
      "Epoch 57/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 0.8823 - acc: 0.7280\n",
      "Epoch 58/200\n",
      "489/489 [==============================] - 0s 56us/step - loss: 0.8775 - acc: 0.7362\n",
      "Epoch 59/200\n",
      "489/489 [==============================] - 0s 58us/step - loss: 0.8863 - acc: 0.7444\n",
      "Epoch 60/200\n",
      "489/489 [==============================] - 0s 56us/step - loss: 0.8798 - acc: 0.6994\n",
      "Epoch 61/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 0.8733 - acc: 0.7464\n",
      "Epoch 62/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.8674 - acc: 0.7382\n",
      "Epoch 63/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.8536 - acc: 0.7485\n",
      "Epoch 64/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.8594 - acc: 0.7485\n",
      "Epoch 65/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.8531 - acc: 0.7485\n",
      "Epoch 66/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.8385 - acc: 0.7566\n",
      "Epoch 67/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.8418 - acc: 0.7403\n",
      "Epoch 68/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.8388 - acc: 0.7239\n",
      "Epoch 69/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.8308 - acc: 0.7607\n",
      "Epoch 70/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 0.8340 - acc: 0.7239\n",
      "Epoch 71/200\n",
      "489/489 [==============================] - 0s 53us/step - loss: 0.8284 - acc: 0.7485\n",
      "Epoch 72/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.8350 - acc: 0.7403\n",
      "Epoch 73/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.8193 - acc: 0.7526\n",
      "Epoch 74/200\n",
      "489/489 [==============================] - 0s 53us/step - loss: 0.8182 - acc: 0.7546\n",
      "Epoch 75/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.8183 - acc: 0.7505\n",
      "Epoch 76/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.8138 - acc: 0.7505\n",
      "Epoch 77/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 0.8110 - acc: 0.7546\n",
      "Epoch 78/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.8066 - acc: 0.7464\n",
      "Epoch 79/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.8053 - acc: 0.7587\n",
      "Epoch 80/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.8094 - acc: 0.7464\n",
      "Epoch 81/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.8054 - acc: 0.7464\n",
      "Epoch 82/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.7984 - acc: 0.7546\n",
      "Epoch 83/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.8032 - acc: 0.7423\n",
      "Epoch 84/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7945 - acc: 0.7607\n",
      "Epoch 85/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.7996 - acc: 0.7464\n",
      "Epoch 86/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7911 - acc: 0.7546\n",
      "Epoch 87/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.7933 - acc: 0.7403\n",
      "Epoch 88/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.7802 - acc: 0.7566\n",
      "Epoch 89/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.7907 - acc: 0.7546\n",
      "Epoch 90/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.7840 - acc: 0.7464\n",
      "Epoch 91/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7755 - acc: 0.7628\n",
      "Epoch 92/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.7729 - acc: 0.7485\n",
      "Epoch 93/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.7775 - acc: 0.7526\n",
      "Epoch 94/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.7715 - acc: 0.7526\n",
      "Epoch 95/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.7760 - acc: 0.7382\n",
      "Epoch 96/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.7773 - acc: 0.7505\n",
      "Epoch 97/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7733 - acc: 0.7505\n",
      "Epoch 98/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.7730 - acc: 0.7423\n",
      "Epoch 99/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7745 - acc: 0.7464\n",
      "Epoch 100/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7845 - acc: 0.7301\n",
      "Epoch 101/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.7577 - acc: 0.7526\n",
      "Epoch 102/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7624 - acc: 0.7566\n",
      "Epoch 103/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7673 - acc: 0.7607\n",
      "Epoch 104/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7519 - acc: 0.7587\n",
      "Epoch 105/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7648 - acc: 0.7526\n",
      "Epoch 106/200\n",
      "489/489 [==============================] - 0s 51us/step - loss: 0.7678 - acc: 0.7260\n",
      "Epoch 107/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7606 - acc: 0.7505\n",
      "Epoch 108/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7513 - acc: 0.7566\n",
      "Epoch 109/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7458 - acc: 0.7587\n",
      "Epoch 110/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.7512 - acc: 0.7546\n",
      "Epoch 111/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7416 - acc: 0.7587\n",
      "Epoch 112/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7496 - acc: 0.7566\n",
      "Epoch 113/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7389 - acc: 0.7607\n",
      "Epoch 114/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7503 - acc: 0.7587\n",
      "Epoch 115/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7402 - acc: 0.7607\n",
      "Epoch 116/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7341 - acc: 0.7526\n",
      "Epoch 117/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7396 - acc: 0.7587\n",
      "Epoch 118/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.7314 - acc: 0.7628\n",
      "Epoch 119/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 0.7308 - acc: 0.7628\n",
      "Epoch 120/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.7325 - acc: 0.7628\n",
      "Epoch 121/200\n",
      "489/489 [==============================] - 0s 58us/step - loss: 0.7304 - acc: 0.7710\n",
      "Epoch 122/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7279 - acc: 0.7566\n",
      "Epoch 123/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7264 - acc: 0.7566\n",
      "Epoch 124/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7259 - acc: 0.7628\n",
      "Epoch 125/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7213 - acc: 0.7566\n",
      "Epoch 126/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7168 - acc: 0.7689\n",
      "Epoch 127/200\n",
      "489/489 [==============================] - 0s 64us/step - loss: 0.7168 - acc: 0.7669\n",
      "Epoch 128/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7123 - acc: 0.7566\n",
      "Epoch 129/200\n",
      "489/489 [==============================] - 0s 57us/step - loss: 0.7262 - acc: 0.7710\n",
      "Epoch 130/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.7184 - acc: 0.7628\n",
      "Epoch 131/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7161 - acc: 0.7689\n",
      "Epoch 132/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7137 - acc: 0.7607\n",
      "Epoch 133/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7081 - acc: 0.7669\n",
      "Epoch 134/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7121 - acc: 0.7587\n",
      "Epoch 135/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7115 - acc: 0.7648\n",
      "Epoch 136/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7043 - acc: 0.7669\n",
      "Epoch 137/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7101 - acc: 0.7669\n",
      "Epoch 138/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7107 - acc: 0.7546\n",
      "Epoch 139/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6979 - acc: 0.7669\n",
      "Epoch 140/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7126 - acc: 0.7464\n",
      "Epoch 141/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7045 - acc: 0.7669\n",
      "Epoch 142/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7038 - acc: 0.7648\n",
      "Epoch 143/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7026 - acc: 0.7607\n",
      "Epoch 144/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.7024 - acc: 0.7669\n",
      "Epoch 145/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.7068 - acc: 0.7566\n",
      "Epoch 146/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6977 - acc: 0.7648\n",
      "Epoch 147/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.7078 - acc: 0.7689\n",
      "Epoch 148/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.7076 - acc: 0.7505\n",
      "Epoch 149/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6930 - acc: 0.7669\n",
      "Epoch 150/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6959 - acc: 0.7587\n",
      "Epoch 151/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.6961 - acc: 0.7771\n",
      "Epoch 152/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.6934 - acc: 0.7566\n",
      "Epoch 153/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.7077 - acc: 0.7689\n",
      "Epoch 154/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6934 - acc: 0.7546\n",
      "Epoch 155/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.6840 - acc: 0.7771\n",
      "Epoch 156/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6871 - acc: 0.7689\n",
      "Epoch 157/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 0.6898 - acc: 0.7648\n",
      "Epoch 158/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6763 - acc: 0.7566\n",
      "Epoch 159/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6856 - acc: 0.7791\n",
      "Epoch 160/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6840 - acc: 0.7648\n",
      "Epoch 161/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.6834 - acc: 0.7628\n",
      "Epoch 162/200\n",
      "489/489 [==============================] - 0s 56us/step - loss: 0.6837 - acc: 0.7710\n",
      "Epoch 163/200\n",
      "489/489 [==============================] - 0s 55us/step - loss: 0.6779 - acc: 0.7689\n",
      "Epoch 164/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6785 - acc: 0.7710\n",
      "Epoch 165/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6737 - acc: 0.7710\n",
      "Epoch 166/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6803 - acc: 0.7771\n",
      "Epoch 167/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6700 - acc: 0.7648\n",
      "Epoch 168/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6636 - acc: 0.7669\n",
      "Epoch 169/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.6759 - acc: 0.7628\n",
      "Epoch 170/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.6714 - acc: 0.7710\n",
      "Epoch 171/200\n",
      "489/489 [==============================] - 0s 52us/step - loss: 0.6702 - acc: 0.7669\n",
      "Epoch 172/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6733 - acc: 0.7832\n",
      "Epoch 173/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6797 - acc: 0.7607\n",
      "Epoch 174/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6611 - acc: 0.7689\n",
      "Epoch 175/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6661 - acc: 0.7689\n",
      "Epoch 176/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6664 - acc: 0.7689\n",
      "Epoch 177/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6694 - acc: 0.7791\n",
      "Epoch 178/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6641 - acc: 0.7730\n",
      "Epoch 179/200\n",
      "489/489 [==============================] - 0s 50us/step - loss: 0.6606 - acc: 0.7751\n",
      "Epoch 180/200\n",
      "489/489 [==============================] - 0s 49us/step - loss: 0.6623 - acc: 0.7730\n",
      "Epoch 181/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6942 - acc: 0.7730\n",
      "Epoch 182/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6583 - acc: 0.7669\n",
      "Epoch 183/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6697 - acc: 0.7628\n",
      "Epoch 184/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6550 - acc: 0.7730\n",
      "Epoch 185/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6703 - acc: 0.7791\n",
      "Epoch 186/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6636 - acc: 0.7771\n",
      "Epoch 187/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6573 - acc: 0.7894\n",
      "Epoch 188/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6541 - acc: 0.7730\n",
      "Epoch 189/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6527 - acc: 0.7771\n",
      "Epoch 190/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6629 - acc: 0.7628\n",
      "Epoch 191/200\n",
      "489/489 [==============================] - 0s 48us/step - loss: 0.6500 - acc: 0.7751\n",
      "Epoch 192/200\n",
      "489/489 [==============================] - 0s 54us/step - loss: 0.6537 - acc: 0.7607\n",
      "Epoch 193/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6555 - acc: 0.7791\n",
      "Epoch 194/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6478 - acc: 0.7730\n",
      "Epoch 195/200\n",
      "489/489 [==============================] - 0s 55us/step - loss: 0.6424 - acc: 0.7791\n",
      "Epoch 196/200\n",
      "489/489 [==============================] - 0s 47us/step - loss: 0.6523 - acc: 0.7730\n",
      "Epoch 197/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.6438 - acc: 0.7771\n",
      "Epoch 198/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6654 - acc: 0.7873\n",
      "Epoch 199/200\n",
      "489/489 [==============================] - 0s 46us/step - loss: 0.6444 - acc: 0.7751\n",
      "Epoch 200/200\n",
      "489/489 [==============================] - 0s 45us/step - loss: 0.6339 - acc: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c46ed5f28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can't work well either just %78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "[[ 8  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  5  8  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  2  7  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  1  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  3  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  3  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  2  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11  1  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0 10  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0  1  9  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  1  7  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  3 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89         9\n",
      "           1       0.83      0.91      0.87        11\n",
      "           2       0.53      1.00      0.69        10\n",
      "           3       0.67      0.57      0.62        14\n",
      "           4       0.78      0.64      0.70        11\n",
      "           5       0.67      0.73      0.70        11\n",
      "           6       0.75      0.69      0.72        13\n",
      "           7       0.75      0.50      0.60         6\n",
      "           8       0.33      0.40      0.36         5\n",
      "           9       1.00      0.85      0.92        13\n",
      "          10       0.77      0.83      0.80        12\n",
      "          11       0.90      0.64      0.75        14\n",
      "          12       0.70      0.78      0.74         9\n",
      "          13       0.60      0.86      0.71         7\n",
      "          14       1.00      0.79      0.88        19\n",
      "\n",
      "    accuracy                           0.75       164\n",
      "   macro avg       0.74      0.74      0.73       164\n",
      "weighted avg       0.78      0.75      0.75       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred2=model.predict(X_test)\n",
    "ypred=ypred2.argmax(1)\n",
    "y_test=y_test.argmax(1)\n",
    "print(metrik.accuracy_score(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.confusion_matrix(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.classification_report(y_true=y_test,y_pred=ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can't work good enough. Ä°nterestingly. Add more layers maybe work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(500,kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(120,kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(60,kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(15,kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 2.6025 - acc: 0.3088\n",
      "Epoch 2/200\n",
      "489/489 [==============================] - 0s 314us/step - loss: 1.8197 - acc: 0.3988\n",
      "Epoch 3/200\n",
      "489/489 [==============================] - 0s 331us/step - loss: 1.3451 - acc: 0.4847\n",
      "Epoch 4/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 1.2656 - acc: 0.5051\n",
      "Epoch 5/200\n",
      "489/489 [==============================] - 0s 305us/step - loss: 1.2136 - acc: 0.5337\n",
      "Epoch 6/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 1.1655 - acc: 0.5992\n",
      "Epoch 7/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 1.1069 - acc: 0.6278\n",
      "Epoch 8/200\n",
      "489/489 [==============================] - 0s 309us/step - loss: 1.0912 - acc: 0.6053\n",
      "Epoch 9/200\n",
      "489/489 [==============================] - 0s 324us/step - loss: 1.0785 - acc: 0.6155\n",
      "Epoch 10/200\n",
      "489/489 [==============================] - 0s 303us/step - loss: 1.0262 - acc: 0.6319\n",
      "Epoch 11/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 1.0183 - acc: 0.6401\n",
      "Epoch 12/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 1.0405 - acc: 0.6421\n",
      "Epoch 13/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 1.0171 - acc: 0.6339\n",
      "Epoch 14/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 1.0053 - acc: 0.6564\n",
      "Epoch 15/200\n",
      "489/489 [==============================] - 0s 299us/step - loss: 0.9846 - acc: 0.7076\n",
      "Epoch 16/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 0.9835 - acc: 0.6748\n",
      "Epoch 17/200\n",
      "489/489 [==============================] - 0s 316us/step - loss: 0.9709 - acc: 0.6708\n",
      "Epoch 18/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.9217 - acc: 0.6953\n",
      "Epoch 19/200\n",
      "489/489 [==============================] - 0s 313us/step - loss: 0.8911 - acc: 0.7137\n",
      "Epoch 20/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.9027 - acc: 0.7464\n",
      "Epoch 21/200\n",
      "489/489 [==============================] - 0s 307us/step - loss: 0.8624 - acc: 0.7137\n",
      "Epoch 22/200\n",
      "489/489 [==============================] - 0s 331us/step - loss: 0.8787 - acc: 0.7178\n",
      "Epoch 23/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.8608 - acc: 0.7239\n",
      "Epoch 24/200\n",
      "489/489 [==============================] - 0s 310us/step - loss: 0.8841 - acc: 0.7096\n",
      "Epoch 25/200\n",
      "489/489 [==============================] - 0s 323us/step - loss: 0.8885 - acc: 0.7014\n",
      "Epoch 26/200\n",
      "489/489 [==============================] - 0s 320us/step - loss: 0.8730 - acc: 0.7137\n",
      "Epoch 27/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.8786 - acc: 0.7076\n",
      "Epoch 28/200\n",
      "489/489 [==============================] - 0s 329us/step - loss: 0.8828 - acc: 0.7096\n",
      "Epoch 29/200\n",
      "489/489 [==============================] - 0s 313us/step - loss: 0.8708 - acc: 0.7117\n",
      "Epoch 30/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.8663 - acc: 0.7076\n",
      "Epoch 31/200\n",
      "489/489 [==============================] - 0s 299us/step - loss: 0.8650 - acc: 0.7198\n",
      "Epoch 32/200\n",
      "489/489 [==============================] - 0s 326us/step - loss: 0.8555 - acc: 0.7096\n",
      "Epoch 33/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 0.8275 - acc: 0.7382\n",
      "Epoch 34/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 0.8145 - acc: 0.7342\n",
      "Epoch 35/200\n",
      "489/489 [==============================] - 0s 320us/step - loss: 0.8260 - acc: 0.7219\n",
      "Epoch 36/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.7953 - acc: 0.7423\n",
      "Epoch 37/200\n",
      "489/489 [==============================] - 0s 302us/step - loss: 0.8270 - acc: 0.6953\n",
      "Epoch 38/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.8861 - acc: 0.6933\n",
      "Epoch 39/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.8311 - acc: 0.7382\n",
      "Epoch 40/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.7942 - acc: 0.7362\n",
      "Epoch 41/200\n",
      "489/489 [==============================] - 0s 323us/step - loss: 0.8507 - acc: 0.7219\n",
      "Epoch 42/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.8243 - acc: 0.7198\n",
      "Epoch 43/200\n",
      "489/489 [==============================] - 0s 319us/step - loss: 0.7804 - acc: 0.7382\n",
      "Epoch 44/200\n",
      "489/489 [==============================] - 0s 341us/step - loss: 0.7979 - acc: 0.7280\n",
      "Epoch 45/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 0.7900 - acc: 0.7342\n",
      "Epoch 46/200\n",
      "489/489 [==============================] - 0s 353us/step - loss: 0.7859 - acc: 0.7198\n",
      "Epoch 47/200\n",
      "489/489 [==============================] - 0s 331us/step - loss: 0.7696 - acc: 0.7403\n",
      "Epoch 48/200\n",
      "489/489 [==============================] - 0s 317us/step - loss: 0.7594 - acc: 0.7485\n",
      "Epoch 49/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.7474 - acc: 0.7648\n",
      "Epoch 50/200\n",
      "489/489 [==============================] - 0s 309us/step - loss: 0.7225 - acc: 0.7607\n",
      "Epoch 51/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.7293 - acc: 0.7607\n",
      "Epoch 52/200\n",
      "489/489 [==============================] - 0s 305us/step - loss: 0.7600 - acc: 0.7444\n",
      "Epoch 53/200\n",
      "489/489 [==============================] - 0s 330us/step - loss: 0.7429 - acc: 0.7587\n",
      "Epoch 54/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.7531 - acc: 0.7464\n",
      "Epoch 55/200\n",
      "489/489 [==============================] - 0s 314us/step - loss: 0.7398 - acc: 0.7689\n",
      "Epoch 56/200\n",
      "489/489 [==============================] - 0s 297us/step - loss: 0.7363 - acc: 0.7423\n",
      "Epoch 57/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.7375 - acc: 0.7587\n",
      "Epoch 58/200\n",
      "489/489 [==============================] - 0s 304us/step - loss: 0.7260 - acc: 0.7546\n",
      "Epoch 59/200\n",
      "489/489 [==============================] - 0s 305us/step - loss: 0.7287 - acc: 0.7546\n",
      "Epoch 60/200\n",
      "489/489 [==============================] - 0s 338us/step - loss: 0.6995 - acc: 0.7607\n",
      "Epoch 61/200\n",
      "489/489 [==============================] - 0s 294us/step - loss: 0.7010 - acc: 0.7648\n",
      "Epoch 62/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.7146 - acc: 0.7587\n",
      "Epoch 63/200\n",
      "489/489 [==============================] - 0s 306us/step - loss: 0.7318 - acc: 0.7464\n",
      "Epoch 64/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.7418 - acc: 0.7505\n",
      "Epoch 65/200\n",
      "489/489 [==============================] - 0s 309us/step - loss: 0.6850 - acc: 0.7628\n",
      "Epoch 66/200\n",
      "489/489 [==============================] - 0s 329us/step - loss: 0.7182 - acc: 0.7710\n",
      "Epoch 67/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.6986 - acc: 0.7669\n",
      "Epoch 68/200\n",
      "489/489 [==============================] - 0s 304us/step - loss: 0.6916 - acc: 0.7464\n",
      "Epoch 69/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 0.6916 - acc: 0.7648\n",
      "Epoch 70/200\n",
      "489/489 [==============================] - 0s 332us/step - loss: 0.6742 - acc: 0.7669\n",
      "Epoch 71/200\n",
      "489/489 [==============================] - 0s 313us/step - loss: 0.7267 - acc: 0.7382\n",
      "Epoch 72/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.7371 - acc: 0.7485\n",
      "Epoch 73/200\n",
      "489/489 [==============================] - 0s 330us/step - loss: 0.6640 - acc: 0.7710\n",
      "Epoch 74/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.7154 - acc: 0.7526\n",
      "Epoch 75/200\n",
      "489/489 [==============================] - 0s 309us/step - loss: 0.7030 - acc: 0.7546\n",
      "Epoch 76/200\n",
      "489/489 [==============================] - 0s 304us/step - loss: 0.6660 - acc: 0.7628\n",
      "Epoch 77/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.6732 - acc: 0.7975\n",
      "Epoch 78/200\n",
      "489/489 [==============================] - 0s 306us/step - loss: 0.6864 - acc: 0.7485\n",
      "Epoch 79/200\n",
      "489/489 [==============================] - 0s 349us/step - loss: 0.6894 - acc: 0.7710\n",
      "Epoch 80/200\n",
      "489/489 [==============================] - 0s 331us/step - loss: 0.7383 - acc: 0.7423\n",
      "Epoch 81/200\n",
      "489/489 [==============================] - 0s 317us/step - loss: 0.7072 - acc: 0.7607\n",
      "Epoch 82/200\n",
      "489/489 [==============================] - 0s 320us/step - loss: 0.6729 - acc: 0.7566\n",
      "Epoch 83/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.6791 - acc: 0.7485\n",
      "Epoch 84/200\n",
      "489/489 [==============================] - 0s 302us/step - loss: 0.6860 - acc: 0.7464\n",
      "Epoch 85/200\n",
      "489/489 [==============================] - 0s 343us/step - loss: 0.6518 - acc: 0.7730\n",
      "Epoch 86/200\n",
      "489/489 [==============================] - 0s 336us/step - loss: 0.6368 - acc: 0.7751\n",
      "Epoch 87/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.6855 - acc: 0.7669\n",
      "Epoch 88/200\n",
      "489/489 [==============================] - 0s 305us/step - loss: 0.6624 - acc: 0.7587\n",
      "Epoch 89/200\n",
      "489/489 [==============================] - 0s 306us/step - loss: 0.6464 - acc: 0.7587\n",
      "Epoch 90/200\n",
      "489/489 [==============================] - 0s 332us/step - loss: 0.5996 - acc: 0.7894\n",
      "Epoch 91/200\n",
      "489/489 [==============================] - 0s 333us/step - loss: 0.6289 - acc: 0.7914\n",
      "Epoch 92/200\n",
      "489/489 [==============================] - 0s 328us/step - loss: 0.6752 - acc: 0.7771\n",
      "Epoch 93/200\n",
      "489/489 [==============================] - 0s 337us/step - loss: 0.6450 - acc: 0.7689\n",
      "Epoch 94/200\n",
      "489/489 [==============================] - 0s 329us/step - loss: 0.6383 - acc: 0.7791\n",
      "Epoch 95/200\n",
      "489/489 [==============================] - 0s 306us/step - loss: 0.6277 - acc: 0.7853\n",
      "Epoch 96/200\n",
      "489/489 [==============================] - 0s 326us/step - loss: 0.5971 - acc: 0.7832\n",
      "Epoch 97/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.6054 - acc: 0.7853\n",
      "Epoch 98/200\n",
      "489/489 [==============================] - 0s 326us/step - loss: 0.6224 - acc: 0.8057\n",
      "Epoch 99/200\n",
      "489/489 [==============================] - 0s 341us/step - loss: 0.6107 - acc: 0.7771\n",
      "Epoch 100/200\n",
      "489/489 [==============================] - 0s 310us/step - loss: 0.6258 - acc: 0.7894\n",
      "Epoch 101/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 0.6333 - acc: 0.7812\n",
      "Epoch 102/200\n",
      "489/489 [==============================] - 0s 316us/step - loss: 0.5979 - acc: 0.7894\n",
      "Epoch 103/200\n",
      "489/489 [==============================] - 0s 310us/step - loss: 0.6192 - acc: 0.7812\n",
      "Epoch 104/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.5793 - acc: 0.7873\n",
      "Epoch 105/200\n",
      "489/489 [==============================] - 0s 329us/step - loss: 0.6038 - acc: 0.7771\n",
      "Epoch 106/200\n",
      "489/489 [==============================] - 0s 326us/step - loss: 0.5738 - acc: 0.7996\n",
      "Epoch 107/200\n",
      "489/489 [==============================] - 0s 322us/step - loss: 0.6053 - acc: 0.7771\n",
      "Epoch 108/200\n",
      "489/489 [==============================] - 0s 324us/step - loss: 0.5614 - acc: 0.8139\n",
      "Epoch 109/200\n",
      "489/489 [==============================] - 0s 305us/step - loss: 0.5720 - acc: 0.7996\n",
      "Epoch 110/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.5559 - acc: 0.7975\n",
      "Epoch 111/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.5553 - acc: 0.8037\n",
      "Epoch 112/200\n",
      "489/489 [==============================] - 0s 332us/step - loss: 0.5515 - acc: 0.7935\n",
      "Epoch 113/200\n",
      "489/489 [==============================] - 0s 335us/step - loss: 0.5633 - acc: 0.7873\n",
      "Epoch 114/200\n",
      "489/489 [==============================] - 0s 324us/step - loss: 0.5858 - acc: 0.8057\n",
      "Epoch 115/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 0.6408 - acc: 0.7710\n",
      "Epoch 116/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.5782 - acc: 0.7975\n",
      "Epoch 117/200\n",
      "489/489 [==============================] - 0s 316us/step - loss: 0.5861 - acc: 0.7894\n",
      "Epoch 118/200\n",
      "489/489 [==============================] - 0s 314us/step - loss: 0.5841 - acc: 0.7975\n",
      "Epoch 119/200\n",
      "489/489 [==============================] - 0s 322us/step - loss: 0.5439 - acc: 0.8119\n",
      "Epoch 120/200\n",
      "489/489 [==============================] - 0s 314us/step - loss: 0.5230 - acc: 0.8262\n",
      "Epoch 121/200\n",
      "489/489 [==============================] - 0s 304us/step - loss: 0.5192 - acc: 0.8119\n",
      "Epoch 122/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.5171 - acc: 0.8119\n",
      "Epoch 123/200\n",
      "489/489 [==============================] - 0s 301us/step - loss: 0.5355 - acc: 0.8180\n",
      "Epoch 124/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.5647 - acc: 0.7894\n",
      "Epoch 125/200\n",
      "489/489 [==============================] - 0s 336us/step - loss: 0.5545 - acc: 0.7894\n",
      "Epoch 126/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.5379 - acc: 0.8139\n",
      "Epoch 127/200\n",
      "489/489 [==============================] - 0s 322us/step - loss: 0.5529 - acc: 0.7996\n",
      "Epoch 128/200\n",
      "489/489 [==============================] - 0s 317us/step - loss: 0.5937 - acc: 0.7751\n",
      "Epoch 129/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 0.5634 - acc: 0.7873\n",
      "Epoch 130/200\n",
      "489/489 [==============================] - 0s 319us/step - loss: 0.5602 - acc: 0.7975\n",
      "Epoch 131/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 0.5472 - acc: 0.8037\n",
      "Epoch 132/200\n",
      "489/489 [==============================] - 0s 379us/step - loss: 0.5394 - acc: 0.8180\n",
      "Epoch 133/200\n",
      "489/489 [==============================] - 0s 338us/step - loss: 0.5615 - acc: 0.7996\n",
      "Epoch 134/200\n",
      "489/489 [==============================] - 0s 305us/step - loss: 0.5821 - acc: 0.7873\n",
      "Epoch 135/200\n",
      "489/489 [==============================] - 0s 328us/step - loss: 0.5957 - acc: 0.7710\n",
      "Epoch 136/200\n",
      "489/489 [==============================] - 0s 332us/step - loss: 0.5980 - acc: 0.7996\n",
      "Epoch 137/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.5270 - acc: 0.8098\n",
      "Epoch 138/200\n",
      "489/489 [==============================] - 0s 336us/step - loss: 0.5571 - acc: 0.7996\n",
      "Epoch 139/200\n",
      "489/489 [==============================] - 0s 324us/step - loss: 0.5039 - acc: 0.8303\n",
      "Epoch 140/200\n",
      "489/489 [==============================] - 0s 313us/step - loss: 0.5245 - acc: 0.8057\n",
      "Epoch 141/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.5201 - acc: 0.8098\n",
      "Epoch 142/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.5242 - acc: 0.8098\n",
      "Epoch 143/200\n",
      "489/489 [==============================] - 0s 306us/step - loss: 0.5048 - acc: 0.8160\n",
      "Epoch 144/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.4813 - acc: 0.8405\n",
      "Epoch 145/200\n",
      "489/489 [==============================] - 0s 335us/step - loss: 0.4998 - acc: 0.8180\n",
      "Epoch 146/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.4829 - acc: 0.8303\n",
      "Epoch 147/200\n",
      "489/489 [==============================] - 0s 301us/step - loss: 0.4807 - acc: 0.8364\n",
      "Epoch 148/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.5008 - acc: 0.8344\n",
      "Epoch 149/200\n",
      "489/489 [==============================] - 0s 338us/step - loss: 0.4918 - acc: 0.8139\n",
      "Epoch 150/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.5354 - acc: 0.8160\n",
      "Epoch 151/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.4982 - acc: 0.8200\n",
      "Epoch 152/200\n",
      "489/489 [==============================] - 0s 316us/step - loss: 0.5650 - acc: 0.8016\n",
      "Epoch 153/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 0.4994 - acc: 0.8241\n",
      "Epoch 154/200\n",
      "489/489 [==============================] - 0s 302us/step - loss: 0.5264 - acc: 0.8016\n",
      "Epoch 155/200\n",
      "489/489 [==============================] - 0s 355us/step - loss: 0.4996 - acc: 0.8057\n",
      "Epoch 156/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.4944 - acc: 0.8241\n",
      "Epoch 157/200\n",
      "489/489 [==============================] - 0s 322us/step - loss: 0.4983 - acc: 0.8139\n",
      "Epoch 158/200\n",
      "489/489 [==============================] - 0s 342us/step - loss: 0.4593 - acc: 0.8405\n",
      "Epoch 159/200\n",
      "489/489 [==============================] - 0s 350us/step - loss: 0.4484 - acc: 0.8507\n",
      "Epoch 160/200\n",
      "489/489 [==============================] - 0s 322us/step - loss: 0.4320 - acc: 0.8446\n",
      "Epoch 161/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.4644 - acc: 0.8466\n",
      "Epoch 162/200\n",
      "489/489 [==============================] - 0s 351us/step - loss: 0.4402 - acc: 0.8323\n",
      "Epoch 163/200\n",
      "489/489 [==============================] - 0s 329us/step - loss: 0.4537 - acc: 0.8364\n",
      "Epoch 164/200\n",
      "489/489 [==============================] - 0s 342us/step - loss: 0.4415 - acc: 0.8364\n",
      "Epoch 165/200\n",
      "489/489 [==============================] - 0s 327us/step - loss: 0.4518 - acc: 0.8384\n",
      "Epoch 166/200\n",
      "489/489 [==============================] - 0s 319us/step - loss: 0.4347 - acc: 0.8405\n",
      "Epoch 167/200\n",
      "489/489 [==============================] - 0s 318us/step - loss: 0.4377 - acc: 0.8344\n",
      "Epoch 168/200\n",
      "489/489 [==============================] - 0s 320us/step - loss: 0.4156 - acc: 0.8487\n",
      "Epoch 169/200\n",
      "489/489 [==============================] - 0s 334us/step - loss: 0.4463 - acc: 0.8446\n",
      "Epoch 170/200\n",
      "489/489 [==============================] - 0s 338us/step - loss: 0.5360 - acc: 0.8160\n",
      "Epoch 171/200\n",
      "489/489 [==============================] - 0s 343us/step - loss: 0.5056 - acc: 0.8262\n",
      "Epoch 172/200\n",
      "489/489 [==============================] - 0s 314us/step - loss: 0.4507 - acc: 0.8241\n",
      "Epoch 173/200\n",
      "489/489 [==============================] - 0s 313us/step - loss: 0.4229 - acc: 0.8487\n",
      "Epoch 174/200\n",
      "489/489 [==============================] - 0s 343us/step - loss: 0.4305 - acc: 0.8364\n",
      "Epoch 175/200\n",
      "489/489 [==============================] - 0s 316us/step - loss: 0.4264 - acc: 0.8528\n",
      "Epoch 176/200\n",
      "489/489 [==============================] - 0s 323us/step - loss: 0.4150 - acc: 0.8548\n",
      "Epoch 177/200\n",
      "489/489 [==============================] - 0s 337us/step - loss: 0.4359 - acc: 0.8446\n",
      "Epoch 178/200\n",
      "489/489 [==============================] - 0s 323us/step - loss: 0.4250 - acc: 0.8487\n",
      "Epoch 179/200\n",
      "489/489 [==============================] - 0s 330us/step - loss: 0.3825 - acc: 0.8650\n",
      "Epoch 180/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.4067 - acc: 0.8384\n",
      "Epoch 181/200\n",
      "489/489 [==============================] - 0s 334us/step - loss: 0.4614 - acc: 0.8323\n",
      "Epoch 182/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.4733 - acc: 0.8364\n",
      "Epoch 183/200\n",
      "489/489 [==============================] - 0s 337us/step - loss: 0.4305 - acc: 0.8405\n",
      "Epoch 184/200\n",
      "489/489 [==============================] - 0s 316us/step - loss: 0.4295 - acc: 0.8425\n",
      "Epoch 185/200\n",
      "489/489 [==============================] - 0s 334us/step - loss: 0.4108 - acc: 0.8548\n",
      "Epoch 186/200\n",
      "489/489 [==============================] - 0s 326us/step - loss: 0.4026 - acc: 0.8528\n",
      "Epoch 187/200\n",
      "489/489 [==============================] - 0s 311us/step - loss: 0.3973 - acc: 0.8425\n",
      "Epoch 188/200\n",
      "489/489 [==============================] - 0s 336us/step - loss: 0.3942 - acc: 0.8609\n",
      "Epoch 189/200\n",
      "489/489 [==============================] - 0s 329us/step - loss: 0.3979 - acc: 0.8528\n",
      "Epoch 190/200\n",
      "489/489 [==============================] - 0s 350us/step - loss: 0.3836 - acc: 0.8712\n",
      "Epoch 191/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.3872 - acc: 0.8630\n",
      "Epoch 192/200\n",
      "489/489 [==============================] - 0s 321us/step - loss: 0.3621 - acc: 0.8753\n",
      "Epoch 193/200\n",
      "489/489 [==============================] - 0s 300us/step - loss: 0.4105 - acc: 0.8569\n",
      "Epoch 194/200\n",
      "489/489 [==============================] - 0s 325us/step - loss: 0.3993 - acc: 0.8446\n",
      "Epoch 195/200\n",
      "489/489 [==============================] - 0s 319us/step - loss: 0.3795 - acc: 0.8609\n",
      "Epoch 196/200\n",
      "489/489 [==============================] - 0s 324us/step - loss: 0.3575 - acc: 0.8691\n",
      "Epoch 197/200\n",
      "489/489 [==============================] - 0s 312us/step - loss: 0.3712 - acc: 0.8691\n",
      "Epoch 198/200\n",
      "489/489 [==============================] - 0s 308us/step - loss: 0.3640 - acc: 0.8609\n",
      "Epoch 199/200\n",
      "489/489 [==============================] - 0s 326us/step - loss: 0.3578 - acc: 0.8691\n",
      "Epoch 200/200\n",
      "489/489 [==============================] - 0s 315us/step - loss: 0.3548 - acc: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c59586b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y2_train, y2_test = train_test_split(scale_x, dummy_y, test_size=0.25, random_state=42)\n",
    "model.fit(X_train,y2_train,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7317073170731707\n",
      "[[ 8  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  9  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  6  3  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0 10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  2 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  3  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1 10  1  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0 10  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  2  9  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  1  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  3  5 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.90      0.82      0.86        11\n",
      "           2       0.56      0.90      0.69        10\n",
      "           3       0.90      0.64      0.75        14\n",
      "           4       0.75      0.55      0.63        11\n",
      "           5       0.56      0.91      0.69        11\n",
      "           6       0.83      0.77      0.80        13\n",
      "           7       0.60      0.50      0.55         6\n",
      "           8       0.43      0.60      0.50         5\n",
      "           9       1.00      0.77      0.87        13\n",
      "          10       0.71      0.83      0.77        12\n",
      "          11       0.90      0.64      0.75        14\n",
      "          12       0.58      0.78      0.67         9\n",
      "          13       0.50      0.86      0.63         7\n",
      "          14       0.92      0.58      0.71        19\n",
      "\n",
      "    accuracy                           0.73       164\n",
      "   macro avg       0.74      0.74      0.72       164\n",
      "weighted avg       0.78      0.73      0.74       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred3=model.predict(X_test)\n",
    "ypred=ypred3.argmax(1)\n",
    "y_test=y2_test.argmax(1)\n",
    "print(metrik.accuracy_score(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.confusion_matrix(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.classification_report(y_true=y_test,y_pred=ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance is not good. Deep NN can't work best for these dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe KNN is worked just fine. Distance based Algorithm work well in this type of situation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(653, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7073170731707317\n",
      "[[ 8  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 10  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  9  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  1  6  6  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  3  6  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  1  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  2  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  3  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  2  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11  1  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0 10  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  1  2  0  1  9  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  6  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  2 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84         9\n",
      "           1       0.77      0.91      0.83        11\n",
      "           2       0.50      0.90      0.64        10\n",
      "           3       0.55      0.43      0.48        14\n",
      "           4       0.67      0.55      0.60        11\n",
      "           5       0.73      0.73      0.73        11\n",
      "           6       0.75      0.69      0.72        13\n",
      "           7       0.60      0.50      0.55         6\n",
      "           8       0.40      0.40      0.40         5\n",
      "           9       1.00      0.85      0.92        13\n",
      "          10       0.77      0.83      0.80        12\n",
      "          11       1.00      0.64      0.78        14\n",
      "          12       0.67      0.67      0.67         9\n",
      "          13       0.38      0.43      0.40         7\n",
      "          14       0.80      0.84      0.82        19\n",
      "\n",
      "    accuracy                           0.71       164\n",
      "   macro avg       0.69      0.68      0.68       164\n",
      "weighted avg       0.73      0.71      0.71       164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "knn.fit(x_train,y_train)\n",
    "ypred=knn.predict(x_test)\n",
    "print(metrik.accuracy_score(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.confusion_matrix(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.classification_report(y_true=y_test,y_pred=ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ä°nterestingly interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7560975609756098\n",
      "neighbour: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "best=0\n",
    "best_acc=0\n",
    "for i in range(1,15):\n",
    "    knn=KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(x_train,y_train)\n",
    "    ypred=knn.predict(x_test)\n",
    "    acc=metrik.accuracy_score(y_true=y_test,y_pred=ypred)\n",
    "    if best_acc<acc:\n",
    "        best=i\n",
    "        best_acc=acc\n",
    "print(\"Accuracy: \"+str(best_acc))\n",
    "print(\"neighbour: \"+str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best is %75 with KNN or 1NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe our classes is too many. Many use only 0 or 1 0 asidic 1 basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybinary=y>7\n",
    "ybinary=ybinary.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975609756097561\n",
      "[[83  2]\n",
      " [ 2 77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        85\n",
      "           1       0.97      0.97      0.97        79\n",
      "\n",
      "    accuracy                           0.98       164\n",
      "   macro avg       0.98      0.98      0.98       164\n",
      "weighted avg       0.98      0.98      0.98       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, ybinary, test_size=0.25, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train,y_train)\n",
    "ypred=knn.predict(x_test)\n",
    "print(metrik.accuracy_score(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.confusion_matrix(y_true=y_test,y_pred=ypred))\n",
    "print(metrik.classification_report(y_true=y_test,y_pred=ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These APPROACH work just fine. Howewer we forget notr in this time :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
