{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction Job Termination"},{"metadata":{},"cell_type":"markdown","source":"## Importing Necesary Code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading in the Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/MFG10YearTerminationData.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preliminary Cleaning of the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\n\n- No null values\n- Dataset is a mix of integers and strings\n- Some columns are repetitive, and can be removed without further analysis\n    - age can be determined from birthdate_key and recorddate_key, one set can be discarded\n    - length_of_service can be determined from orighiredate_key and recorddate_key, one set can be discarded\n    - depatment_name and job_title are also pretty similar, so one column can be discarded\n    - city_name also has little relevance\n- Some columns give away information that or machine learning model is trying to predoct, so these colums must be removed\n    - termreason_desc gives away the fact that those employees were terminated\n    - termtype_desc also gives away the fact that those employees were terminated\n- Various entries in the columns are in the form of strings, we must change them to integral values\n\nBased on these observations, we can start removing and reformatting data"},{"metadata":{},"cell_type":"markdown","source":"### Dropping Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns = ['birthdate_key', 'recorddate_key', 'orighiredate_key', 'terminationdate_key', 'termreason_desc', 'termtype_desc', 'department_name', 'gender_full'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reformating strings into integral data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['job_title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nJob_title is the most tedious column, as it has many distinct entries, we will generalize like jobs into categories, and\nthen turn them into numerical values\n'''\n\nboard = ['VP Stores', 'Director, Recruitment', 'VP Human Resources', 'VP Finance',\n         'Director, Accounts Receivable', 'Director, Accounting',\n         'Director, Employee Records', 'Director, Accounts Payable',\n         'Director, HR Technology', 'Director, Investments',\n         'Director, Labor Relations', 'Director, Audit', 'Director, Training',\n         'Director, Compensation']\n\nexecutive = ['Exec Assistant, Finance', 'Exec Assistant, Legal Counsel',\n             'CHief Information Officer', 'CEO', 'Exec Assistant, Human Resources',\n             'Exec Assistant, VP Stores']\n\nmanager = ['Customer Service Manager', 'Processed Foods Manager', 'Meats Manager',\n           'Bakery Manager', 'Produce Manager', 'Store Manager', 'Trainer', 'Dairy Manager']\n\n\nemployee = ['Meat Cutter', 'Dairy Person', 'Produce Clerk', 'Baker', 'Cashier',\n            'Shelf Stocker', 'Recruiter', 'HRIS Analyst', 'Accounting Clerk',\n            'Benefits Admin', 'Labor Relations Analyst', 'Accounts Receiveable Clerk',\n            'Accounts Payable Clerk', 'Auditor', 'Compensation Analyst',\n            'Investment Analyst', 'Systems Analyst', 'Corporate Lawyer', 'Legal Counsel']\n\ndef changeTitle(row):\n    if row in board:\n        return 'board'\n    elif row in executive:\n        return 'executive'\n    elif row in manager:\n        return 'manager'\n    else:\n        return 'employee'\n    \ndf['job_title'] = df['job_title'].apply(changeTitle)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['job_title'] = df['job_title'].map({'board': 3, 'executive': 2, 'manager': 1, 'employee': 0})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['city_name'].value_counts()\n\n# We will sort these cities by population, and then turn them into integral values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_pop_2011 = {'Vancouver':2313328,\n                 'Victoria':344615,\n                 'Nanaimo':146574,\n                 'New Westminster':65976,\n                 'Kelowna':179839,\n                 'Burnaby':223218,\n                 'Kamloops':85678,\n                 'Prince George':71974,\n                 'Cranbrook':19319,\n                 'Surrey':468251,\n                 'Richmond':190473,\n                 'Terrace':11486,\n                 'Chilliwack':77936,\n                 'Trail':7681,\n                 'Langley':25081,\n                 'Vernon':38180,\n                 'Squamish':17479,\n                 'Quesnel':10007,\n                 'Abbotsford':133497,\n                 'North Vancouver':48196,\n                 'Fort St John':18609,\n                 'Williams Lake':10832,\n                 'West Vancouver':42694,\n                 'Port Coquitlam':55985,\n                 'Aldergrove':12083,\n                 'Fort Nelson':3561,\n                 'Nelson':10230,\n                 'New Westminister':65976,\n                 'Grand Forks':3985,\n                 'White Rock':19339,\n                 'Haney':76052,\n                 'Princeton':2724,\n                 'Dawson Creek':11583,\n                 'Bella Bella':1095,\n                 'Ocean Falls':129,\n                 'Pitt Meadows':17736,\n                 'Cortes Island':1007,\n                 'Valemount':1020,\n                 'Dease Lake':58,\n                 'Blue River':215}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make a copy of city names\ndf['Pop'] = df['city_name']\n\n# Map from city name to population\ndf['Pop'] = df.Pop.map(city_pop_2011)\n\n# Make a new column for population category\ndf['Pop_category'] = df.Pop\n\n# Categorise according to population size\n# >= 100,000 is City\n# 10,000 to 99,999 is Rural\n# < 10,000 is Remote\n# Guidance from Australian Institute of Health and Welfare\n# http://www.aihw.gov.au/rural-health-rrma-classification/\ncity_ix = (df['Pop'] >= 100000)\nrural_ix = ((df['Pop'] < 100000) & (df['Pop'] >= 10000))\nremote_ix = (df['Pop'] < 10000)\ndf.loc[city_ix, 'Pop_category'] = 'City'\ndf.loc[rural_ix, 'Pop_category'] = 'Rural'\ndf.loc[remote_ix, 'Pop_category'] = 'Remote'\n\ndf['Pop_category'] = df['Pop_category'].map({'City' : 0, 'Rural' : 1, 'Remote' : 2})\n\n# Check the replacement went to plan\ndf.Pop_category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['gender_short'] = df['gender_short'].map({'M': 1, 'F': 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['STATUS'] = df['STATUS'].map({'ACTIVE': 1, 'TERMINATED': 0})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BUSINESS_UNIT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BUSINESS_UNIT'] = df['BUSINESS_UNIT'].map({'STORES': 0, 'HEADOFFICE' :1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interpreting the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"out_of_co = df[df.STATUS == 0]\nin_co = df[df.STATUS == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = sns.jointplot(out_of_co.age, out_of_co.length_of_service, color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = sns.FacetGrid(out_of_co, col='Pop_category', row='job_title', palette='Set1_r', \n                  hue='gender_short', margin_titles=True)\nb = (a.map(plt.scatter, 'age', 'length_of_service').add_legend())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = sns.FacetGrid(in_co, col='Pop_category', row='job_title', palette='Set1_r', \n                  hue='gender_short', margin_titles=True)\nd = (c.map(plt.scatter, 'age', 'length_of_service').add_legend())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\n\n- Nobody was terminated if they were at the executive level, or higher.\n- No major discrepancy between male and female termination\n- Executives and board members live in the cities"},{"metadata":{},"cell_type":"markdown","source":"## Preparing The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting columns that are not relevant to predictions\nshort_df = df.drop(columns = ['EmployeeID', 'store_name','job_title','BUSINESS_UNIT', 'city_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = short_df['STATUS']\nX = short_df.drop('STATUS', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                    random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Models "},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding the Feature Importance in a Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finds how important each feature is for the model to make a prediction\nfeature_importances = pd.DataFrame(model.feature_importances_,\n                                   index = X_train.columns,\n                                   columns=['importance']).sort_values('importance',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(feature_importances.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(feature_importances.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(feature_importances.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots()\n\nimportances = model.feature_importances_\nstd = np.std([model.feature_importances_ for tree in model.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\nnames = list(feature_importances.index)\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\n\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), names, rotation=90)\nplt.xlim([-1, X.shape[1]])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next Steps\n\nI would use a nueral network, as they might be more effective in predicting based on differnt sized datasets"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}