{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What we will be doing? \n1. Doing some basic EDA over the data. \n3. Fitting a Logistic regression model over the model."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/predicting-a-pulsar-star/pulsar_stars.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = [\"mean_intrgt\", \"std_intrgt\", \"kurtosis_intrgt\", \"skew_intrgt\", \"mean_dmsnr\", \"std_dmsnr\", \"kurtosis_dmsnr\", \"skew_dmsnr\", \"class\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data[data[\"class\"] == 0][\"mean_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"mean_intrgt\"], label = \"Class 1\")\nplt.title(\"Mean of the integrated profile\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DM-SNR curve\nsns.kdeplot(data[data[\"class\"] == 0][\"mean_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"mean_dmsnr\"], label = \"Class 1\")\nplt.title(\"Mean of the DM-SNR curve\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data[data[\"class\"] == 0][\"std_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"std_intrgt\"], label = \"Class 1\")\nplt.title(\"Std. Dev. of the integrated profile\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DM-SNR curve\nsns.kdeplot(data[data[\"class\"] == 0][\"std_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"std_dmsnr\"], label = \"Class 1\")\nplt.title(\"Std. Dev. of the DM-SNR curve\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data[data[\"class\"] == 0][\"kurtosis_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"kurtosis_intrgt\"], label = \"Class 1\")\nplt.title(\"Excess Kurtosis of Integrated Profile\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data[data[\"class\"] == 0][\"kurtosis_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"kurtosis_dmsnr\"], label = \"Class 1\")\nplt.title(\"Excess Kurtosis of DM-SNR Curve\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data[data[\"class\"] == 0][\"skew_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"skew_intrgt\"], label = \"Class 1\")\nplt.title(\"Skewness of Integrated Profile\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data[data[\"class\"] == 0][\"skew_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"skew_dmsnr\"], label = \"Class 1\")\nplt.title(\"Skewness of the DM-SNR Curve\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[\"class\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA \npca = PCA(n_components=2)\npca_data = pca.fit_transform(data.iloc[:, :-1])\n\npca_data = pd.DataFrame(pca_data)\npca_data[\"class\"] = data[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pca_data[pca_data[\"class\"] == 1].iloc[:, 0], pca_data[pca_data[\"class\"] == 1].iloc[:, 1], color = \"green\",label = \"Class 1\" )\nplt.scatter(pca_data[pca_data[\"class\"] == 0].iloc[:, 0], pca_data[pca_data[\"class\"] == 0].iloc[:, 1], color = \"red\", label = \"Class 0\" )\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pca_data[pca_data[\"class\"] == 0].iloc[:, 0], pca_data[pca_data[\"class\"] == 0].iloc[:, 1], color = \"red\", label = \"Class 0\" )\nplt.scatter(pca_data[pca_data[\"class\"] == 1].iloc[:, 0], pca_data[pca_data[\"class\"] == 1].iloc[:, 1], color = \"blue\", alpha= .2,label = \"Class 1\" )\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\nfor i in data.columns[:-1]:\n    for j in data.columns[:-1]:\n        corr = pearsonr(data[i], data[j])[0]\n        if corr > .7 or corr < -.7:\n            print(i, \" \", j, pearsonr(data[i], data[j])[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, shuffle = True, random_state  = 8)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .1, shuffle = True, random_state  = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_val, preds)\nsns.heatmap(confusion_matrix(y_val, preds), annot = True, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(X_test)\nconfusion_matrix(y_test, test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", (4846+ 404)/(4846+ 404 + 95 + 25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is highly unbalanced and in unbalanced accuracy is not a good metric. We will be using precision, recall & F1 Score as our metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = (4846)/(4846+ 25)\nrecall = (4846)/(4846+ 95)\nprint(\"Precision: \",precision)\nprint(\"Recall: \", recall)\nprint(\"f1 score: \", (2*precision*recall)/(precision + recall))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What you can try? \n* Feature Selection -> Using one of the features from the features having high correlation \n* Using PCA with different number of components and training on that data. \n* Trying other classifiers"},{"metadata":{},"cell_type":"markdown","source":"Feedbacks are highly appreciated. <br>\n\nUpvote if you like this kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}